[{"id": 46672128, "title": "Writing an LLM from scratch, part 31 \u2013 the models are now on Hugging Face", "cluster": 13, "x": 6.643881320953369, "y": 3.241251230239868}, {"id": 46671499, "title": "LLM Pareto Frontier", "cluster": 13, "x": 7.01235294342041, "y": 3.440072774887085}, {"id": 46670910, "title": "Claude Agent Skill for Terraform and OpenTofu", "cluster": 38, "x": 8.516199111938477, "y": 3.7902214527130127}, {"id": 46670891, "title": "Benchmarking my parser generator against LLVM: I have a new target", "cluster": 13, "x": 6.9964165687561035, "y": 3.5865442752838135}, {"id": 46669352, "title": "Do LLMs Help Humans Reverse Engineer Software?", "cluster": 13, "x": 6.643566608428955, "y": 3.418468713760376}, {"id": 46667707, "title": "Coding with LLMs can still be fun", "cluster": 13, "x": 6.67294979095459, "y": 3.682406187057495}, {"id": 46666484, "title": "QWED AI \u2013 Open-source deterministic verification layer for LLMs", "cluster": 12, "x": 7.078001976013184, "y": 2.9805402755737305}, {"id": 46665781, "title": "Cutting LLM token Usage by ~80% using REPL driven document analysis", "cluster": 13, "x": 6.998044967651367, "y": 3.560482978820801}, {"id": 46665639, "title": "Best approach for generating SVG graphics with LLMs?", "cluster": 13, "x": 7.018501281738281, "y": 3.6086738109588623}, {"id": 46665577, "title": "Friend's Guide to Agentic Engineering", "cluster": 38, "x": 8.680164337158203, "y": 3.6736857891082764}, {"id": 46665183, "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "cluster": 13, "x": 6.8649001121521, "y": 3.170034646987915}, {"id": 46664930, "title": "Model is intended for use particularly for language learning", "cluster": 208, "x": 8.424261093139648, "y": 4.600339412689209}, {"id": 46664368, "title": "Oh My PI: coding agent CLI, unified LLM API, TUI and web UI libraries", "cluster": 13, "x": 6.922311305999756, "y": 3.3814010620117188}, {"id": 46664297, "title": "VaultGemma: A Differentially Private LLM", "cluster": 13, "x": 6.551260471343994, "y": 3.3693299293518066}, {"id": 46662474, "title": "Just shipped an agent mode (ReAct) in my CLI for LLMs", "cluster": 13, "x": 6.773632049560547, "y": 3.6927568912506104}, {"id": 46662300, "title": "Agent skills for full-stack development", "cluster": 38, "x": 8.646330833435059, "y": 3.6435399055480957}, {"id": 46661468, "title": "Prompt Repetition Improves Non-Reasoning LLMs", "cluster": 13, "x": 6.80206823348999, "y": 3.0800232887268066}, {"id": 46661464, "title": "Writing an LLM from scratch, part 31 \u2013 the models are now on Hugging Face", "cluster": 13, "x": 6.62360143661499, "y": 3.2443816661834717}, {"id": 46661432, "title": "Private LLM Inference on Consumer Blackwell GPUs", "cluster": 13, "x": 6.953222274780273, "y": 3.325083017349243}, {"id": 46661442, "title": "Histomat of F/OSS: We should reclaim LLMs, not reject them", "cluster": 13, "x": 6.524744033813477, "y": 3.13725209236145}, {"id": 46661320, "title": "How to make LLMs and Agents work on large amounts of data", "cluster": 13, "x": 6.959883689880371, "y": 3.3131396770477295}, {"id": 46660159, "title": "BAML is a domain-specific language to generate structured outputs from LLMs", "cluster": 13, "x": 6.931715965270996, "y": 3.729487180709839}, {"id": 46659489, "title": "Will we forget what pre-LLM era looked like?", "cluster": 13, "x": 6.5523858070373535, "y": 3.082519054412842}, {"id": 46659044, "title": "LLMs are deciding your career", "cluster": 13, "x": 6.601836681365967, "y": 3.118675470352173}, {"id": 46658706, "title": "Open Responses: specification for building interoperable LLM interfaces", "cluster": 13, "x": 6.947009563446045, "y": 3.7503392696380615}, {"id": 46658248, "title": "I Cut Vercel's JSON-Render LLM Costs by 89% Using Toon", "cluster": 13, "x": 6.961249828338623, "y": 3.6216843128204346}, {"id": 46657525, "title": "Git Gandalf (Local LLM\u2013Powered Pre-Commit Code Reviewer)", "cluster": 13, "x": 6.86079216003418, "y": 3.7310192584991455}, {"id": 46656449, "title": "Seen the same LLM prompt break invariants weeks later in prod?", "cluster": 13, "x": 6.8458333015441895, "y": 3.4047603607177734}, {"id": 46655701, "title": "Learning better decision trees \u2013 LLMs as Heuristics for Program Synthesis", "cluster": 13, "x": 6.858911991119385, "y": 3.595649480819702}, {"id": 46655389, "title": "Open Claude Cowork Compatible with Any LLM API on Win/Linux/macOS", "cluster": 13, "x": 6.8865275382995605, "y": 3.815284252166748}, {"id": 46655104, "title": "How Etsy Uses LLMs to Improve Search Relevance", "cluster": 13, "x": 6.830827236175537, "y": 3.4246273040771484}, {"id": 46654147, "title": "Meet the new biologists treating LLMs like aliens", "cluster": 13, "x": 6.539918899536133, "y": 2.982170581817627}, {"id": 46653981, "title": "LLMs Are Lagging Indicators", "cluster": 13, "x": 6.908240795135498, "y": 3.4189696311950684}, {"id": 46653849, "title": "The past, present and future of LLM coding", "cluster": 13, "x": 6.6452436447143555, "y": 3.568372964859009}, {"id": 46653774, "title": "Langauge Modeling, Part 3: Vanilla RNNs", "cluster": 208, "x": 8.553900718688965, "y": 4.464540958404541}, {"id": 46653544, "title": "Floss and Training LLMs", "cluster": 13, "x": 6.648402690887451, "y": 3.263941526412964}, {"id": 46653361, "title": "PAZ O.S. \u2013 A \"Bio-Civic\" Alignment Framework for Ethical LLMs", "cluster": 13, "x": 6.731826305389404, "y": 3.078249216079712}, {"id": 46652944, "title": "Install.md: A standard for LLM-executable installation", "cluster": 13, "x": 6.8672194480896, "y": 3.7772622108459473}, {"id": 46651405, "title": "Cutting LLM token Usage by ~80% using REPL driven document analysis", "cluster": 13, "x": 7.005429744720459, "y": 3.5599112510681152}, {"id": 46650866, "title": "I'm not a good enough engineer to code with LLMs", "cluster": 13, "x": 6.62732458114624, "y": 3.602064371109009}, {"id": 46649853, "title": "RTS for Agents", "cluster": 38, "x": 8.652774810791016, "y": 3.6305501461029053}, {"id": 46649246, "title": "Visualizing the full technology stack of an LLM query [video]", "cluster": 7, "x": 7.005643844604492, "y": 3.2080376148223877}, {"id": 46649419, "title": "Building the Agent Workspace", "cluster": 38, "x": 8.669233322143555, "y": 3.6391210556030273}, {"id": 46649208, "title": "Why LLMs Are Not (Yet) the Silver Bullet for Unstructured Data Processing", "cluster": 13, "x": 6.83493185043335, "y": 3.321100950241089}, {"id": 46649038, "title": "Disproof of Large Language Model Consciousness", "cluster": 208, "x": 8.293810844421387, "y": 4.260989665985107}, {"id": 46647816, "title": "Training large language models on narrow tasks can lead to broad misalignment", "cluster": 208, "x": 8.248924255371094, "y": 4.341139316558838}, {"id": 46647384, "title": "Open Responses \u2013 Interoperable LLM Interfaces Based on the OpenAI Responses API", "cluster": 13, "x": 6.889866352081299, "y": 3.6755213737487793}, {"id": 46646892, "title": "Making (Very) Small LLMs Smarter with RAG", "cluster": 13, "x": 6.792839050292969, "y": 3.28941011428833}, {"id": 46646706, "title": "Histomat of F/OSS: We should reclaim LLMs, not reject them", "cluster": 13, "x": 6.475944519042969, "y": 3.1873528957366943}, {"id": 46646540, "title": "LLM Authorization", "cluster": 13, "x": 6.693205833435059, "y": 3.4657533168792725}, {"id": 46646600, "title": "MCP to Check LLM Prices Right from Claude Code and Cursor", "cluster": 13, "x": 6.9076924324035645, "y": 3.6797678470611572}, {"id": 46646141, "title": "Article 5: The Case Against Code Or: What Evolution Teaches Us About LLMs", "cluster": 13, "x": 6.6275553703308105, "y": 3.6720287799835205}, {"id": 46646195, "title": "Research Papers on SLMs", "cluster": 13, "x": 6.828614711761475, "y": 3.312786102294922}, {"id": 46645311, "title": "Rubenerd: The Rubenerd LLM Licencing Pac", "cluster": 13, "x": 6.796956539154053, "y": 3.484004020690918}, {"id": 46644376, "title": "You Don't Need an ORM", "cluster": 357, "x": 6.651560306549072, "y": 3.2936224937438965}, {"id": 46644129, "title": "I built WalkPrep in two days with an LLM and almost overbuilt it", "cluster": 13, "x": 6.70681095123291, "y": 3.418609380722046}, {"id": 46643608, "title": "Histomat of F/OSS: We should reclaim LLMs, not reject them", "cluster": 13, "x": 6.519352436065674, "y": 3.140089988708496}, {"id": 46643565, "title": "Training large language models on narrow tasks can lead to broad misalignment", "cluster": 208, "x": 8.239882469177246, "y": 4.355107307434082}, {"id": 46642846, "title": "vLLM-MLX \u2013 Run LLMs on Mac at 464 tok/s", "cluster": 13, "x": 7.061087131500244, "y": 3.8037283420562744}, {"id": 46642736, "title": "Open-source specification for building multi-provider LLM interfaces", "cluster": 13, "x": 6.877553462982178, "y": 3.711419105529785}, {"id": 46642825, "title": "How to Speak LLM", "cluster": 13, "x": 6.6546149253845215, "y": 3.2790424823760986}, {"id": 46640373, "title": "Pools of Extraction: How I Hack on Software Projects with LLMs (2025)", "cluster": 13, "x": 6.5992512702941895, "y": 3.60079288482666}, {"id": 46637295, "title": "Are open source maintainers going to be the main sufferers from LLM", "cluster": 13, "x": 6.623939514160156, "y": 3.5588080883026123}, {"id": 46636943, "title": "All LLMs Must Shut the Hell Up", "cluster": 13, "x": 6.526427268981934, "y": 3.1049623489379883}, {"id": 46636530, "title": "Solving TaxCalcBench: LLMs solve taxes by navigating IRS taxcode like a codebase [pdf]", "cluster": 13, "x": 6.683889865875244, "y": 3.661565065383911}, {"id": 46635309, "title": "LLM Structured Outputs Handbook", "cluster": 13, "x": 6.931123733520508, "y": 3.5044586658477783}, {"id": 46635426, "title": "A benchmark for LLM vericoding: formally verified program synthesis", "cluster": 13, "x": 6.957497596740723, "y": 3.483003854751587}, {"id": 46634387, "title": "When the LLM Programs Its Own Thinking", "cluster": 13, "x": 6.636453151702881, "y": 3.103766679763794}, {"id": 46634049, "title": "Training large language models on narrow tasks can lead to broad misalignment", "cluster": 208, "x": 8.248499870300293, "y": 4.3452630043029785}, {"id": 46634035, "title": "Can LLMs Express Their Uncertainty? Not Really", "cluster": 13, "x": 6.605323314666748, "y": 2.9857735633850098}, {"id": 46633391, "title": "The Imitation Game: Using LLMs as Chatbots to Combat Chat-Based Cybercrimes", "cluster": 13, "x": 6.587372303009033, "y": 3.269761323928833}, {"id": 46632278, "title": "Research Papers Defining the SLM Revolution", "cluster": 13, "x": 6.679662704467773, "y": 3.220518112182617}, {"id": 46630867, "title": "What I Tell Colleagues About Using LLMs for Engineering", "cluster": 13, "x": 6.669466972351074, "y": 3.3544187545776367}, {"id": 46630471, "title": "Chat is the least interesting interface to LLMs", "cluster": 13, "x": 6.54300594329834, "y": 3.3382983207702637}, {"id": 46626441, "title": "Vm0", "cluster": 13, "x": 7.147339820861816, "y": 3.9559197425842285}, {"id": 46623729, "title": "Prompt Repetition Improves Non-Reasoning LLMs", "cluster": 13, "x": 6.8085832595825195, "y": 3.067497968673706}, {"id": 46622241, "title": "Chatperone \u2013 LLM chatbots with full parental controls", "cluster": 13, "x": 6.608883857727051, "y": 3.3723485469818115}, {"id": 46621220, "title": "Chat is the least interesting interface to LLMs", "cluster": 13, "x": 6.582571983337402, "y": 3.3756229877471924}, {"id": 46620338, "title": "How to Use LLMs for Continuous, Creative Code Refactoring", "cluster": 13, "x": 6.784300327301025, "y": 3.739823818206787}, {"id": 46619611, "title": "Nature: Training LLM's on narrow tasks can lead to broad misalignment", "cluster": 13, "x": 6.713370323181152, "y": 3.1158604621887207}, {"id": 46619197, "title": "High-Performance LLM Inference", "cluster": 13, "x": 7.122595310211182, "y": 3.467681646347046}, {"id": 46618781, "title": "Local LLMs are how nerds now justify a big computer they don't need", "cluster": 13, "x": 6.389857292175293, "y": 3.3596904277801514}, {"id": 46615169, "title": "Parallel Primitives for Multi-Agent Workflows", "cluster": 38, "x": 8.546977996826172, "y": 3.776705741882324}, {"id": 46613853, "title": "Prompt Repetition Improves Non-Reasoning LLMs", "cluster": 13, "x": 6.802338600158691, "y": 3.073859214782715}, {"id": 46613982, "title": "Let LLMs Visualize Data for You", "cluster": 13, "x": 6.983188629150391, "y": 3.568450927734375}, {"id": 46613997, "title": "LLMs are a 400-year-long confidence trick", "cluster": 13, "x": 6.590687274932861, "y": 3.0714545249938965}, {"id": 46613546, "title": "Agent Skills are now available in Google Antigravity", "cluster": 35, "x": 8.493014335632324, "y": 3.8204050064086914}, {"id": 46611256, "title": "Bottom-up programming as the root of LLM dev skepticism", "cluster": 13, "x": 6.535993576049805, "y": 3.413320779800415}, {"id": 46611176, "title": "Terry Tao: \"LLMs Are Simpler Than You Think \u2013 The Real Mystery Is Why They Work\" [video]", "cluster": 7, "x": 6.761411190032959, "y": 3.0566253662109375}, {"id": 46611100, "title": "What will enshittification of LLMs look like?", "cluster": 13, "x": 6.765523910522461, "y": 3.225003480911255}, {"id": 46609591, "title": "The insecure evangelism of LLM maximalists", "cluster": 13, "x": 6.528055667877197, "y": 3.0709879398345947}, {"id": 46608904, "title": "Thinking about the people who shouldn't use LLMs", "cluster": 13, "x": 6.510589122772217, "y": 3.116100788116455}, {"id": 46608306, "title": "NFR Breaking Shannon Entropy with Instance-Specific LSTM,Arithmetic Coding", "cluster": 13, "x": 6.519673824310303, "y": 3.732308864593506}, {"id": 46603800, "title": "AxonFlow \u2013 a control plane for production LLM and agent workflows", "cluster": 13, "x": 6.909740924835205, "y": 3.3153254985809326}, {"id": 46602737, "title": "vLLM large scale serving: DeepSeek 2.2k tok/s/h200 with wide-ep", "cluster": 13, "x": 7.13650369644165, "y": 3.8501176834106445}, {"id": 46602218, "title": "Five Practical Lessons for Serving Models with Triton Inference Server", "cluster": 13, "x": 7.2056732177734375, "y": 3.344191074371338}, {"id": 46601078, "title": "How vLLM Delivers High Throughput LLM Serving - An Engineer\u2019s View", "cluster": 13, "x": 6.804709434509277, "y": 3.5045719146728516}, {"id": 46600866, "title": "Stuck Between Ambiguous and Essential: How LLMs Model Words", "cluster": 13, "x": 6.6932854652404785, "y": 3.170320749282837}, {"id": 46600459, "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers", "cluster": 13, "x": 7.000397205352783, "y": 3.839700937271118}, {"id": 46600593, "title": "llms .py \u2013 Extensible OSS ChatGPT UI, RAG, Tool Calling, Image/Audio Gen", "cluster": 13, "x": 6.915769577026367, "y": 3.909029960632324}, {"id": 46600305, "title": "Three LLMs in a Trenchcoat", "cluster": 13, "x": 6.600130558013916, "y": 3.1681785583496094}, {"id": 46599666, "title": "How General Counsel Can Operationalise AIVO Inside Legal Workflows", "cluster": 38, "x": 8.596538543701172, "y": 3.617469549179077}, {"id": 46599467, "title": "Sandbox your LLM agent \u2013 Vibekit", "cluster": 13, "x": 6.672739028930664, "y": 3.4196255207061768}, {"id": 46599044, "title": "LLM powered data structures: A lock-free binary search tree", "cluster": 13, "x": 6.8637261390686035, "y": 3.5715646743774414}, {"id": 46598022, "title": "How General Counsel Can Operationalise AIVO Inside Legal Workflows", "cluster": 38, "x": 8.601418495178223, "y": 3.5910604000091553}, {"id": 46597962, "title": "Nailing Jell-O to the Wall, Again. Why China Will Struggle to Contain LLMs", "cluster": 13, "x": 6.5052714347839355, "y": 3.044739007949829}, {"id": 46596965, "title": "Google Releases Gemma Scope 2 to Deepen Understanding of LLM Behavior", "cluster": 13, "x": 6.897059917449951, "y": 3.398545265197754}, {"id": 46596713, "title": "Compare LLM Responses with OverallGPT", "cluster": 13, "x": 6.927757263183594, "y": 3.439943790435791}, {"id": 46596317, "title": "Living with LLMs Everywhere \u2013 How Ambient LLMs Negate Security Policy", "cluster": 13, "x": 6.4588093757629395, "y": 3.3710179328918457}, {"id": 46596185, "title": "Mystery: Why do some LLMs produce more coil noise on Mac Studio M3 Ultra?", "cluster": 13, "x": 6.7042236328125, "y": 3.310120105743408}, {"id": 46595888, "title": "Grounding LLMs with Recursive Code Execution", "cluster": 13, "x": 6.709818363189697, "y": 3.7493977546691895}, {"id": 46593755, "title": "LLMs Are Tools, Not Replacements", "cluster": 13, "x": 6.549651145935059, "y": 3.2228779792785645}, {"id": 46593192, "title": "Linus Torvalds vibe coding (Google's LLMs?)", "cluster": 13, "x": 6.648919105529785, "y": 3.603489637374878}, {"id": 46591809, "title": "When XLA Isn't Enough: From Pallas to VLIW with Splash Attention on TPU", "cluster": 13, "x": 7.176396369934082, "y": 3.7280337810516357}, {"id": 46590949, "title": "I spent my winter break teaching an LLM to play Diplomacy with RL", "cluster": 13, "x": 6.621792316436768, "y": 3.193162441253662}, {"id": 46590280, "title": "TimeCapsuleLLM: LLM trained only on data from 1800-1875", "cluster": 13, "x": 6.8426103591918945, "y": 3.226902961730957}, {"id": 46589637, "title": "Beyond Vector Search: Why LLMs Need Episodic Memory", "cluster": 13, "x": 6.841143608093262, "y": 3.355771064758301}, {"id": 46589386, "title": "Why Ontario Digital Service couldn't procure '98% safe' LLMs (15M Canadians)", "cluster": 13, "x": 6.471630573272705, "y": 3.302232503890991}, {"id": 46588837, "title": "LLVM: The bad parts", "cluster": 13, "x": 6.604626178741455, "y": 3.0958445072174072}, {"id": 46588639, "title": "Traditional NLP is not dead", "cluster": 13, "x": 6.598926067352295, "y": 3.346505641937256}, {"id": 46588312, "title": "IntentGrid \u2013 An LLM benchmark requiring spatial reasoning and 3-step planning", "cluster": 13, "x": 6.89252233505249, "y": 3.3173391819000244}, {"id": 46588337, "title": "Three NHS trusts still using fax machines", "cluster": 70, "x": 6.69370698928833, "y": 3.5292532444000244}, {"id": 46587835, "title": "CreeperVM", "cluster": 13, "x": 6.752400875091553, "y": 3.3810110092163086}, {"id": 46587533, "title": "Beyond Python: Why LLMs Need More Stable, Open Source Code", "cluster": 13, "x": 6.755444049835205, "y": 3.6303954124450684}, {"id": 46587303, "title": "LiteRT \u2013 Google's Edge ML Framework", "cluster": 13, "x": 7.0175957679748535, "y": 3.5461816787719727}, {"id": 46587372, "title": "Quantization and distillation effects on code LLMs", "cluster": 13, "x": 6.696495056152344, "y": 3.734363555908203}, {"id": 46587071, "title": "GeneploreAI/gibberifier: Stun LLMs with random Unicode characters", "cluster": 13, "x": 6.657936096191406, "y": 3.5354065895080566}, {"id": 46587036, "title": "The new biologists treating LLMs like aliens", "cluster": 13, "x": 6.548625469207764, "y": 2.9875752925872803}, {"id": 46585706, "title": "Scope: Hierarchical planner beats LLMs, 55x faster, 1/160k size", "cluster": 13, "x": 6.961987018585205, "y": 3.5283167362213135}, {"id": 46585448, "title": "Universal Commerce Protocol: open standard for agentic commerce", "cluster": 36, "x": 8.523758888244629, "y": 3.608264207839966}, {"id": 46584855, "title": "DroPE: Extending the Context of LLMs by Dropping Their Positional Embeddings", "cluster": 13, "x": 6.875764846801758, "y": 3.310818672180176}, {"id": 46583662, "title": "Universal Commerce Protocol: Open standard for agentic commerce interoperability", "cluster": 36, "x": 8.511457443237305, "y": 3.689241409301758}, {"id": 46583830, "title": "Complete Guide to Agentic Commerce Standards", "cluster": 36, "x": 8.527183532714844, "y": 3.6186861991882324}, {"id": 46583635, "title": "LLVM: The Bad Parts", "cluster": 13, "x": 6.559909343719482, "y": 3.0749893188476562}, {"id": 46583605, "title": "Training an LLM to Play Diplomacy with RL", "cluster": 13, "x": 6.619626998901367, "y": 3.220872163772583}, {"id": 46581433, "title": "When XLA Isn't Enough: From Pallas to VLIW with Splash Attention on TPU", "cluster": 13, "x": 7.133052349090576, "y": 3.73089337348938}, {"id": 46580501, "title": "Embrace your lack: on Pluribus and LLMs", "cluster": 13, "x": 6.502231597900391, "y": 3.144869804382324}, {"id": 46579840, "title": "Survey on integrating large language models with knowledge-based methods (2025)", "cluster": 208, "x": 8.230456352233887, "y": 4.490653991699219}, {"id": 46579179, "title": "LLMs \u2013 Part 1: Tokenization and Embeddings", "cluster": 13, "x": 6.934382438659668, "y": 3.4094326496124268}, {"id": 46579187, "title": "LLMs \u2013 Part 2: Order Matters \u2013 Positional Encoding", "cluster": 13, "x": 6.892553806304932, "y": 3.349777936935425}, {"id": 46578236, "title": "LLVM: The Bad Parts", "cluster": 13, "x": 6.596333026885986, "y": 3.088062047958374}, {"id": 46577249, "title": "Choosing a tech stack in a world where LLMs write all the code", "cluster": 13, "x": 6.6022539138793945, "y": 3.5697855949401855}, {"id": 46576302, "title": "Running LLMs Locally with Docker Model Runner and Python", "cluster": 13, "x": 6.935330867767334, "y": 3.782695770263672}, {"id": 46575889, "title": "A History of Disbelief in Large Language Models", "cluster": 208, "x": 8.226946830749512, "y": 4.349627494812012}, {"id": 46575268, "title": "LLM poetry and the \"greatness\" question: Experiments by Gwern and Mercor", "cluster": 13, "x": 6.6618242263793945, "y": 3.0177040100097656}, {"id": 46575127, "title": "Google: Don't make \"bite-sized\" content for LLMs", "cluster": 13, "x": 6.722742557525635, "y": 3.4035542011260986}, {"id": 46574978, "title": "Will LLMs Help or Hurt New Programming Languages?", "cluster": 13, "x": 6.637581825256348, "y": 3.5895941257476807}, {"id": 46574548, "title": "llcat: /usr/bin/cat for LLMs", "cluster": 13, "x": 6.871175765991211, "y": 3.573059558868408}, {"id": 46573225, "title": "Amazon Redshift AutoWLM and SQA internals plus commentary (and a bit on CSC)", "cluster": 13, "x": 7.296617031097412, "y": 3.642134189605713}, {"id": 46572992, "title": "Jupyter Agents: training LLMs to reason with notebooks", "cluster": 13, "x": 6.8839111328125, "y": 3.31132173538208}, {"id": 46568911, "title": "Agent Native Architectures", "cluster": 38, "x": 8.58089828491211, "y": 3.7282633781433105}, {"id": 46568076, "title": "VLLM or llama.cpp: Choosing the right LLM inference engine for your use case", "cluster": 13, "x": 6.895792484283447, "y": 3.4634993076324463}, {"id": 46566501, "title": "AskChess: An LLM integrated chess coach", "cluster": 13, "x": 6.818265914916992, "y": 3.196549415588379}, {"id": 46566619, "title": "Why users shouldn\u2019t choose their own LLM models", "cluster": 13, "x": 6.570614814758301, "y": 3.1686367988586426}, {"id": 46565409, "title": "LLMs have burned Billions but couldn't build another Tailwind", "cluster": 13, "x": 6.538237571716309, "y": 3.2228217124938965}, {"id": 46564009, "title": "Google: Don't make \"bite-sized\" content for LLMs if you care about search rank", "cluster": 13, "x": 6.778658390045166, "y": 3.462754249572754}, {"id": 46563259, "title": "Digging into the LLM-as-a-Judge Results", "cluster": 13, "x": 6.796020030975342, "y": 3.213893175125122}, {"id": 46561699, "title": "Volcano Model DBMS", "cluster": 13, "x": 7.095300197601318, "y": 3.8699424266815186}, {"id": 46561481, "title": "Training Your Own LLM on a MacBook in 10 Minutes", "cluster": 13, "x": 6.857515335083008, "y": 3.746652364730835}, {"id": 46560206, "title": "Nano-VLLM", "cluster": 13, "x": 7.02182149887085, "y": 3.7938950061798096}, {"id": 46559501, "title": "Artificial Analysis: Independent LLM Evals as a Service", "cluster": 13, "x": 6.892606735229492, "y": 3.024033308029175}, {"id": 46559459, "title": "Digital Red Queen: Adversarial Program Evolution in Core War with LLMs", "cluster": 13, "x": 6.567050457000732, "y": 3.4639909267425537}, {"id": 46559223, "title": "I made a tool to filter LLM API providers by speed, quant, context and more", "cluster": 13, "x": 6.83150577545166, "y": 3.5972959995269775}, {"id": 46557255, "title": "The Ainex Limit: Geometric Proof of LLM Collapse via Recursive Loops", "cluster": 13, "x": 6.731335639953613, "y": 3.1683523654937744}, {"id": 46557393, "title": "Libtree: Ldd as a tree saying why a library is found or not", "cluster": 13, "x": 6.774567604064941, "y": 3.435619592666626}, {"id": 46556726, "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy", "cluster": 13, "x": 6.6519036293029785, "y": 3.1224937438964844}, {"id": 46556526, "title": "LLM's and Smaller, Less Popular Programming Languages", "cluster": 13, "x": 6.776876926422119, "y": 3.6637792587280273}, {"id": 46556262, "title": "Per-query energy consumption of LLMs", "cluster": 13, "x": 6.9436540603637695, "y": 3.4200000762939453}, {"id": 46556236, "title": "Challenges and Research Directions for Large Language Model Inference Hardware", "cluster": 208, "x": 8.22938060760498, "y": 4.477960109710693}, {"id": 46555251, "title": "How Mathematical Modeling Is Transforming Clinical Decision-Making", "cluster": 13, "x": 6.922038555145264, "y": 3.123391628265381}, {"id": 46554745, "title": "The LLM Podcast", "cluster": 13, "x": 6.673905372619629, "y": 3.2132208347320557}, {"id": 46553677, "title": "Agentic Project Management", "cluster": 38, "x": 8.665567398071289, "y": 3.6162986755371094}, {"id": 46553605, "title": "An Optimizing JIT for LLM Tool-Use to Code", "cluster": 13, "x": 6.848086833953857, "y": 3.6588306427001953}, {"id": 46553139, "title": "TuneKit: Fine-Tune SLMs", "cluster": 13, "x": 7.034420490264893, "y": 3.772331953048706}, {"id": 46553119, "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "cluster": 13, "x": 6.847239971160889, "y": 3.317613124847412}, {"id": 46552779, "title": "Digging into the LLM-as-a-Judge Results", "cluster": 13, "x": 6.7601189613342285, "y": 3.2303688526153564}, {"id": 46552721, "title": "A tiny LM that does inference at compile time", "cluster": 13, "x": 7.538398742675781, "y": 3.7903623580932617}, {"id": 46549778, "title": "Testing 2 open-weight models across coding tasks: GLM 4.7 and MiniMax M2.1", "cluster": 208, "x": 8.132040023803711, "y": 4.518044471740723}, {"id": 46549435, "title": "Language Modeling, Part 2: Training Dynamics", "cluster": 208, "x": 8.52313232421875, "y": 4.468991279602051}, {"id": 46549242, "title": "System Design for Production Diffusion LLM Serving with Limited Memory Footprint", "cluster": 13, "x": 6.9939141273498535, "y": 3.63580322265625}, {"id": 46548938, "title": "Writing an LLM from scratch, part 30 \u2013 digging into the LLM-as-a-judge results", "cluster": 13, "x": 6.7662034034729, "y": 3.1890389919281006}, {"id": 46548408, "title": "Running a real consumer app on a 70B LLM at sub-cent cost per scan", "cluster": 13, "x": 7.077829360961914, "y": 3.685537576675415}, {"id": 46546879, "title": "Digital Red Queen: Adversarial Program Evolution in Core War with LLMs", "cluster": 13, "x": 6.467631816864014, "y": 3.4298691749572754}, {"id": 46546871, "title": "How Well Do LLMs Know Double-Entry Accounting?", "cluster": 13, "x": 6.749460697174072, "y": 3.142470121383667}, {"id": 46546365, "title": "LLMs as Judges: Measuring Bias, Hinting Effects, and Tier Preferences", "cluster": 13, "x": 6.693026542663574, "y": 3.0484039783477783}, {"id": 46545587, "title": "Task-free intelligence testing of LLMs", "cluster": 12, "x": 7.027782917022705, "y": 2.649061918258667}, {"id": 46544421, "title": "Large Causal Models from Large Language Models", "cluster": 208, "x": 8.222599983215332, "y": 4.289263725280762}, {"id": 46544419, "title": "Code for Cats \u2013 or how your LLM is a cosplayer", "cluster": 13, "x": 6.6381096839904785, "y": 3.596524953842163}, {"id": 46543741, "title": "Working Around VS Code APIs to Render LLM Suggestions", "cluster": 13, "x": 6.817475318908691, "y": 3.69271183013916}, {"id": 46543555, "title": "What is MLflow? (A good primer.)", "cluster": 13, "x": 7.066246032714844, "y": 3.6440560817718506}, {"id": 46542846, "title": "AI Voice Agent Architecture 101: STT-LLM-TTS and WebSockets?", "cluster": 12, "x": 7.028912544250488, "y": 2.660526990890503}, {"id": 46543021, "title": "LLM agent architectures fail silently as they grow", "cluster": 13, "x": 6.563375473022461, "y": 3.201866626739502}, {"id": 46542761, "title": "Digital Red Queen: Adversarial Program Evolution in Core War with LLMs", "cluster": 13, "x": 6.543540954589844, "y": 3.4954357147216797}, {"id": 46542409, "title": "Language Models as One-Time Teacher for Hierarchical Planning in Text Environs", "cluster": 208, "x": 8.463605880737305, "y": 4.418343544006348}, {"id": 46542287, "title": "LLM Poetry and the \"Greatness\" Question", "cluster": 13, "x": 6.620611190795898, "y": 3.026355504989624}, {"id": 46540807, "title": "Tailwind and open source in the LLM era: when documentation no longer monetizes", "cluster": 13, "x": 6.676285266876221, "y": 3.618928909301758}, {"id": 46540489, "title": "How do language models solve Bayesian network inference?", "cluster": 208, "x": 8.39486026763916, "y": 4.341522693634033}, {"id": 46539967, "title": "Pre-Compiled Headers Being Debated for LLVM/Clang to Speed-Up Build by 1.5~2x", "cluster": 13, "x": 7.235072135925293, "y": 4.002847194671631}, {"id": 46539616, "title": "LLM-feat: Python library for automated feature engineering with Pandas", "cluster": 13, "x": 7.193328857421875, "y": 4.039377212524414}, {"id": 46539155, "title": "Tailwind lays of 75% of their team due to traffic drop to docs because of LLMs", "cluster": 13, "x": 6.599385738372803, "y": 3.1967904567718506}, {"id": 46539050, "title": "Hallucinations in LLMs: What are they and what causes them", "cluster": 13, "x": 6.601938724517822, "y": 3.0097899436950684}, {"id": 46537389, "title": "LLM Guided GPU Kernel Optimization", "cluster": 13, "x": 7.252003192901611, "y": 3.7298853397369385}, {"id": 46537069, "title": "LLM Poetry and the \"Greatness\" Question", "cluster": 13, "x": 6.618017673492432, "y": 3.0235648155212402}, {"id": 46536934, "title": "Persistent Compromise of LLM Agents via Poisoned Experience Retrieval", "cluster": 13, "x": 6.5025105476379395, "y": 3.185905933380127}, {"id": 46536090, "title": "Inside An LLM", "cluster": 13, "x": 6.610431671142578, "y": 3.184250831604004}, {"id": 46534218, "title": "CheckMyLLM \u2013 A real-time \"status board\" for LLM reliability", "cluster": 13, "x": 6.919973373413086, "y": 3.4388771057128906}, {"id": 46532697, "title": "Per-query energy consumption of LLMs", "cluster": 13, "x": 6.937765121459961, "y": 3.4007253646850586}, {"id": 46532411, "title": "LLM from scratch, part 29 \u2013 using DDP to train a base model in the cloud", "cluster": 13, "x": 6.9148173332214355, "y": 3.540036678314209}, {"id": 46532341, "title": "The Silence of the LLaMbs: Getting LLMs to Shut Up", "cluster": 13, "x": 6.509650707244873, "y": 3.094965696334839}, {"id": 46530619, "title": "Prompt optimization can outperform reinforcement learning on LLMs", "cluster": 13, "x": 7.016942024230957, "y": 3.3449442386627197}, {"id": 46529746, "title": "Supercomputer and Better LLM (TinyLM) and Verified Media Generator and Ya", "cluster": 13, "x": 6.925131797790527, "y": 3.651338577270508}, {"id": 46529383, "title": "Implementing NaN Boxing in a Stack-Based VM", "cluster": 13, "x": 6.857296466827393, "y": 3.9610466957092285}, {"id": 46528918, "title": "Task-free intelligence testing of LLMs", "cluster": 12, "x": 7.0259623527526855, "y": 2.6517295837402344}, {"id": 46527858, "title": "Benchmarking 34 LLMs on Nonogram (Logic Puzzle) Solving", "cluster": 13, "x": 7.00537109375, "y": 3.3289613723754883}, {"id": 46527722, "title": "Implementing a (Vibed) LLM Coding Agent in Prolog", "cluster": 13, "x": 6.804973125457764, "y": 3.4991159439086914}, {"id": 46527581, "title": "LLM Problems Observed in Humans", "cluster": 13, "x": 6.605258941650391, "y": 3.046055793762207}, {"id": 46527626, "title": "Why Study CS? Thoughts on LLM-assisted software engineering", "cluster": 13, "x": 6.615821838378906, "y": 3.5808537006378174}, {"id": 46527179, "title": "This blog has generated 1955 pages of blog posts with LLMs it seems", "cluster": 13, "x": 6.777533054351807, "y": 3.4646217823028564}, {"id": 46526423, "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "cluster": 13, "x": 6.395480632781982, "y": 3.2475316524505615}, {"id": 46524777, "title": "VoiceWise \u2013 Understand long voice notes without listening twice", "cluster": 7, "x": 7.076991081237793, "y": 3.2730040550231934}, {"id": 46524693, "title": "LLMs Are Performance-Enhancing Drugs for the Mind", "cluster": 13, "x": 6.643752574920654, "y": 3.111058473587036}, {"id": 46523973, "title": "I Made Visualizing LLM Model Collapse at Gen 20", "cluster": 13, "x": 6.892471790313721, "y": 3.522348642349243}, {"id": 46521869, "title": "Would you pay for audit of your LLM responses", "cluster": 13, "x": 6.788357734680176, "y": 3.3514254093170166}, {"id": 46521446, "title": "Programming is not coding: The cognitive cost of LLM generation", "cluster": 13, "x": 6.687633514404297, "y": 3.514540433883667}, {"id": 46521250, "title": "Paper2md \u2013 convert papers to Markdown to be used for LLM context", "cluster": 13, "x": 6.890397071838379, "y": 3.621882915496826}, {"id": 46519398, "title": "Meaning in Large Language Models: Form vs. Function", "cluster": 208, "x": 8.298240661621094, "y": 4.573711395263672}, {"id": 46518323, "title": "Beyond Full Builds: GPU Optimized LLM Framework with Minimal Executable Programs", "cluster": 13, "x": 7.076876163482666, "y": 3.8487167358398438}, {"id": 46518064, "title": "Mathematical framework 4 cntrling LLM behavior via information-geometric Torsion", "cluster": 13, "x": 6.9704437255859375, "y": 3.2095744609832764}, {"id": 46517331, "title": "Engineering an LLM-Based Data Classifier", "cluster": 13, "x": 6.944507598876953, "y": 3.565178155899048}, {"id": 46515987, "title": "Hierarchical Autoregressive Modeling for Memory-Efficient Language Generation", "cluster": 208, "x": 8.346381187438965, "y": 4.4575653076171875}, {"id": 46515562, "title": "Persistent Backdoor Attacks Under Continual Fine-Tuning of LLMs", "cluster": 13, "x": 6.4602532386779785, "y": 3.3524057865142822}, {"id": 46514034, "title": "An Interactive Guide to How LLMs Generate Text", "cluster": 13, "x": 6.882603168487549, "y": 3.5644774436950684}, {"id": 46513533, "title": "LLM's shouldn't always land the plane", "cluster": 13, "x": 6.54782247543335, "y": 3.097852945327759}, {"id": 46513102, "title": "Measuring LLM Personality: A Quantitative Comparison of GPT-5.2 and Opus 4.5", "cluster": 13, "x": 6.8583807945251465, "y": 3.2518601417541504}, {"id": 46509445, "title": "\"I love you\" \"too\": LLM Attention Explained", "cluster": 13, "x": 6.6678972244262695, "y": 3.1364541053771973}, {"id": 46508696, "title": "LLM Optimized Engineering Principles", "cluster": 13, "x": 6.836945533752441, "y": 3.2803053855895996}, {"id": 46508621, "title": "Can a Large Language Model Understand the Waste Land?", "cluster": 208, "x": 8.220580101013184, "y": 4.342500686645508}, {"id": 46508063, "title": "A Systematic Analysis of Biases in Large Language Models", "cluster": 208, "x": 8.287034034729004, "y": 4.377028942108154}, {"id": 46508139, "title": "The Patient Is Not a Document: Moving from LLMs to a World Model for Oncology", "cluster": 13, "x": 6.614381313323975, "y": 3.1798593997955322}, {"id": 46505325, "title": "Semantic Layer for LLM Apps", "cluster": 13, "x": 6.832071781158447, "y": 3.6416399478912354}, {"id": 46505244, "title": "Eval Testing LLMs in PHPUnit", "cluster": 13, "x": 6.8697967529296875, "y": 3.384371280670166}, {"id": 46505296, "title": "Scientific production in the era of large language models [pdf]", "cluster": 208, "x": 8.246201515197754, "y": 4.534604549407959}, {"id": 46504193, "title": "Evolution Without an Oracle: Driving Effective Evolution with LLM Judges", "cluster": 13, "x": 6.898135662078857, "y": 3.243302822113037}, {"id": 46502956, "title": "LLMs can write code, but they can't design systems", "cluster": 13, "x": 6.575803756713867, "y": 3.5375871658325195}, {"id": 46502989, "title": "Parallel Primitives for Multi-Agent Workflows", "cluster": 38, "x": 8.537163734436035, "y": 3.7477803230285645}, {"id": 46502077, "title": "LLMs Are Becoming an Explanation Layer, Not a Search Replacement", "cluster": 13, "x": 6.686398506164551, "y": 3.335426092147827}, {"id": 46501819, "title": "I feel insulted when an LLM answers the phone", "cluster": 13, "x": 6.514405727386475, "y": 3.1287078857421875}, {"id": 46500652, "title": "What LLMs are really good at", "cluster": 13, "x": 6.617941856384277, "y": 3.154269218444824}, {"id": 46500252, "title": "Critical Views on Large Language Models, an Academic Reading List", "cluster": 208, "x": 8.274406433105469, "y": 4.37324857711792}, {"id": 46499184, "title": "Most LLM conversations are noise: a cheap way to decide what to remember", "cluster": 13, "x": 6.587242603302002, "y": 3.123528242111206}, {"id": 46498622, "title": "Can't Beat BERT\u2013comparing small LLMs and fine-tuned encoders on classification", "cluster": 13, "x": 7.080849647521973, "y": 3.523618698120117}, {"id": 46497873, "title": "Journal-guardian: JournalCTL Watcher with local LLM explanations for errors", "cluster": 13, "x": 6.8357110023498535, "y": 3.5203495025634766}, {"id": 46497700, "title": "First LLM Coded Redis PR Opened by Antirez", "cluster": 13, "x": 6.621994495391846, "y": 3.567340135574341}, {"id": 46497669, "title": "Verification Driven Development \u2013 Avoiding slop with agents", "cluster": 38, "x": 8.580252647399902, "y": 3.7101967334747314}, {"id": 46497548, "title": "ProLLM Leaderboards", "cluster": 13, "x": 7.005475044250488, "y": 3.3547632694244385}, {"id": 46496026, "title": "LLMs Are Currently Not Helpful at All for Math Research: Hamkins", "cluster": 13, "x": 6.683225631713867, "y": 3.1347875595092773}, {"id": 46494714, "title": "What is Agent context engine", "cluster": 38, "x": 8.617544174194336, "y": 3.6604716777801514}, {"id": 46494636, "title": "vLLM: An Efficient Inference Engine for Large Language Models", "cluster": 208, "x": 7.999690055847168, "y": 4.359072208404541}, {"id": 46491439, "title": "Our Setup for A/B Testing LLMs with Millions of Users", "cluster": 13, "x": 6.922426700592041, "y": 3.465744733810425}, {"id": 46489233, "title": "LLMs can't beat the easiest quest in Space Rangers 2", "cluster": 13, "x": 6.602009296417236, "y": 3.1397621631622314}, {"id": 46487923, "title": "LLMs are terrible at charade riddle (French style)", "cluster": 13, "x": 6.58781099319458, "y": 3.129044771194458}, {"id": 46486902, "title": "General plug-and-play inference lib for RLMs", "cluster": 13, "x": 7.462391376495361, "y": 3.8468122482299805}, {"id": 46486336, "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "cluster": 208, "x": 8.435575485229492, "y": 4.280754566192627}, {"id": 46485261, "title": "Physics of Language Models", "cluster": 208, "x": 8.39173698425293, "y": 4.444859027862549}, {"id": 46482899, "title": "Scientific production in the era of large language models", "cluster": 208, "x": 8.222787857055664, "y": 4.348869323730469}, {"id": 46482766, "title": "Mafia Arena: Benchmarking LLMs in the game of mafia", "cluster": 13, "x": 6.871410846710205, "y": 3.231066942214966}, {"id": 46481849, "title": "Scaling Latent Reasoning via Looped Language Models", "cluster": 208, "x": 8.368664741516113, "y": 4.3456130027771}, {"id": 46481013, "title": "Would a Language Model Push You Off a Bridge?", "cluster": 208, "x": 8.379227638244629, "y": 4.440784931182861}, {"id": 46479981, "title": "LLMs will never be alive or intelligent", "cluster": 13, "x": 6.534355163574219, "y": 3.075641632080078}, {"id": 46479320, "title": "The Year of the LLM Desktop", "cluster": 13, "x": 6.6734771728515625, "y": 3.298980712890625}, {"id": 46479100, "title": "Have LLMs destroyed essay mills?", "cluster": 13, "x": 6.5503249168396, "y": 3.152965784072876}, {"id": 46478330, "title": "LLM classication based search engine", "cluster": 13, "x": 6.86110258102417, "y": 3.5668694972991943}, {"id": 46476563, "title": "LLM Sitemap: A New Idea for AI-Readable Site Architecture", "cluster": 12, "x": 7.036230564117432, "y": 2.7574057579040527}, {"id": 46475495, "title": "Can AI Read Your SaaS? Why Llms.txt Beats MCP for Developers", "cluster": 12, "x": 7.028064250946045, "y": 2.627429246902466}, {"id": 46475395, "title": "Recursive Language Models", "cluster": 208, "x": 8.314719200134277, "y": 4.516434669494629}, {"id": 46472213, "title": "Terry Tao: \"LLMs are simpler than you think\"", "cluster": 13, "x": 6.704873561859131, "y": 3.161585569381714}, {"id": 46471238, "title": "Lynkr \u2013 Multi-Provider LLM Proxy", "cluster": 13, "x": 6.790931224822998, "y": 3.606015205383301}, {"id": 46471097, "title": "One line, one agent: LLM-native language NERD goes agent-first", "cluster": 13, "x": 6.691885471343994, "y": 3.4744510650634766}, {"id": 46467239, "title": "The large language model series developed by Qwen", "cluster": 208, "x": 8.171568870544434, "y": 4.437557220458984}, {"id": 46466993, "title": "Online Planning Method Integrating LLMs into Nested Rollout Policy Adaptation", "cluster": 13, "x": 6.844121932983398, "y": 3.45867919921875}, {"id": 46466770, "title": "Using LLMs to compare 500 Pages of Macro Research (with citations)", "cluster": 13, "x": 6.964003086090088, "y": 3.484560489654541}, {"id": 46466368, "title": "Anything: Import anything into Python (generated by LLM)", "cluster": 13, "x": 6.882364749908447, "y": 3.729915142059326}, {"id": 46466318, "title": "Would you use this LLM routing tool?", "cluster": 13, "x": 6.764354705810547, "y": 3.4907474517822266}, {"id": 46466178, "title": "Prompt-only theorem proving with adversarial LLM agents", "cluster": 13, "x": 7.049885272979736, "y": 3.2080681324005127}, {"id": 46466072, "title": "Character-level language diffusion model trained on Tiny Shakespeare", "cluster": 208, "x": 8.316015243530273, "y": 4.330333232879639}, {"id": 46465744, "title": "Firmhive: LLMs as Firmware Experts, a Runtime-Grown Tree-of-Agents Framework", "cluster": 13, "x": 6.928486347198486, "y": 3.4171464443206787}, {"id": 46464313, "title": "Getting Real with LLMs", "cluster": 13, "x": 6.55790901184082, "y": 3.0917532444000244}, {"id": 46463769, "title": "Personalization from Matrix Factorization to LLMs", "cluster": 13, "x": 6.939840793609619, "y": 3.4424166679382324}, {"id": 46460374, "title": "Pelican: Multi-step SVG generation with vLLMs", "cluster": 13, "x": 7.061651229858398, "y": 3.7301673889160156}, {"id": 46459152, "title": "IQuest Coder \u2013 code LLMs for software engineering and competitive programming", "cluster": 13, "x": 6.705033779144287, "y": 3.59767746925354}, {"id": 46459072, "title": "WeDLM: Reconciling Diffusion LM with Standard Causal Attention", "cluster": 13, "x": 7.0572052001953125, "y": 3.2549185752868652}, {"id": 46458368, "title": "Testing Frontier LLMs on Space Rangers 2 Text Quests", "cluster": 13, "x": 6.892238140106201, "y": 3.426265239715576}, {"id": 46457894, "title": "Mock LLM APIs locally with real-world streaming physics", "cluster": 13, "x": 6.816288471221924, "y": 3.6478850841522217}, {"id": 46456682, "title": "Building an internal agent: Code-driven vs. LLM-driven workflows", "cluster": 13, "x": 6.925146579742432, "y": 3.50199818611145}, {"id": 46455487, "title": "Every LLM hallucinates that std:vector deletes elements in LIFO order", "cluster": 13, "x": 6.634925842285156, "y": 3.10429310798645}, {"id": 46455093, "title": "Fixing Linux Kernel Bugs with LLMs", "cluster": 13, "x": 6.857221603393555, "y": 3.6802921295166016}, {"id": 46453670, "title": "An LLM-Driven Multi-Agent Framework for Telescope Proposal Peer Review", "cluster": 13, "x": 6.944160461425781, "y": 3.1985926628112793}, {"id": 46452511, "title": "The Second Great Error Model Convergence", "cluster": 208, "x": 8.198699951171875, "y": 4.281065464019775}, {"id": 46452478, "title": "GraphRouter: A Graph-Based Router for LLM Selections", "cluster": 13, "x": 6.927838325500488, "y": 3.6792593002319336}, {"id": 46452460, "title": "What's the right way to route queries across multiple LLMs?", "cluster": 13, "x": 6.8722615242004395, "y": 3.4654972553253174}, {"id": 46451173, "title": "Towards More Reliable CRM Agent", "cluster": 38, "x": 6.9212212562561035, "y": 3.18809175491333}, {"id": 46450217, "title": "Nerd: A language for LLMs, not humans", "cluster": 13, "x": 6.6385321617126465, "y": 3.1515896320343018}, {"id": 46449366, "title": "I Vibe-Coded a Family Planner for \u00a30. A deep dive into my LLM experiment", "cluster": 13, "x": 6.671107292175293, "y": 3.39166259765625}, {"id": 46448925, "title": "How to Use LLM as a Judge (Without Getting Burned)", "cluster": 13, "x": 6.669084072113037, "y": 3.2684335708618164}, {"id": 46446576, "title": "Building Domain-Specific Small Language Models via Guided Data Generation", "cluster": 208, "x": 8.209671974182129, "y": 4.5987467765808105}, {"id": 46445004, "title": "Notes on Building Agentic Tools Using Local LLMs", "cluster": 13, "x": 6.943620204925537, "y": 3.3495335578918457}, {"id": 46444872, "title": "Exposing LLM-Generated Logical Flaws in Reasoning via Automated Theorem Proving", "cluster": 13, "x": 6.762753963470459, "y": 3.179598808288574}, {"id": 46443861, "title": "LLMRouter: An Open-Source Library for LLM Routing", "cluster": 13, "x": 6.886420726776123, "y": 3.7417733669281006}, {"id": 46442207, "title": "LLM Vision: Visual intelligence for your smart home", "cluster": 13, "x": 6.935662746429443, "y": 3.5574147701263428}, {"id": 46442143, "title": "The Second Great Error Model Convergence", "cluster": 208, "x": 8.19362735748291, "y": 4.259765148162842}, {"id": 46440833, "title": "LLVM AI tool policy: human in the loop", "cluster": 12, "x": 7.015732765197754, "y": 2.624469757080078}, {"id": 46439878, "title": "LLMs for Medical Practice: Look Out", "cluster": 13, "x": 6.641328811645508, "y": 3.2115790843963623}, {"id": 46439325, "title": "Updated LLM Benchmark (Gemini 3 Flash)", "cluster": 13, "x": 6.974297523498535, "y": 3.4975745677948}, {"id": 46438849, "title": "ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-Hoc Teammate Modeling", "cluster": 13, "x": 6.865031719207764, "y": 3.399831771850586}, {"id": 46437861, "title": "Boogiebench: LLM Music Composition with Strudel", "cluster": 13, "x": 6.987277507781982, "y": 3.733029365539551}, {"id": 46436580, "title": "Failing at Using a Local LLM for Vinyl Record Color Extraction", "cluster": 13, "x": 6.732272148132324, "y": 3.4058144092559814}, {"id": 46436429, "title": "We built a guardrail layer to keep LLMs from breaking production databases", "cluster": 13, "x": 6.665122985839844, "y": 3.562596559524536}, {"id": 46435780, "title": "Tool Design Patterns for Agents", "cluster": 38, "x": 8.657825469970703, "y": 3.6279592514038086}, {"id": 46434409, "title": "1M for Non-Specialists", "cluster": 13, "x": 6.75710391998291, "y": 3.3925819396972656}, {"id": 46433452, "title": "LLM Efficiency: From Hyperscale Optimizations to Universal Deployability", "cluster": 13, "x": 6.980722904205322, "y": 3.5582659244537354}, {"id": 46431737, "title": "Large Language Models Struggle to Learn Long-Tail Knowledge (2023)", "cluster": 208, "x": 8.218124389648438, "y": 4.358001232147217}, {"id": 46430838, "title": "Have LLMs improved for Swift coding in the last 12 months?", "cluster": 11, "x": 6.8359527587890625, "y": 3.706070899963379}, {"id": 46430585, "title": "Threads \u2013 A context strategy for humans and LLMs", "cluster": 13, "x": 6.8299665451049805, "y": 3.2346508502960205}, {"id": 46430336, "title": "LLMs have bad taste at where to draw abstraction boundaries", "cluster": 13, "x": 6.714252471923828, "y": 3.1581199169158936}, {"id": 46429900, "title": "Reflections on a Year of Prolog and LLMs", "cluster": 13, "x": 6.779300212860107, "y": 3.1021082401275635}, {"id": 46429827, "title": "Why Enterprises Cannot Disclaim Consumer Harm Caused by LLM \"Optimization\"", "cluster": 13, "x": 6.57078742980957, "y": 3.163947582244873}, {"id": 46427724, "title": "The Second Great Error Model Convergence", "cluster": 208, "x": 8.202601432800293, "y": 4.277094841003418}, {"id": 46426902, "title": "You are absolutely right? \u2013 LLM workflows and thoughts about the future", "cluster": 13, "x": 6.6660261154174805, "y": 3.133552312850952}, {"id": 46426941, "title": "The Emerging Market for Intelligence: Pricing, Supply, and Demand for LLMs", "cluster": 12, "x": 6.831495761871338, "y": 2.9652535915374756}, {"id": 46426062, "title": "How LLMs Work: Top Executive-Level Questions", "cluster": 13, "x": 6.693996906280518, "y": 3.201019763946533}, {"id": 46424136, "title": "LLMs Are Not Fun", "cluster": 13, "x": 6.5272297859191895, "y": 3.0904762744903564}, {"id": 46423597, "title": "Running Cloud LLMs Inside LM Studio", "cluster": 13, "x": 6.789492130279541, "y": 3.642404556274414}, {"id": 46421895, "title": "VLLM: The High-Throughput and Memory-Efficient Serving Engine for LLMs", "cluster": 13, "x": 6.94587516784668, "y": 3.614551544189453}, {"id": 46421332, "title": "Why I'm building my own CLIs for agents", "cluster": 38, "x": 8.674515724182129, "y": 3.56219744682312}, {"id": 46420636, "title": "On LLMs in Programming", "cluster": 13, "x": 6.677852153778076, "y": 3.5791220664978027}, {"id": 46418058, "title": "The Second Great Error Model Convergence", "cluster": 208, "x": 8.195305824279785, "y": 4.281925678253174}, {"id": 46417542, "title": "Reddit, but with multiple LLM agents, works locally", "cluster": 13, "x": 6.698763847351074, "y": 3.4908714294433594}, {"id": 46417439, "title": "Love Letters to Writers and LLMs", "cluster": 13, "x": 6.590406894683838, "y": 3.2120091915130615}, {"id": 46416427, "title": "NPM needs an analog to pnpm's minimumReleaseAge and yarn's npmMinimalAgeGate", "cluster": 13, "x": 6.868740558624268, "y": 3.2993264198303223}, {"id": 46416105, "title": "New LLM Pre-Training and Post-Training Paradigms", "cluster": 13, "x": 6.815312385559082, "y": 3.300161838531494}, {"id": 46414646, "title": "Memelang: Terse SQL uses \"axial grammar\" for LLM generation", "cluster": 13, "x": 6.989575386047363, "y": 3.544748306274414}, {"id": 46413597, "title": "rLLM: Reinforcement Learning for Language Agents", "cluster": 13, "x": 7.137852668762207, "y": 3.399360418319702}, {"id": 46412755, "title": "Automating Deception: Scalable Multi-Turn LLM Jailbreaks", "cluster": 13, "x": 6.725184917449951, "y": 3.577909469604492}, {"id": 46411757, "title": "Local LLMs are how nerds now justify a big computer they don't need", "cluster": 13, "x": 6.387047290802002, "y": 3.358588457107544}, {"id": 46411486, "title": "The Optimal Architecture for Small Language Models", "cluster": 208, "x": 8.172070503234863, "y": 4.43406343460083}, {"id": 46411539, "title": "Designing Predictable LLM-Verifier Systems for Formal Method Guarantee", "cluster": 13, "x": 6.875291347503662, "y": 3.423262596130371}, {"id": 46411278, "title": "On LLMs in Programming", "cluster": 13, "x": 6.676064968109131, "y": 3.5931732654571533}, {"id": 46410153, "title": "Salesforce pulls back from LLMs, pivots Agentforce to deterministic automation", "cluster": 13, "x": 6.692742824554443, "y": 3.1955130100250244}, {"id": 46409162, "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent", "cluster": 208, "x": 8.314776420593262, "y": 4.3613386154174805}, {"id": 46408510, "title": "C \u2013> Java != Java \u2013> LLM", "cluster": 13, "x": 6.748615264892578, "y": 3.609647512435913}, {"id": 46405159, "title": "LLMs in Programming", "cluster": 13, "x": 6.653659820556641, "y": 3.5800797939300537}, {"id": 46403045, "title": "Large Causal Models from Large Language Models", "cluster": 208, "x": 8.21937084197998, "y": 4.285229682922363}, {"id": 46402897, "title": "Prompt Repetition Improves Non-Reasoning LLMs", "cluster": 13, "x": 6.80253267288208, "y": 3.0607235431671143}, {"id": 46402799, "title": "Memelang: Token-efficient LLM query language", "cluster": 13, "x": 6.994201183319092, "y": 3.5635666847229004}, {"id": 46401482, "title": "LLM-powered coding mass-produces technical debt", "cluster": 13, "x": 6.633423328399658, "y": 3.638228416442871}, {"id": 46401438, "title": "Linux kernel community discussion on ML/LLM tools in kernel development", "cluster": 13, "x": 7.0136213302612305, "y": 3.7623891830444336}, {"id": 46400527, "title": "Commandments of LLM Use", "cluster": 13, "x": 6.631978988647461, "y": 3.2551658153533936}, {"id": 46400383, "title": "AIChat: All-in-One LLM CLI Tool", "cluster": 13, "x": 6.875911235809326, "y": 3.7884013652801514}, {"id": 46399696, "title": "Stop benchmarking LLMs. Make them fight", "cluster": 13, "x": 6.575244426727295, "y": 3.188594341278076}, {"id": 46399259, "title": "Finding Where to Compromise with LLM's", "cluster": 13, "x": 6.586806297302246, "y": 3.1400394439697266}, {"id": 46398998, "title": "I documented a local 20B LLM perceiving its hardware state without data access", "cluster": 13, "x": 6.8850321769714355, "y": 3.6207871437072754}, {"id": 46398989, "title": "LLM Conversations Viewer", "cluster": 13, "x": 6.657731056213379, "y": 3.539663076400757}, {"id": 46398315, "title": "Large Causal Models from Large Language Models", "cluster": 208, "x": 8.198686599731445, "y": 4.263943195343018}, {"id": 46396208, "title": "OGhidra: Automating dataflow analysis and vulnerability discovery via local LLMs", "cluster": 13, "x": 6.574465751647949, "y": 3.459850311279297}, {"id": 46395565, "title": "LLM Sycophancy: The Risk of Vulnerable Misguidance in AI Medical Advice", "cluster": 12, "x": 6.892971038818359, "y": 2.735701322555542}, {"id": 46394181, "title": "Debaite: Tool for multiple LLM models to refine ideas by arguing with each other", "cluster": 13, "x": 6.941696643829346, "y": 3.344944477081299}, {"id": 46390624, "title": "We compare how humans and LLMs form judgments across 7 epistemological stages", "cluster": 13, "x": 6.733737945556641, "y": 3.1150147914886475}, {"id": 46390114, "title": "The Optimal Architecture for Small Language Models", "cluster": 208, "x": 8.169456481933594, "y": 4.426331996917725}, {"id": 46388716, "title": "The changing drivers of LLM adoption", "cluster": 13, "x": 6.62993860244751, "y": 3.138941764831543}, {"id": 46387034, "title": "A framework for technical writing in the age of LLMs", "cluster": 13, "x": 6.570626258850098, "y": 3.525007963180542}, {"id": 46386509, "title": "LLMs from Scratch Using Middle School Math \u2013 TDS Archive", "cluster": 13, "x": 6.887047290802002, "y": 3.395888090133667}, {"id": 46385865, "title": "LLM Learning Resources", "cluster": 13, "x": 6.8006205558776855, "y": 3.3584272861480713}, {"id": 46385577, "title": "Observability dashboard for an arbitrary LLM langgraph", "cluster": 13, "x": 7.058929443359375, "y": 3.674764633178711}, {"id": 46383550, "title": "Agent-Skills-for-Context-Engineering", "cluster": 38, "x": 8.682169914245605, "y": 3.6446053981781006}, {"id": 46379368, "title": "LLM-API-Key-Proxy: Universal LLM Gateway: One API, Every LLM", "cluster": 13, "x": 6.781565189361572, "y": 3.6970059871673584}, {"id": 46379323, "title": "The Case Against LLMs as Rerankers", "cluster": 13, "x": 6.555205821990967, "y": 3.0667972564697266}, {"id": 46378704, "title": "Ask HN: Where do deterministic rules break down for LLM guardrails?", "cluster": 13, "x": 6.631890296936035, "y": 3.156012773513794}, {"id": 46377995, "title": "A Scalable Communication Protocol for Networks of Large Language Models", "cluster": 208, "x": 8.188359260559082, "y": 4.454216480255127}, {"id": 46377730, "title": "Can LLMs stop when producing any output violates their own rules?", "cluster": 13, "x": 6.585703372955322, "y": 3.218660354614258}, {"id": 46377537, "title": "Microsoft Agent Framework", "cluster": 37, "x": 8.520566940307617, "y": 3.7169995307922363}, {"id": 46375306, "title": "Animated LLM \u2013 Understand the Mechanics of LLMs", "cluster": 13, "x": 6.80519962310791, "y": 3.2493081092834473}, {"id": 46375236, "title": "A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems", "cluster": 13, "x": 6.4702348709106445, "y": 3.2815024852752686}, {"id": 46373614, "title": "How to safely let LLMs query your databases via sandboxed materialized views", "cluster": 13, "x": 6.659823417663574, "y": 3.5615386962890625}, {"id": 46372466, "title": "Do we need semantic layer anymore? What are the limitations of LLMs?", "cluster": 13, "x": 6.753539085388184, "y": 3.3416531085968018}, {"id": 46370272, "title": "Useful Agentic Workflows", "cluster": 38, "x": 8.634637832641602, "y": 3.6512725353240967}, {"id": 46369663, "title": "Why Custom Evals Matter for Production LLMs", "cluster": 13, "x": 6.779615879058838, "y": 3.244666576385498}, {"id": 46367690, "title": "I used RL fine-tuning to make an LLM generate ugly and unpythonic FizzBuzz code", "cluster": 13, "x": 6.744555950164795, "y": 3.7142233848571777}, {"id": 46367468, "title": "Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Structuring", "cluster": 13, "x": 6.943241119384766, "y": 3.341064691543579}, {"id": 46366197, "title": "LLM Inference Performance Benchmarking from Scratch", "cluster": 13, "x": 7.139009952545166, "y": 3.46631121635437}, {"id": 46366004, "title": "Memelang: An Axial Grammar for LLM-Generated Vector-Relational Queries", "cluster": 13, "x": 7.049598693847656, "y": 3.59476637840271}, {"id": 46365226, "title": "LoPA: Scaling Diffusion LLM Single-Sample Throughput to 1000 TPS", "cluster": 13, "x": 7.074206352233887, "y": 3.456606388092041}, {"id": 46362829, "title": "Memelang: An Axial Grammar for LLM-Generated Vector-Relational Queries", "cluster": 13, "x": 7.078341484069824, "y": 3.5769948959350586}, {"id": 46362317, "title": "DataFlow: Using LLMs to Build Reproducible, End-to-End Data Pipelines", "cluster": 13, "x": 7.017199993133545, "y": 3.5600569248199463}, {"id": 46360815, "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe", "cluster": 13, "x": 7.0384979248046875, "y": 3.6006765365600586}, {"id": 46359812, "title": "Speculative Decoding in LLMs", "cluster": 13, "x": 6.876705646514893, "y": 3.211981773376465}, {"id": 46358172, "title": "Best way to annotate large parquet LLM logs without full rewrites?", "cluster": 13, "x": 7.083198070526123, "y": 3.6282174587249756}, {"id": 46357127, "title": "Intent: An LLM-Powered Reranker Library That Explains Itself", "cluster": 13, "x": 6.918642997741699, "y": 3.380207061767578}, {"id": 46356415, "title": "Metamind \u2013 Cognitive architecture for LLM agents", "cluster": 13, "x": 6.968466758728027, "y": 2.895151138305664}, {"id": 46354970, "title": "Scaling LLMs to Larger Codebases", "cluster": 13, "x": 6.82730770111084, "y": 3.626718521118164}, {"id": 46354773, "title": "On the Existence, Impact, and Origin of Hallucination-Associated Neurons in LLMs", "cluster": 13, "x": 6.605705738067627, "y": 2.9817798137664795}, {"id": 46354701, "title": "A Unified CLI Tool for All Your LLMs That Promises Improved UX", "cluster": 13, "x": 6.887943744659424, "y": 3.7112233638763428}, {"id": 46354787, "title": "Comparing language model performance on creative writing transformations", "cluster": 208, "x": 8.299562454223633, "y": 4.4240312576293945}, {"id": 46352620, "title": "The Community Stories of VLLM and SGL", "cluster": 13, "x": 6.735296249389648, "y": 3.1418395042419434}, {"id": 46351905, "title": "Reverse Engineering Hyperliquid", "cluster": 13, "x": 6.686044692993164, "y": 3.0227506160736084}, {"id": 46351578, "title": "LLM Communications in the Wild", "cluster": 13, "x": 6.6876935958862305, "y": 3.4197468757629395}, {"id": 46351560, "title": "Token-Oriented Object Notation: A compact, human-readable JSON encoding for LLMs", "cluster": 13, "x": 6.922743320465088, "y": 3.685513496398926}, {"id": 46351353, "title": "DGX-Spark-Finetune-LLM", "cluster": 13, "x": 7.117171287536621, "y": 3.9183132648468018}, {"id": 46349171, "title": "LVM Thin Provisioning (2016)", "cluster": 13, "x": 7.027797698974609, "y": 3.7190165519714355}, {"id": 46347705, "title": "500 Hours of vibe coding: LLMs fight my coding standards at every turn", "cluster": 13, "x": 6.581329345703125, "y": 3.556472063064575}, {"id": 46346027, "title": "Is resumable LLM streaming hard? No, it's just annoying", "cluster": 13, "x": 6.857110500335693, "y": 3.651036262512207}, {"id": 46344728, "title": "Let's discuss: future of refactoring in the era of LLMs", "cluster": 13, "x": 6.625214099884033, "y": 3.499115228652954}, {"id": 46343822, "title": "File-based persistent memory for LLM assistants", "cluster": 13, "x": 6.860257148742676, "y": 3.5684773921966553}, {"id": 46343301, "title": "Current LLM tooling makes understanding optional", "cluster": 13, "x": 6.742795944213867, "y": 3.3672564029693604}, {"id": 46342692, "title": "Former DeepMind engineer (possibly) experiences LLM pyschosis", "cluster": 13, "x": 6.556319236755371, "y": 3.0475010871887207}, {"id": 46340078, "title": "Advanced Tools \u2013 Bringing Anthropic's advanced tool use to any LLM provider", "cluster": 13, "x": 6.879870414733887, "y": 3.353440999984741}, {"id": 46339950, "title": "Claude (Agent) skill to audit waiting UX", "cluster": 34, "x": 7.857213497161865, "y": 4.553598880767822}, {"id": 46338960, "title": "Up to date free LLM resources", "cluster": 13, "x": 6.815809726715088, "y": 3.5722224712371826}, {"id": 46337056, "title": "Single Board Module for Local LLM", "cluster": 13, "x": 6.848745822906494, "y": 3.588167428970337}, {"id": 46336990, "title": "LLM Benchmark: Frontier models now statistically indistinguishable", "cluster": 13, "x": 6.886728286743164, "y": 3.285588502883911}, {"id": 46336455, "title": "Has anyone tried quantum randomness to drive temp functions for LLMs?", "cluster": 13, "x": 6.8967156410217285, "y": 3.242492437362671}, {"id": 46334582, "title": "Self Evolving Live SWE Agent", "cluster": 38, "x": 8.629530906677246, "y": 3.6353671550750732}, {"id": 46334562, "title": "Open Source Historical LLM trained exclusively on 19th century text", "cluster": 13, "x": 6.8065667152404785, "y": 3.484339952468872}, {"id": 46334365, "title": "Understanding Conversational AI: Philosophy, Ethics, and Social Impact of LLMs", "cluster": 12, "x": 6.99337911605835, "y": 2.6306543350219727}, {"id": 46333414, "title": "Generating DSL Code with LLMs", "cluster": 13, "x": 6.789073467254639, "y": 3.7454488277435303}, {"id": 46331877, "title": "We ran Anthropic\u2019s interviews through structured LLM analysis", "cluster": 13, "x": 6.78334379196167, "y": 3.1617300510406494}, {"id": 46331455, "title": "Braid: Bounded reasoning for LLMs using symbolic Mermaid graphs", "cluster": 13, "x": 6.955617904663086, "y": 3.2087814807891846}, {"id": 46331532, "title": "A \"Ready-to-Use\" Template for LLVM Out-of-Tree Passes", "cluster": 13, "x": 6.943469047546387, "y": 3.7930314540863037}, {"id": 46330726, "title": "LLM Year in Review", "cluster": 13, "x": 6.706441879272461, "y": 3.2304108142852783}, {"id": 46330513, "title": "CAD: Disaggregating Core Attention for Efficient Long-Context LLM Training", "cluster": 13, "x": 6.933216571807861, "y": 3.5247364044189453}, {"id": 46330472, "title": "Building an LLM evaluation framework: best practices", "cluster": 13, "x": 6.859172821044922, "y": 3.3446080684661865}, {"id": 46329940, "title": "AI's Unpaid Debt: How LLM Scrapers Destroy the Social Contract of Open Source", "cluster": 13, "x": 6.614067077636719, "y": 3.4031310081481934}, {"id": 46328855, "title": "CTO bench: an online LLM coding benchmark", "cluster": 13, "x": 6.739731311798096, "y": 3.583829164505005}, {"id": 46328463, "title": "Agent Ledger \u2013 A transaction ledger for reasoning systems", "cluster": 36, "x": 8.550983428955078, "y": 3.579561471939087}, {"id": 46325860, "title": "Training and Evaluating LLMs as General-Purpose Activation Explainers", "cluster": 13, "x": 6.851104736328125, "y": 3.2378435134887695}, {"id": 46325561, "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "cluster": 208, "x": 8.181009292602539, "y": 4.4105024337768555}, {"id": 46324080, "title": "Evaluating Large Language Models in Scientific Discovery", "cluster": 208, "x": 8.260053634643555, "y": 4.329237461090088}, {"id": 46322869, "title": "The Price of Intelligence \u2013 Three Risks Inherent in LLMs", "cluster": 12, "x": 6.884496688842773, "y": 2.8412587642669678}, {"id": 46321965, "title": "Agent Skills (Open Standard)", "cluster": 38, "x": 8.614006996154785, "y": 3.669299364089966}, {"id": 46320448, "title": "Understanding Encoder and Decoder LLMs", "cluster": 13, "x": 6.868141174316406, "y": 3.4462313652038574}, {"id": 46319888, "title": "LLM-Interview-Questions-and-Answers: 100 LLM interview questions with answers", "cluster": 13, "x": 6.84353494644165, "y": 3.277250289916992}, {"id": 46319826, "title": "History LLMs: Models trained exclusively on pre-1913 texts", "cluster": 13, "x": 6.8341898918151855, "y": 3.3293943405151367}, {"id": 46318930, "title": "LionsOS Design, Implementation and Performance", "cluster": 13, "x": 7.081809043884277, "y": 3.8229787349700928}, {"id": 46318890, "title": "LLMs' impact on science: Booming publications, stagnating quality", "cluster": 13, "x": 6.733326435089111, "y": 3.1973021030426025}, {"id": 46318352, "title": "Geerling: Apple didn't have to go this hard - Testing LLMs using RDMA [video]", "cluster": 7, "x": 6.827109336853027, "y": 3.2374284267425537}, {"id": 46317702, "title": "We can't measure LLM reasoning because LLMs don't inhabit a world", "cluster": 13, "x": 6.700139999389648, "y": 3.1354222297668457}, {"id": 46313044, "title": "Running a full voice stack (ASR \u2013> LLM \u2013> TTS) locally with Docker", "cluster": 13, "x": 6.945997714996338, "y": 3.91361665725708}, {"id": 46310755, "title": "DCMS Pheix \u2013 First RTM Release \u00abfrom the Basement\u00bb", "cluster": 13, "x": 7.056423187255859, "y": 3.970766067504883}, {"id": 46310295, "title": "Hetzner AX102 Review: Why DB Need Enterprise NVMe \u2013 PLP and Fsync Performance", "cluster": 13, "x": 7.193875789642334, "y": 3.879470109939575}, {"id": 46309331, "title": "AI Playground - free comparison and test lab for LLMs(GPT, Claude, Gemini, more)", "cluster": 12, "x": 7.018310546875, "y": 2.6468448638916016}, {"id": 46307928, "title": "Mini-SGLang: A lightweight yet high-performance inference framework for LLM", "cluster": 13, "x": 7.026184558868408, "y": 3.5093417167663574}, {"id": 46305875, "title": "Opus 1.6 Audio Codec Adds New Machine Learning Functionality", "cluster": 67, "x": 7.5424113273620605, "y": 4.303781032562256}, {"id": 46304858, "title": "Learn Lisp/Fennel Programming Against Neovim", "cluster": 13, "x": 7.136354446411133, "y": 4.123437881469727}, {"id": 46303744, "title": "Anchored Diffusion Language Model", "cluster": 208, "x": 8.4564208984375, "y": 4.293633937835693}, {"id": 46303211, "title": "Decompiling the Synergy: Human\u2013LLM Teaming in Reverse Engineering [pdf]", "cluster": 13, "x": 6.810349941253662, "y": 3.2680959701538086}, {"id": 46302558, "title": "Building a Security Scanner for LLM Apps", "cluster": 13, "x": 6.6147637367248535, "y": 3.543773651123047}, {"id": 46301811, "title": "4D LLM - Describe Anything, Anywhere, at Any Moment", "cluster": 13, "x": 6.752035617828369, "y": 3.2676503658294678}, {"id": 46301136, "title": "Can large language models generalize analogy solving like children can? [pdf]", "cluster": 208, "x": 8.367358207702637, "y": 4.348538875579834}, {"id": 46300788, "title": "Reinforcement Learning Infrastructure for LLM Agents", "cluster": 13, "x": 6.933948993682861, "y": 3.203225612640381}, {"id": 46300661, "title": "Prompt caching: 10x cheaper LLM tokens", "cluster": 13, "x": 7.051429271697998, "y": 3.568553924560547}, {"id": 46300034, "title": "Let's (Not) Just Put Things in Context: Test-Time Training for Long-Context LLMs", "cluster": 13, "x": 6.769382476806641, "y": 3.2103757858276367}, {"id": 46298555, "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "cluster": 13, "x": 6.391871929168701, "y": 3.237900972366333}, {"id": 46298492, "title": "New Ways to Corrupt LLMs", "cluster": 13, "x": 6.420997619628906, "y": 3.201317310333252}, {"id": 46298332, "title": "Minimum Viable Benchmark (For Evaluating LLMs)", "cluster": 13, "x": 6.914627552032471, "y": 3.3777377605438232}, {"id": 46297015, "title": "LLM Hypercompetence", "cluster": 13, "x": 6.874233722686768, "y": 3.348815679550171}, {"id": 46296791, "title": "LLM Pricing Calculator", "cluster": 13, "x": 6.9375176429748535, "y": 3.2940776348114014}, {"id": 46296298, "title": "Making LLMs Useful", "cluster": 13, "x": 6.7212138175964355, "y": 3.3631396293640137}, {"id": 46296046, "title": "LLMs Excel at Easy Verification Problems", "cluster": 13, "x": 6.8427510261535645, "y": 3.32867693901062}, {"id": 46295910, "title": "I had a private chat with an LLM", "cluster": 13, "x": 6.5798659324646, "y": 3.182680606842041}, {"id": 46295838, "title": "Learning a new programming language with an LLM", "cluster": 13, "x": 6.6342315673828125, "y": 3.5793752670288086}, {"id": 46294558, "title": "The persistent need for general knowledge and subject-matter expertise with LLMs", "cluster": 13, "x": 6.6476240158081055, "y": 3.1402478218078613}, {"id": 46290934, "title": "GeminiJack: A prompt-injection challenge demonstrating real-world LLM abuse", "cluster": 13, "x": 6.526488304138184, "y": 3.2341055870056152}, {"id": 46290620, "title": "Prompt caching for cheaper LLM tokens", "cluster": 13, "x": 6.889626979827881, "y": 3.4891011714935303}, {"id": 46290146, "title": "Would you pay for a tool that executes LLM code to verify it?", "cluster": 13, "x": 6.647385120391846, "y": 3.620342969894409}, {"id": 46290040, "title": "Generative Engine Optimization (GEO): A technical blueprint for ranking in LLMs", "cluster": 13, "x": 6.994571685791016, "y": 3.516951084136963}, {"id": 46290020, "title": "Is resumable LLM streaming hard? No, it's just annoying, but we built it anyway.", "cluster": 13, "x": 6.7969970703125, "y": 3.61279034614563}, {"id": 46289799, "title": "Beaver: An Efficient Deterministic LLM Verifier", "cluster": 13, "x": 6.847213268280029, "y": 3.4556126594543457}, {"id": 46289662, "title": "Local-first analysis of a million genetic traits with LLM-assisted exploration", "cluster": 13, "x": 7.097431182861328, "y": 3.4514150619506836}, {"id": 46289611, "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "cluster": 13, "x": 6.699368953704834, "y": 3.714695930480957}, {"id": 46289355, "title": "Full AI Voice Agent (Whisper and 700M LLM and NeuTTS) running offline [video]", "cluster": 3, "x": 6.923795223236084, "y": 2.7508254051208496}, {"id": 46288616, "title": "NuggetBench: Can LLMs see when chicken nuggets look like geographical areas?", "cluster": 13, "x": 6.929784297943115, "y": 3.4771945476531982}, {"id": 46288111, "title": "Open-source LLM cascading, up to 92% cost savings on benchmarks", "cluster": 13, "x": 6.980674743652344, "y": 3.592405319213867}, {"id": 46288219, "title": "LLM-friendly PR review workflows in your CLI", "cluster": 13, "x": 6.8687663078308105, "y": 3.5726356506347656}, {"id": 46288014, "title": "Vectorized MAXSCORE over WAND, especially for long LLM-generated queries", "cluster": 13, "x": 7.030377388000488, "y": 3.495450735092163}, {"id": 46287952, "title": "Multi-Agent MCP Skillset Architecture", "cluster": 38, "x": 8.606446266174316, "y": 3.666806936264038}, {"id": 46287739, "title": "I asked an LLM to teach me Clojure", "cluster": 13, "x": 6.664549827575684, "y": 3.5673835277557373}, {"id": 46287626, "title": "Detailed balance in large language model-driven agents", "cluster": 208, "x": 8.221940994262695, "y": 4.291323184967041}, {"id": 46287284, "title": "LLMs Are Left-Leaning Liberals: The Hidden Political Bias of LLMs", "cluster": 13, "x": 6.537768363952637, "y": 3.047844886779785}, {"id": 46287119, "title": "Large language models are not about natural language", "cluster": 208, "x": 8.240352630615234, "y": 4.294933319091797}, {"id": 46286652, "title": "I ported JustHTML from Python to JavaScript with LLMs in 4.5 hours", "cluster": 13, "x": 6.814173698425293, "y": 3.7685978412628174}, {"id": 46285260, "title": "Enhancing LLMs with LoRA \u2013 Standardized Recipes for Capability Enhancement", "cluster": 13, "x": 6.9010210037231445, "y": 3.5394184589385986}, {"id": 46284510, "title": "Language modeling with high school math background", "cluster": 208, "x": 8.588733673095703, "y": 4.478337287902832}, {"id": 46284548, "title": "Klarna Launches Agentic Product Protocol", "cluster": 36, "x": 8.494366645812988, "y": 3.6787126064300537}, {"id": 46281739, "title": "Do LLMs Understand? AI Pioneer Yann LeCun Spars with DeepMind's Adam Brown [video]", "cluster": 12, "x": 6.923692226409912, "y": 2.661447048187256}, {"id": 46280394, "title": "Yann LeCun on why LLMs and AGI are \"total BS\" [video]", "cluster": 7, "x": 6.760993003845215, "y": 2.944383382797241}, {"id": 46279856, "title": "Detailed balance in large language model-driven agents", "cluster": 208, "x": 8.223945617675781, "y": 4.290825366973877}, {"id": 46278984, "title": "Learning a new programming language with an LLM", "cluster": 13, "x": 6.6699604988098145, "y": 3.6022183895111084}, {"id": 46277547, "title": "A public API for LLM response time tracking", "cluster": 13, "x": 6.894960880279541, "y": 3.6099536418914795}, {"id": 46277256, "title": "Deciduous: Better programming with LLMs using a living memory and decision graph", "cluster": 13, "x": 6.8659749031066895, "y": 3.470240831375122}, {"id": 46276757, "title": "I made RSS better with Obsidian and summaries powered by my local LLM", "cluster": 13, "x": 6.757644176483154, "y": 3.7483651638031006}, {"id": 46276821, "title": "External Memory and Governance for Human\u2013LLM Collaboration", "cluster": 13, "x": 6.842422008514404, "y": 3.0872104167938232}, {"id": 46276575, "title": "LLM guidelines, do you have the same pblm", "cluster": 13, "x": 6.698632717132568, "y": 3.2243945598602295}, {"id": 46275412, "title": "Llmwalk: Explore the answer-space of open LLMs", "cluster": 13, "x": 6.83113431930542, "y": 3.319700241088867}, {"id": 46272862, "title": "Olmo 3 and the Open LLM Renaissance", "cluster": 13, "x": 6.720967769622803, "y": 3.3771657943725586}, {"id": 46272917, "title": "When LICM Fails Us", "cluster": 13, "x": 6.509219646453857, "y": 3.1283833980560303}, {"id": 46272695, "title": "Playbook-Based Agent Reference Architecture", "cluster": 38, "x": 8.557968139648438, "y": 3.6765825748443604}, {"id": 46272652, "title": "Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in LLMs", "cluster": 13, "x": 6.679967403411865, "y": 3.080326557159424}, {"id": 46270947, "title": "Distilling persona vectors into LLM weights", "cluster": 13, "x": 6.912540912628174, "y": 3.2451541423797607}, {"id": 46269850, "title": "Elysia: Decision Tree Agentic Framework", "cluster": 38, "x": 8.681849479675293, "y": 3.6167726516723633}, {"id": 46269791, "title": "How LLMs Think Like Clinicians", "cluster": 13, "x": 6.601817607879639, "y": 3.0261149406433105}, {"id": 46269448, "title": "Your Mac has a fast, offline LLM", "cluster": 13, "x": 6.872693061828613, "y": 3.744600296020508}, {"id": 46268006, "title": "Evalite: Evaluate your LLM-powered apps with TypeScript", "cluster": 13, "x": 6.793237686157227, "y": 3.6076083183288574}, {"id": 46267487, "title": "Google's Advent of Agents", "cluster": 35, "x": 8.43387508392334, "y": 3.7980101108551025}, {"id": 46267292, "title": "Rethinking a Mathematical Notation for Possible LLM Applications", "cluster": 13, "x": 6.7632880210876465, "y": 3.1949775218963623}, {"id": 46266357, "title": "WeKnora \u2013 LLM-Powered Document Understanding and Retrieval Framework", "cluster": 13, "x": 7.02703857421875, "y": 3.7656280994415283}, {"id": 46265865, "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "cluster": 13, "x": 6.383882999420166, "y": 3.251371145248413}, {"id": 46265916, "title": "Treating LLMs as \"Stochastic CPUs\" Instead of Chatbots (Undergrad)", "cluster": 13, "x": 6.582761287689209, "y": 3.244041681289673}, {"id": 46265027, "title": "A REST API for LLM latency and uptime metrics", "cluster": 13, "x": 6.917270183563232, "y": 3.6140480041503906}, {"id": 46264407, "title": "I added native time awareness to CrewAI to fix LLM date hallucinations", "cluster": 13, "x": 6.937220573425293, "y": 3.343905210494995}, {"id": 46263781, "title": "Metot \u2013 Using LLMs for structural argument mapping (not just summarization)", "cluster": 13, "x": 6.972412109375, "y": 3.4931440353393555}, {"id": 46263066, "title": "Steve Eisman is cautious about LLMs, influenced by Gary Marcus", "cluster": 13, "x": 6.617587566375732, "y": 3.069763660430908}, {"id": 46263158, "title": "Llmedge an on device LLM, vision, and speech inference library for Android", "cluster": 13, "x": 6.936217308044434, "y": 3.801978349685669}, {"id": 46261947, "title": "An open-source energy and latency profiler for LLMs: ELANA", "cluster": 13, "x": 6.96992826461792, "y": 3.619075298309326}, {"id": 46261131, "title": "Want to know what LLMs think about your brand?", "cluster": 13, "x": 6.582383632659912, "y": 3.077840805053711}, {"id": 46257829, "title": "Reproducibility Test-Time Training on Nearest Neighbors for LLMs", "cluster": 13, "x": 7.108534812927246, "y": 3.3641858100891113}, {"id": 46257740, "title": "Enabling small language models to solve complex reasoning tasks", "cluster": 208, "x": 8.394294738769531, "y": 4.323378086090088}, {"id": 46257734, "title": "Benchmarking LLMs on whether they can play FizzBuzz", "cluster": 13, "x": 6.748292446136475, "y": 3.254612922668457}, {"id": 46257125, "title": "llamafile: Distribute and Run LLMs with a Single File", "cluster": 13, "x": 6.901404857635498, "y": 3.757460594177246}, {"id": 46256130, "title": "Visualizing real-time LLM latency metrics", "cluster": 13, "x": 6.967813968658447, "y": 3.4642868041992188}, {"id": 46256220, "title": "The Emerging Market for Intelligence: Pricing, Supply, and Demand for LLMs", "cluster": 12, "x": 6.835392951965332, "y": 2.9801337718963623}, {"id": 46255794, "title": "Skills-kit/Framework for AI-generated, testable automation skills for every LLM", "cluster": 12, "x": 7.036462306976318, "y": 2.6212987899780273}, {"id": 46254965, "title": "Most LLM cost isn't compute \u2013 it's identity drift (110-cycle GPT-4o benchmark)", "cluster": 13, "x": 7.058539867401123, "y": 3.453230619430542}, {"id": 46253703, "title": "LLM Reflexion meta Core vs ML", "cluster": 13, "x": 7.0042805671691895, "y": 3.3350439071655273}, {"id": 46253056, "title": "Public Prompt License (PPL) \u2013 prompt-native licensing for LLM prompts", "cluster": 13, "x": 6.8246965408325195, "y": 3.5861034393310547}, {"id": 46252935, "title": "How well do LLMs generate R code?", "cluster": 13, "x": 6.786921501159668, "y": 3.6680495738983154}, {"id": 46252843, "title": "From Hand-Signed Forms to LLM's: The Evolution of Order Submission", "cluster": 13, "x": 6.672308921813965, "y": 3.2784454822540283}, {"id": 46250759, "title": "Can LLMs give us AGI if they are bad at arithmetic?", "cluster": 13, "x": 6.70826530456543, "y": 3.158623456954956}, {"id": 46250804, "title": "LLM-Derived Knowledge Graphs", "cluster": 13, "x": 7.036179065704346, "y": 3.4954001903533936}, {"id": 46250009, "title": "A LLM trained only on data from certain time periods to reduce modern bias", "cluster": 13, "x": 6.860042572021484, "y": 3.2828452587127686}, {"id": 46247385, "title": "Two Failures of Self-Consistency in the MultiStep Reasoning of LLMs (2024) [pdf]", "cluster": 13, "x": 6.877049922943115, "y": 3.1829216480255127}, {"id": 46247112, "title": "A Glance at GPU Goodness in Java: LLM Inference with TornadoVM", "cluster": 13, "x": 7.187893867492676, "y": 3.659893035888672}, {"id": 46246235, "title": "Deciduous: A tool to work better LLMs using a decisions DAG and insight querying", "cluster": 13, "x": 6.852588176727295, "y": 3.312154531478882}, {"id": 46244889, "title": "Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs", "cluster": 13, "x": 6.861031532287598, "y": 3.241997718811035}, {"id": 46244505, "title": "Benchmark that evaluates LLMs using 759 NYT Connections puzzles", "cluster": 13, "x": 7.002281188964844, "y": 3.322782039642334}, {"id": 46244458, "title": "New in Llama.cpp: Model Management", "cluster": 13, "x": 6.933261394500732, "y": 3.671130418777466}, {"id": 46244244, "title": "Vectorized MAXSCORE over WAND, especially for long LLM-generated queries", "cluster": 13, "x": 7.035661220550537, "y": 3.5006535053253174}, {"id": 46243037, "title": "Ask your LLM for receipts: What I learned teaching Claude C++ crash triage", "cluster": 13, "x": 6.693360328674316, "y": 3.530156373977661}, {"id": 46242838, "title": "Debug Mode for LLMs in vLLora", "cluster": 13, "x": 6.888077735900879, "y": 3.677863121032715}, {"id": 46242795, "title": "Training LLMs for honesty via confessions", "cluster": 13, "x": 6.59416389465332, "y": 3.1715965270996094}, {"id": 46242432, "title": "NanoGPT is the first LLM to train and run inference in space", "cluster": 13, "x": 7.101655960083008, "y": 3.544734239578247}, {"id": 46240604, "title": "PAG: A Formal Grammar for Structuring LLM Prompts", "cluster": 13, "x": 6.867562770843506, "y": 3.397136926651001}, {"id": 46240490, "title": "How Well Do LLMs Understand Tunisian Arabic?", "cluster": 13, "x": 6.619697093963623, "y": 3.113910675048828}, {"id": 46240321, "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "cluster": 13, "x": 6.3788933753967285, "y": 3.2305471897125244}, {"id": 46239997, "title": "LLaDA2.0: Scaling Up Diffusion Language Models to 100B [pdf]", "cluster": 208, "x": 8.166362762451172, "y": 4.451924800872803}, {"id": 46238784, "title": "NYT Connections LLM Benchmark", "cluster": 13, "x": 6.945451736450195, "y": 3.4035592079162598}, {"id": 46236674, "title": "Using LLMs at Oxide", "cluster": 13, "x": 6.766648292541504, "y": 3.506356716156006}, {"id": 46235130, "title": "100% Local LLM. Mistral Vibe vs. Opencode. A Claude Code Alternative? [video]", "cluster": 13, "x": 6.757357597351074, "y": 3.7552366256713867}, {"id": 46234329, "title": "New in Llama.cpp: Model Management", "cluster": 13, "x": 6.956813812255859, "y": 3.699064254760742}, {"id": 46234395, "title": "Fedora introduces LLM that suggests using apt to solve the issue", "cluster": 13, "x": 6.875857830047607, "y": 3.696333408355713}, {"id": 46232024, "title": "LangPatrol: A static analyzer for LLM prompts that catches bugs before inference", "cluster": 13, "x": 6.959647178649902, "y": 3.9041616916656494}, {"id": 46230790, "title": "Understanding the Role and Limits of API-Based LLM Monitoring", "cluster": 13, "x": 6.828906059265137, "y": 3.457402467727661}, {"id": 46230249, "title": "Vibing on the fly by having an LLM write functions during runtime", "cluster": 13, "x": 6.7494587898254395, "y": 3.8275935649871826}, {"id": 46229315, "title": "Using Claude Code to Fine-Tune Open Source LLMs", "cluster": 13, "x": 6.81918478012085, "y": 3.812814950942993}, {"id": 46228602, "title": "LLM-Powered Relevance Assessment for Pinterest Search", "cluster": 13, "x": 6.833136081695557, "y": 3.4539146423339844}, {"id": 46227896, "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "cluster": 13, "x": 6.390120506286621, "y": 3.2313754558563232}, {"id": 46227347, "title": "Brain-Inspired LLM Alignment", "cluster": 13, "x": 6.789459228515625, "y": 3.185215711593628}, {"id": 46226870, "title": "Humans & LLMs rate deliberation as superior to intuition on reasoning tasks", "cluster": 13, "x": 6.831989288330078, "y": 3.0174827575683594}, {"id": 46225568, "title": "Why We Need to Know LR and Recursive Descent Parsing Techniques", "cluster": 208, "x": 8.26368522644043, "y": 4.448536396026611}, {"id": 46224833, "title": "I ported Karpathy's LLM Council in JavaScript \u2013 trivial to use", "cluster": 13, "x": 6.816585063934326, "y": 3.7342400550842285}, {"id": 46220958, "title": "Hark: Voice prompts for LLMs, meeting minutes, quick voice journaling", "cluster": 13, "x": 6.662764072418213, "y": 3.5364222526550293}, {"id": 46220748, "title": "Unsloth \u2013 Train LLMs 2x faster with 70% less VRAM", "cluster": 13, "x": 7.1071271896362305, "y": 3.8181517124176025}, {"id": 46220505, "title": "New research shows RL may not help a model learn new basic skills", "cluster": 13, "x": 6.766191005706787, "y": 3.0484580993652344}, {"id": 46220277, "title": "Study: ~250 documents is all it takes to backdoor an LLM", "cluster": 13, "x": 6.633389949798584, "y": 3.4495463371276855}, {"id": 46219620, "title": "We don't know what most microbial genes do. Can genomic language models help?", "cluster": 208, "x": 8.238011360168457, "y": 4.2154717445373535}, {"id": 46218428, "title": "Getting Real with LLMs", "cluster": 13, "x": 6.538634777069092, "y": 3.092092275619507}, {"id": 46218392, "title": "Unlocking 15% More Performance: A Case Study in LLVM Optimization for RISC-V", "cluster": 13, "x": 7.263897895812988, "y": 3.8333165645599365}, {"id": 46217752, "title": "My First Language Front End with LLVM Tutorial", "cluster": 13, "x": 6.813677787780762, "y": 3.84976863861084}, {"id": 46217578, "title": "New benchmark shows top LLMs struggle in real mental health care", "cluster": 13, "x": 6.7649760246276855, "y": 3.3153364658355713}, {"id": 46216181, "title": "A tiny entropy experiment to push LLMs into unexpected paths", "cluster": 13, "x": 6.692214488983154, "y": 3.084489345550537}, {"id": 46215459, "title": "Five LLMs Tried to Build a Website. ChatGPT Failed", "cluster": 13, "x": 6.627602577209473, "y": 3.5417416095733643}, {"id": 46215001, "title": "Every LLM gateway we tested failed at scale \u2013 ended up building Bifrost", "cluster": 13, "x": 6.872123718261719, "y": 3.5400736331939697}, {"id": 46214815, "title": "LLM Benchmark by Databricks \u2013 OfficeQA", "cluster": 13, "x": 6.963331699371338, "y": 3.377276659011841}, {"id": 46214731, "title": "Is any of you using LLMs to create full features in big enterprise apps?", "cluster": 13, "x": 6.7730841636657715, "y": 3.58640193939209}, {"id": 46214745, "title": "HuggingFace Skills: Fine-tune any LLM with one sentence for $0.30", "cluster": 13, "x": 6.7353668212890625, "y": 3.458383321762085}, {"id": 46214437, "title": "LLM Council \u2013 Your Local Multi-Model AI Advisory Board", "cluster": 12, "x": 7.016660213470459, "y": 2.6155076026916504}, {"id": 46213923, "title": "Turbocharging LinkedIn's Recommendation Systems with SGLang", "cluster": 13, "x": 7.050943374633789, "y": 3.6937129497528076}, {"id": 46211861, "title": "OpenEvolve: Teaching LLMs to Discover Algorithms Through Evolution", "cluster": 13, "x": 6.884872913360596, "y": 3.3459978103637695}, {"id": 46211279, "title": "NeurIPS 2025 Top Paper Breakdown: LLMs Are Collapsing into Artificial Hivemind", "cluster": 12, "x": 6.985232353210449, "y": 2.6241042613983154}, {"id": 46210661, "title": "I misused LLMs to diagnose myself and ended up bedridden for a week", "cluster": 13, "x": 6.5110392570495605, "y": 3.109332323074341}, {"id": 46210263, "title": "Self-Improving VLM Judges Without Human Annotations", "cluster": 13, "x": 6.6966094970703125, "y": 3.3840718269348145}, {"id": 46210163, "title": "MindEval: New benchmark shows 12 top LLMs fail at mental health care", "cluster": 13, "x": 6.506491661071777, "y": 3.192387342453003}, {"id": 46209093, "title": "Using LLM as a writing assistant (without the slop)", "cluster": 13, "x": 6.723663330078125, "y": 3.490530490875244}, {"id": 46207703, "title": "Words That Make Language Models Perceive", "cluster": 208, "x": 8.357924461364746, "y": 4.339539051055908}, {"id": 46206577, "title": "The Argument for Emergent Language Models", "cluster": 208, "x": 8.269598960876465, "y": 4.409982204437256}, {"id": 46206389, "title": "Lightmatter Aims to Leapfrog I/O Limitations with 3D Photonic Interconnect", "cluster": 13, "x": 7.115139961242676, "y": 3.707573413848877}, {"id": 46205795, "title": "Divyam-LLM-interop:LLM responses,requests translation across APIs and models", "cluster": 13, "x": 6.9054412841796875, "y": 3.8131611347198486}, {"id": 46205110, "title": "Not Minds, but Signs: Reframing LLMs Through Semiotics [pdf]", "cluster": 13, "x": 6.757129192352295, "y": 3.188021421432495}, {"id": 46204691, "title": "Degradation of Multi-Task Prompting Across Six NLP Tasks and LLM Families", "cluster": 13, "x": 6.790582180023193, "y": 3.3125970363616943}, {"id": 46203782, "title": "Offline cybersecurity AI using RAG and local LLM (Python, FAISS, Llama 3.1)", "cluster": 12, "x": 7.044554710388184, "y": 2.652979612350464}, {"id": 46203833, "title": "Learning a new programming language with an LLM", "cluster": 13, "x": 6.675565719604492, "y": 3.5956974029541016}, {"id": 46203824, "title": "SSE sucks for transporting LLM tokens", "cluster": 13, "x": 6.579569339752197, "y": 3.2082347869873047}, {"id": 46203066, "title": "The Fallacy of LLM Reasoning", "cluster": 13, "x": 6.691040515899658, "y": 3.106078624725342}, {"id": 46202881, "title": "An Interesting Problem: The Lua Function `Next`", "cluster": 13, "x": 6.8987555503845215, "y": 3.8032984733581543}, {"id": 46199849, "title": "RNA language models can generalize well on structure prediction tasks", "cluster": 208, "x": 8.396830558776855, "y": 4.399667263031006}, {"id": 46196807, "title": "Scammers poison LLM search to push fake airline customer support numbers", "cluster": 13, "x": 6.500808238983154, "y": 3.267155170440674}, {"id": 46196561, "title": "50\u00d7 faster than LiteLLM: Bifrost is a Go-based LLM gateway built for scale.", "cluster": 13, "x": 6.993901252746582, "y": 3.616288661956787}, {"id": 46194484, "title": "Words that make language models perceive", "cluster": 208, "x": 8.37347412109375, "y": 4.359357833862305}, {"id": 46194346, "title": "Sanskrit native LLM \u2013 Early epoch release", "cluster": 13, "x": 6.926777362823486, "y": 3.7233808040618896}, {"id": 46194084, "title": "LLMs Make Legal Advice Lossy", "cluster": 13, "x": 6.5711236000061035, "y": 3.1444222927093506}, {"id": 46193876, "title": "LLM Weights vs. the Papercuts of Corporate", "cluster": 13, "x": 6.660735130310059, "y": 3.193679094314575}, {"id": 46192602, "title": "GeoVista open-source agentic geolocation", "cluster": 35, "x": 8.636178970336914, "y": 3.9294064044952393}, {"id": 46192469, "title": "Will LLMs be more or less rational consumers than humans?", "cluster": 13, "x": 6.659011363983154, "y": 3.0357985496520996}, {"id": 46191850, "title": "Parallel Development with Git Worktree and LLMs", "cluster": 13, "x": 7.0107951164245605, "y": 3.856598377227783}, {"id": 46187831, "title": "I made a prompt framework that makes LLMs stop hedging and speak straight", "cluster": 13, "x": 6.852526664733887, "y": 3.3606905937194824}, {"id": 46186082, "title": "Gh PR-review: LLM-friendly PR review workflows in your CLI", "cluster": 13, "x": 6.8298139572143555, "y": 3.547363519668579}, {"id": 46185733, "title": "AI Output Format Catalog \u2013 116 standardized tags for predictable LLM responses", "cluster": 12, "x": 7.059837818145752, "y": 2.784574508666992}, {"id": 46185496, "title": "Chrome browser extension for chatting about private pages with local LLMs", "cluster": 13, "x": 6.726589202880859, "y": 3.7368922233581543}, {"id": 46184558, "title": "Using LLMs for Breadcrumbs, Not Code Generation", "cluster": 13, "x": 6.749847888946533, "y": 3.7611570358276367}, {"id": 46183184, "title": "MYRA stack \u2013 Modern Java FFM based libraries", "cluster": 13, "x": 6.981329917907715, "y": 3.88122820854187}, {"id": 46181053, "title": "POI classification and OSM, a match made in hell", "cluster": 13, "x": 6.617009162902832, "y": 3.1379921436309814}, {"id": 46181076, "title": "Java Hello World, LLVM Edition", "cluster": 13, "x": 6.9215407371521, "y": 3.7549009323120117}, {"id": 46181008, "title": "How to Build an LLM-Powered Database Query Bot for Your Web App in 1 Day", "cluster": 13, "x": 6.810634136199951, "y": 3.5166497230529785}, {"id": 46180416, "title": "LLM Fingerprints in Text", "cluster": 13, "x": 6.713487148284912, "y": 3.5068233013153076}, {"id": 46179433, "title": "I built a free tool that extracts Go code semantically for LLM context", "cluster": 13, "x": 6.856947422027588, "y": 3.8087899684906006}, {"id": 46179060, "title": "Trying VLLM Ideas on Apple Silicon with MLX (WIP)", "cluster": 271, "x": 7.087016582489014, "y": 3.860179901123047}, {"id": 46178829, "title": "Anatomy of a Domain Risk Engine: Regex vs. LLMs", "cluster": 13, "x": 6.83839750289917, "y": 3.5609490871429443}, {"id": 46178347, "title": "Using LLMs at Oxide", "cluster": 13, "x": 6.766604423522949, "y": 3.5369315147399902}, {"id": 46177807, "title": "Project Bhavanga: Fixing LLM Context Dilution with Buddhist Psychology", "cluster": 13, "x": 6.853761672973633, "y": 3.192716598510742}, {"id": 46176952, "title": "LLMs Are Lexical Thijarians", "cluster": 13, "x": 6.706732273101807, "y": 3.283653736114502}, {"id": 46176889, "title": "Fine-Tuning an Open Source LLM using Claude Skills", "cluster": 13, "x": 6.766653060913086, "y": 3.68632173538208}, {"id": 46176177, "title": "Hybrid ML and LLM Framework for Identifying Engaging, Breaking Content on Reddit", "cluster": 13, "x": 6.903220176696777, "y": 3.3011796474456787}, {"id": 46175404, "title": "Robust code generation combining grammars and LLMs", "cluster": 13, "x": 6.7788286209106445, "y": 3.779815673828125}, {"id": 46173998, "title": "OpenAI has trained its LLM to confess to bad behavior", "cluster": 13, "x": 6.511490345001221, "y": 3.3270769119262695}, {"id": 46173369, "title": "Biological LLM Evo 2: Getting Started", "cluster": 13, "x": 6.790487766265869, "y": 3.3340320587158203}, {"id": 46173230, "title": "Code Evolution: Self-Improving Software with LLMs and Python", "cluster": 13, "x": 6.823033332824707, "y": 3.7820117473602295}, {"id": 46172484, "title": "I Pushed an LLM (Claude Opus 4.1) to Its Narrative Limit", "cluster": 13, "x": 6.538464069366455, "y": 3.0942881107330322}, {"id": 46167092, "title": "Chess LLM Benchmark: Evaluating LLMs' ability to play chess", "cluster": 13, "x": 6.865290641784668, "y": 3.2318150997161865}, {"id": 46166754, "title": "Tired of spoonfeeding the same prompts to LLM's", "cluster": 13, "x": 6.533435344696045, "y": 3.163304567337036}, {"id": 46165952, "title": "RFC: Forming a Working Group on Formal Specification for LLVM", "cluster": 13, "x": 6.897339344024658, "y": 3.612196445465088}, {"id": 46165951, "title": "We Got Claude to Fine-Tune an Open Source LLM", "cluster": 13, "x": 6.79983377456665, "y": 3.8045992851257324}, {"id": 46165097, "title": "The Patient Is Not a Document: Moving from LLMs to a World Model for Oncology", "cluster": 13, "x": 6.603920936584473, "y": 3.2291738986968994}, {"id": 46164968, "title": "Are large language models worth it?", "cluster": 208, "x": 8.163228034973145, "y": 4.373950004577637}, {"id": 46162423, "title": "Animating SVGs with LLMs", "cluster": 13, "x": 7.09019136428833, "y": 3.683899402618408}, {"id": 46162205, "title": "Beyond Statistics in Inference LLMs Comes Geometry", "cluster": 13, "x": 7.03302001953125, "y": 3.3388166427612305}, {"id": 46159625, "title": "I built an API to give LLMs instant access to documentation for 1000 libraries", "cluster": 13, "x": 6.8169779777526855, "y": 3.7746422290802}, {"id": 46159088, "title": "The first programming language designed for LLM", "cluster": 13, "x": 6.695364475250244, "y": 3.622159481048584}, {"id": 46157639, "title": "Free Beta: Fine-tuning SDK for LLMs, comments welcome", "cluster": 13, "x": 6.886590003967285, "y": 3.71014666557312}, {"id": 46156999, "title": "LLM inference is nearly deterministic. We use this to audit providers", "cluster": 13, "x": 6.916442394256592, "y": 3.2165350914001465}, {"id": 46154650, "title": "Dosh (LLM-powered shell commands)", "cluster": 13, "x": 6.895348072052002, "y": 3.8128039836883545}, {"id": 46154491, "title": "We gave 5 LLMs $100K to trade stocks for 8 months", "cluster": 13, "x": 6.799375057220459, "y": 3.175044298171997}, {"id": 46154244, "title": "Large Language Models for Gravitational Wave Identification", "cluster": 208, "x": 8.549154281616211, "y": 4.279407501220703}, {"id": 46153386, "title": "Investment without optimization: LLM-as-a-Judge tournaments and evolution", "cluster": 13, "x": 6.884757041931152, "y": 3.2027926445007324}, {"id": 46152437, "title": "Fermi estimate comparing human sensory bandwidth to LLM input bandwidth", "cluster": 13, "x": 6.782681465148926, "y": 3.1409730911254883}, {"id": 46151709, "title": "A smarter way for large language models to think about hard problems", "cluster": 208, "x": 8.205809593200684, "y": 4.371246337890625}, {"id": 46151759, "title": "We Got Claude to Fine-Tune an Open Source LLM", "cluster": 13, "x": 6.829817771911621, "y": 3.768698215484619}, {"id": 46151160, "title": "AI Trade Arena: 5 LLMs as Stock Traders over 8 Months", "cluster": 12, "x": 6.935688495635986, "y": 2.7188355922698975}, {"id": 46150738, "title": "A secure cloud vault and usage-tracking service for all your LLM providers", "cluster": 13, "x": 6.668006896972656, "y": 3.6119384765625}, {"id": 46149242, "title": "The Gap for LLMs Isn't Benchmarks \u2013 It's Everyday Value", "cluster": 13, "x": 6.755659580230713, "y": 3.1887316703796387}, {"id": 46149389, "title": "Teaching an LLM to Write Assembly: GBNF-Constrained Generation for a Custom CPU", "cluster": 13, "x": 7.107692718505859, "y": 3.8583595752716064}, {"id": 46149108, "title": "Kodezi Chronos-1  - LLM specialized in code debugging", "cluster": 13, "x": 6.8772382736206055, "y": 3.8657326698303223}, {"id": 46148739, "title": "Beyond representation: rethinking intelligence in the age of LLMs", "cluster": 12, "x": 6.958242893218994, "y": 2.6573424339294434}, {"id": 46148646, "title": "\u03a0-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling", "cluster": 208, "x": 8.353476524353027, "y": 4.438340663909912}, {"id": 46147571, "title": "The misery of fitting probabilistic LLMs into rigid SQL schemas", "cluster": 13, "x": 6.853412628173828, "y": 3.3681325912475586}, {"id": 46146772, "title": "Bits is all you need (and 3.6 bit what you have?) for resource-efficient LLMs?", "cluster": 13, "x": 7.028231620788574, "y": 3.6486761569976807}, {"id": 46146372, "title": "Tracker AI \u2013 A Veterinary LLM Trained on 300k+ Clinical Cases", "cluster": 12, "x": 7.02100944519043, "y": 2.6394760608673096}, {"id": 46146091, "title": "We created API-Bench to test how well LLMs execute against APIs", "cluster": 13, "x": 6.864951133728027, "y": 3.577958822250366}, {"id": 46146194, "title": "Training LLMs for Honesty via Confessions [pdf]", "cluster": 13, "x": 6.650813102722168, "y": 3.226107120513916}, {"id": 46143116, "title": "The LLM Evaluation Guidebook", "cluster": 13, "x": 6.866134166717529, "y": 3.2725677490234375}, {"id": 46141940, "title": "AgentDevCamp", "cluster": 38, "x": 8.658585548400879, "y": 3.6416053771972656}, {"id": 46140672, "title": "Study: Effects of LLMs versus Web Search on Depth of Learning", "cluster": 13, "x": 6.845218181610107, "y": 3.4043936729431152}, {"id": 46138465, "title": "Teaching an LLM a Niche Diagraming Language", "cluster": 13, "x": 6.875, "y": 3.3048925399780273}, {"id": 46138405, "title": "LLMs as a new alien subset of all possible embodied intelligences", "cluster": 12, "x": 6.9035491943359375, "y": 2.734402894973755}, {"id": 46138200, "title": "A central hub for LLM API config info: model-api.info", "cluster": 13, "x": 6.892794132232666, "y": 3.7121200561523438}, {"id": 46138115, "title": "From Moderation to Mediation: Can LLMs Serve as Mediators in Online Flame Wars?", "cluster": 13, "x": 6.588502883911133, "y": 3.100090503692627}, {"id": 46136363, "title": "RAG vs. Traditional ML", "cluster": 13, "x": 6.759120941162109, "y": 3.3333539962768555}, {"id": 46135812, "title": "Pax Historia \u2013 LLM powered alt-history game", "cluster": 13, "x": 6.882963180541992, "y": 3.385164737701416}, {"id": 46135076, "title": "Using LLMs for Web Search", "cluster": 13, "x": 6.820413589477539, "y": 3.5821144580841064}, {"id": 46134397, "title": "Google Adds LLMs.txt to Search Developer Docs", "cluster": 13, "x": 6.893533229827881, "y": 3.7441742420196533}, {"id": 46134344, "title": "How LLM Inference Works", "cluster": 13, "x": 6.95095157623291, "y": 3.2317113876342773}, {"id": 46133558, "title": "LLMs are great at box diagrams", "cluster": 13, "x": 6.911098957061768, "y": 3.4998533725738525}, {"id": 46132708, "title": "MCPMark: A LLM Benchmark based on real-world use cases (in Notion, Playwright..)", "cluster": 13, "x": 6.9098429679870605, "y": 3.3531718254089355}, {"id": 46130216, "title": "Ellora: Enhancing LLMs with LoRA Standardized Recipes for Capability Enhancement", "cluster": 13, "x": 6.850749492645264, "y": 3.4698338508605957}, {"id": 46129631, "title": "Openterface KVM-GO \u2013 Crowd Supply", "cluster": 13, "x": 7.02212381362915, "y": 3.8770720958709717}, {"id": 46128634, "title": "A pragmatic guide to LLM evals for devs", "cluster": 13, "x": 6.862461090087891, "y": 3.333828926086426}, {"id": 46124877, "title": "LLM council web ready to use version", "cluster": 13, "x": 6.768454551696777, "y": 3.6711812019348145}, {"id": 46124425, "title": "LLM from scratch, part 28 \u2013 training a base model from scratch on an RTX 3090", "cluster": 13, "x": 7.08372688293457, "y": 3.6828505992889404}, {"id": 46120765, "title": "LLMs Need Better Executive Function", "cluster": 13, "x": 6.553927421569824, "y": 3.154090404510498}, {"id": 46120615, "title": "VLLM-Omni: A framework for efficient model inference with Omni-modality models", "cluster": 13, "x": 7.228020191192627, "y": 3.4380643367767334}, {"id": 46120540, "title": "Constant-time support for LLVM to protect cryptographic code", "cluster": 13, "x": 6.574920654296875, "y": 3.678950309753418}, {"id": 46120302, "title": "Multi-threaded LLM agent with async \"subconscious\" loop and pgvector memory", "cluster": 13, "x": 6.873692989349365, "y": 3.3147664070129395}, {"id": 46119007, "title": "Compiler-based internationalization: we promise magic, what's the impact?", "cluster": 208, "x": 8.253334999084473, "y": 4.455594062805176}, {"id": 46117863, "title": "Byte-Level Tokenizers Unavoidably Enable LLMs to Generate Ill-Formed UTF-8", "cluster": 13, "x": 6.691141128540039, "y": 3.6405327320098877}, {"id": 46117157, "title": "Three Levels of Running LLMs from Laptop to Cluster-Scale Distributed Inference", "cluster": 13, "x": 7.1329522132873535, "y": 3.5645694732666016}, {"id": 46117196, "title": "What are small language models and how do they differ from large ones?", "cluster": 208, "x": 8.171299934387207, "y": 4.434716701507568}, {"id": 46115734, "title": "Volitional Response Protocol \u2013 What happens when LLMs can decline to engage [pdf]", "cluster": 13, "x": 6.78521728515625, "y": 3.1477441787719727}, {"id": 46112194, "title": "LLMs perf on Path-X or Path-256?", "cluster": 13, "x": 6.992274761199951, "y": 3.748286008834839}, {"id": 46110141, "title": "Teaching LLMs to compose math symbolically, not execute it", "cluster": 13, "x": 6.753182888031006, "y": 3.204402208328247}, {"id": 46109669, "title": "Hosting LLMs on Blockchains \u2013 Cocoon", "cluster": 13, "x": 6.792322635650635, "y": 3.6536624431610107}, {"id": 46109468, "title": "Can LLMs give us AGI if they are bad at arithmetic?", "cluster": 13, "x": 6.694352626800537, "y": 3.144291400909424}, {"id": 46108541, "title": "Evo-Memory: Benchmarking LLM Agent Test-Time Learning with Self-Evolving Memory", "cluster": 13, "x": 7.02891206741333, "y": 3.279585123062134}, {"id": 46108332, "title": "Chatbot with Lazy Mode (LLMParty)", "cluster": 13, "x": 6.703168869018555, "y": 3.5711610317230225}, {"id": 46108417, "title": "Formal Proof: LLM Hallucinations Are Structural, Not Statistical (Coq Verified)", "cluster": 13, "x": 6.784020900726318, "y": 3.059675931930542}, {"id": 46106854, "title": "Observ.dev \u2013 Infrastructure for quicker, cheaper and more reliable LLM calls", "cluster": 13, "x": 6.851053714752197, "y": 3.715053081512451}, {"id": 46104735, "title": "Matcha local RSS adds LLM notifications", "cluster": 13, "x": 6.772337436676025, "y": 3.670996904373169}, {"id": 46104411, "title": "LLMs alone won't desing rockets", "cluster": 13, "x": 6.528967380523682, "y": 3.0873496532440186}, {"id": 46104367, "title": "Day 1 \u2013 Dancer, Dasher and Dosh (LLM-powered shell commands)", "cluster": 13, "x": 6.919462203979492, "y": 3.9057178497314453}, {"id": 46103417, "title": "The potential existential threat of LLMs to online survey research", "cluster": 13, "x": 6.601099014282227, "y": 3.043919801712036}, {"id": 46102241, "title": "SVM by Hand", "cluster": 13, "x": 7.033437252044678, "y": 3.6325795650482178}, {"id": 46102047, "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence", "cluster": 12, "x": 7.020293712615967, "y": 2.644453525543213}, {"id": 46099885, "title": "GhidrAssist and GhidrAssistMCP LLM plugins reached v1.0", "cluster": 13, "x": 6.969359397888184, "y": 3.878155469894409}, {"id": 46099779, "title": "LLM \u2013 Unit Economics", "cluster": 13, "x": 7.148513317108154, "y": 3.2159438133239746}, {"id": 46099450, "title": "Awesome-distributed-ML \u2013 A curated list for distributed [faster] LLM training", "cluster": 13, "x": 6.971354961395264, "y": 3.5242810249328613}, {"id": 46099406, "title": "LumeBiome", "cluster": 13, "x": 6.944488048553467, "y": 3.726138114929199}, {"id": 46098359, "title": "LLVM-MOS \u2013 Clang LLVM fork targeting the 6502", "cluster": 13, "x": 7.0232319831848145, "y": 3.8697595596313477}, {"id": 46098057, "title": "Gated Attention for Large Language Models", "cluster": 208, "x": 8.189858436584473, "y": 4.403969764709473}, {"id": 46097855, "title": "Using Petri nets as a formal language for LLM-assisted development", "cluster": 13, "x": 6.79730749130249, "y": 3.6447629928588867}, {"id": 46096805, "title": "Beyond the Repository \u2013 Communications of the ACM", "cluster": 372, "x": 6.9079461097717285, "y": 3.868009328842163}, {"id": 46096719, "title": "A unified theory of ambiguous cognition for humans and LLMs (FLCT)", "cluster": 13, "x": 6.807169437408447, "y": 3.093067169189453}, {"id": 46096125, "title": "Local AI: 152 Open-Source Tools for 100% Offline LLMs (2025\u20132026)", "cluster": 12, "x": 7.022284030914307, "y": 3.0186498165130615}, {"id": 46095379, "title": "Exploiting open Ollama instances for free LLM inference", "cluster": 13, "x": 6.837212562561035, "y": 3.620924949645996}, {"id": 46094266, "title": "Discord for LLMs in a single 3.7k-line HTML file", "cluster": 13, "x": 6.817617416381836, "y": 3.766842842102051}, {"id": 46093688, "title": "LLM live ranking (Gemini, OpenAI, xAI)", "cluster": 13, "x": 6.941219806671143, "y": 3.5842957496643066}, {"id": 46090798, "title": "I built 'Cursor for Ad Creatives' \u2013 Reverse-engineering ads using LLMs", "cluster": 13, "x": 6.780147552490234, "y": 3.646108865737915}, {"id": 46090238, "title": "LLM Model Live Ranker", "cluster": 13, "x": 6.817070484161377, "y": 3.346886396408081}, {"id": 46089998, "title": "Wikipedia: Writing articles with large language models", "cluster": 208, "x": 8.305716514587402, "y": 4.527101516723633}, {"id": 46089828, "title": "[Wrong link]Watch my hobby project get pounded by LLM scrapers live", "cluster": 13, "x": 6.776706218719482, "y": 3.6745455265045166}, {"id": 46087001, "title": "LLMs and the Human Condition", "cluster": 13, "x": 6.660984516143799, "y": 3.056981325149536}, {"id": 46085637, "title": "LLM Agents Demystified", "cluster": 13, "x": 6.826653957366943, "y": 3.2122368812561035}, {"id": 46085038, "title": "LLMs write code without compilers, could they do philosophy without logic?", "cluster": 13, "x": 6.606369972229004, "y": 3.5383875370025635}, {"id": 46083702, "title": "When LLMs learn to take shortcuts, they become evil", "cluster": 13, "x": 6.460044860839844, "y": 3.1876087188720703}, {"id": 46083628, "title": "Structuring LLM outputs: best practices for legal prompt engineering (2024)", "cluster": 13, "x": 6.89185094833374, "y": 3.3877298831939697}, {"id": 46083162, "title": "A Tale of Two AI Failures: Debugging a Simple Bug with LLMs", "cluster": 12, "x": 6.9741926193237305, "y": 2.620278835296631}, {"id": 46080868, "title": "New security-focused LLM service built on alias1 model launches today", "cluster": 13, "x": 6.751681804656982, "y": 3.682957410812378}, {"id": 46080597, "title": "Anti-patterns while working with LLMs", "cluster": 13, "x": 6.68810510635376, "y": 3.308695077896118}, {"id": 46079790, "title": "SSE sucks for transporting LLM tokens", "cluster": 13, "x": 6.571528911590576, "y": 3.2027506828308105}, {"id": 46079571, "title": "A 27M parameter model beating LLMs on reasoning tasks", "cluster": 13, "x": 7.072434902191162, "y": 3.35992169380188}, {"id": 46079345, "title": "Counterfeit judgments in large language models", "cluster": 208, "x": 8.201156616210938, "y": 4.3098602294921875}, {"id": 46079091, "title": "Could Endpoint SLMs Replace Cloud LLMs? Would Datacenter Race Shudder to a Halt?", "cluster": 13, "x": 6.8735432624816895, "y": 3.4728071689605713}, {"id": 46078761, "title": "What the hell is \"Mental Jumping\" in llm's", "cluster": 13, "x": 6.617135047912598, "y": 3.05818510055542}, {"id": 46077253, "title": "Model Madness: Making Sense of Today's LLM Chaos", "cluster": 13, "x": 6.683046340942383, "y": 3.035372734069824}, {"id": 46076721, "title": "LLM Inference with Ray: Expert parallelism and prefill/decode disaggregation", "cluster": 13, "x": 7.170299530029297, "y": 3.4780638217926025}, {"id": 46076281, "title": "What LLM to use today?", "cluster": 13, "x": 6.767851829528809, "y": 3.3647007942199707}, {"id": 46076128, "title": "A personality-filter for LLM chatbots (holiday project)", "cluster": 13, "x": 6.757144927978516, "y": 3.48537015914917}, {"id": 46076225, "title": "SSE sucks for transporting LLM tokens", "cluster": 13, "x": 6.563726425170898, "y": 3.175205707550049}, {"id": 46075879, "title": "Git-reabsorb: Reorganize Git commits with new structure using an LLM", "cluster": 13, "x": 6.980705738067627, "y": 3.8067128658294678}, {"id": 46076033, "title": "New insight into why LLMs are not great at cracking passwords", "cluster": 13, "x": 6.434463977813721, "y": 3.2793450355529785}, {"id": 46075175, "title": "Local LLM based code reviews at Scripbox", "cluster": 13, "x": 6.768156051635742, "y": 3.677539110183716}, {"id": 46074710, "title": "Study finds LLMs have a tendency to perpetuate delusions", "cluster": 13, "x": 6.5677008628845215, "y": 3.0192363262176514}, {"id": 46074642, "title": "LLM-powered Nexus OS \u2013 forkable meta-thinking with pack-based evolution", "cluster": 13, "x": 7.013034820556641, "y": 3.761641263961792}, {"id": 46074413, "title": "LLM agent workflows fail silently. Here's the reliability layer we wish existed", "cluster": 13, "x": 6.609936237335205, "y": 3.2311134338378906}, {"id": 46073766, "title": "Evidently 0.7.17: open-source LLM tracing and dataset management", "cluster": 13, "x": 6.919027328491211, "y": 3.744173765182495}, {"id": 46073247, "title": "An Empirical Study on Why LLMs Struggle with Password Cracking", "cluster": 13, "x": 6.423430442810059, "y": 3.1809842586517334}, {"id": 46073302, "title": "Docker model runner integrates vllm", "cluster": 13, "x": 7.062851905822754, "y": 3.936180830001831}, {"id": 46071676, "title": "LLM unpredictability isn't a model problem \u2013 it's a process problem", "cluster": 13, "x": 6.690141201019287, "y": 3.155618906021118}, {"id": 46071464, "title": "All your LLMs ranked by speed every minute", "cluster": 13, "x": 6.763887882232666, "y": 3.230546712875366}, {"id": 46070986, "title": "SSE sucks for transporting LLM tokens", "cluster": 13, "x": 6.549481391906738, "y": 3.23989200592041}, {"id": 46070861, "title": "Google Agent Garden", "cluster": 35, "x": 8.476470947265625, "y": 3.824018716812134}, {"id": 46070525, "title": "Looking for 10\u201315 apps to test my LLM security scanner (pre-launch,free reports)", "cluster": 13, "x": 6.808085918426514, "y": 3.5471794605255127}, {"id": 46069716, "title": "LLM Observatory", "cluster": 13, "x": 6.807948589324951, "y": 3.438709020614624}, {"id": 46069771, "title": "We're losing our voice to LLMs", "cluster": 13, "x": 6.499100685119629, "y": 3.0360584259033203}, {"id": 46068666, "title": "Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament", "cluster": 13, "x": 6.561148643493652, "y": 3.576319932937622}, {"id": 46067416, "title": "Major LLMs fabricate evidence when presented with governance critiques", "cluster": 13, "x": 6.696323871612549, "y": 3.0496253967285156}, {"id": 46067294, "title": "Claude Opus 4.5, and why evaluating new LLMs is increasingly difficult", "cluster": 13, "x": 6.641402721405029, "y": 3.1689372062683105}, {"id": 46065628, "title": "LLMs can invent their own compression", "cluster": 13, "x": 6.713776588439941, "y": 3.3351550102233887}, {"id": 46062960, "title": "Abliterated Large Language Models Treat Users as Capable Adults", "cluster": 208, "x": 8.26955795288086, "y": 4.410212516784668}, {"id": 46061982, "title": "Automating Agentic Development", "cluster": 38, "x": 8.561724662780762, "y": 3.6970460414886475}, {"id": 46061588, "title": "LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations", "cluster": 13, "x": 7.035096645355225, "y": 3.3613693714141846}, {"id": 46059939, "title": "A Brief History of Large Language Models", "cluster": 208, "x": 8.201313972473145, "y": 4.383962631225586}, {"id": 46059857, "title": "LLM live model ranker in latency", "cluster": 13, "x": 6.9433369636535645, "y": 3.4591329097747803}, {"id": 46059502, "title": "What Do LLMs Want?", "cluster": 13, "x": 6.637567043304443, "y": 3.1468896865844727}, {"id": 46058995, "title": "Universal LLM Memory Does Not Exist", "cluster": 13, "x": 6.882962226867676, "y": 3.596381902694702}, {"id": 46058033, "title": "Compressed filesystems \u00e0 la language models", "cluster": 208, "x": 8.134469032287598, "y": 4.469323635101318}, {"id": 46057838, "title": "Open source LLM prompt eval and optimization CLI", "cluster": 13, "x": 6.8747148513793945, "y": 3.634753465652466}, {"id": 46057417, "title": "Forecasting Ability of LLMs Depends on What We're Asking", "cluster": 13, "x": 6.775722026824951, "y": 3.195119619369507}, {"id": 46056917, "title": "Elven Rope, Ultra-High Molecular Weight Polyethylene, and LLMs", "cluster": 13, "x": 6.756465435028076, "y": 3.2970879077911377}, {"id": 46056545, "title": "Open source Firefox extension to quickly interact with an LLM on current webpage", "cluster": 13, "x": 6.782683849334717, "y": 3.695035696029663}, {"id": 46055823, "title": "LLMs: The gift that keeps on giving", "cluster": 13, "x": 6.53898286819458, "y": 3.0980064868927}, {"id": 46055114, "title": "ML LLVM Project: Compiler Infrastructure for ML-Driven Optimizations", "cluster": 13, "x": 7.420990467071533, "y": 4.04197359085083}, {"id": 46054419, "title": "LLMs excel at programming\u2013how can they be so bad at it?", "cluster": 13, "x": 6.585024833679199, "y": 3.482731819152832}, {"id": 46053490, "title": "Optimzing Our Jax LLM RL Pipeline", "cluster": 13, "x": 6.8850836753845215, "y": 3.5565154552459717}, {"id": 46053174, "title": "Are LLMs the Best That They Will Ever Be?", "cluster": 13, "x": 6.6158857345581055, "y": 3.1294994354248047}, {"id": 46052776, "title": "AgentDevCamp", "cluster": 38, "x": 8.645009994506836, "y": 3.6632142066955566}, {"id": 46052090, "title": "LLVM Adds Constant-Time Support for Protecting Cryptographic Code", "cluster": 13, "x": 6.526186943054199, "y": 3.6405062675476074}, {"id": 46051363, "title": "LLM Latency Live Ranking", "cluster": 13, "x": 6.9417314529418945, "y": 3.4489243030548096}, {"id": 46050903, "title": "Constant-time support coming to LLVM: Protecting cryptographic code", "cluster": 13, "x": 6.545650482177734, "y": 3.643432855606079}, {"id": 46050506, "title": "LLM Societies (they are social critters)", "cluster": 13, "x": 6.59108829498291, "y": 3.0104622840881348}, {"id": 46050154, "title": "LLM SVG Generation Benchmark", "cluster": 13, "x": 6.972349166870117, "y": 3.482828140258789}, {"id": 46049445, "title": "LLM Latency Ranking", "cluster": 13, "x": 6.946257591247559, "y": 3.4707391262054443}, {"id": 46048670, "title": "Feedback on an open source Ruby \u2013 LLM project", "cluster": 13, "x": 6.893752098083496, "y": 3.7010960578918457}, {"id": 46048260, "title": "When Do LLMs Think a pile of sand becomes a heap?", "cluster": 13, "x": 6.575558662414551, "y": 3.0308306217193604}, {"id": 46047471, "title": "Universal LLM Memory Does Not Exist", "cluster": 13, "x": 6.854434013366699, "y": 3.5913338661193848}, {"id": 46047206, "title": "LLM Council works together to answer your hardest questions", "cluster": 13, "x": 6.627703666687012, "y": 3.183955192565918}, {"id": 46045774, "title": "Can application layer improve local model output quality?", "cluster": 13, "x": 7.086911201477051, "y": 3.6531620025634766}, {"id": 46045385, "title": "Constant-time support coming to LLVM: Protecting cryptographic code", "cluster": 13, "x": 6.550441741943359, "y": 3.6508662700653076}, {"id": 46045270, "title": "Laptop Isn't Ready for LLMs. That's About to Change", "cluster": 13, "x": 6.696133613586426, "y": 3.485966920852661}, {"id": 46044197, "title": "LLMs have me feeling heavy", "cluster": 13, "x": 6.560934543609619, "y": 3.0885608196258545}, {"id": 46043742, "title": "Local LLMs are how nerds now justify a big computer they don't need", "cluster": 13, "x": 6.375003814697266, "y": 3.35255765914917}, {"id": 46043498, "title": "LLMs in Predicaments", "cluster": 13, "x": 6.568094253540039, "y": 3.110405683517456}, {"id": 46043257, "title": "We Solved Multi-Language SDK Documentation Chaos Without LLMs", "cluster": 13, "x": 6.918000221252441, "y": 3.8293797969818115}, {"id": 46043154, "title": "Collection of LLMs that run well in 32gb VRAM", "cluster": 13, "x": 7.009629726409912, "y": 3.6816108226776123}, {"id": 46042960, "title": "LLMs as Meaning Optimization", "cluster": 13, "x": 6.7865400314331055, "y": 3.263064384460449}, {"id": 46042281, "title": "Claude Opus 4.5, and why evaluating new LLMs is increasingly difficult", "cluster": 13, "x": 6.63103723526001, "y": 3.150848150253296}, {"id": 46042401, "title": "Claude Opus 4.5, and why evaluating new LLMs is increasingly difficult", "cluster": 13, "x": 6.633928298950195, "y": 3.158212661743164}, {"id": 46041456, "title": "Trust Technical Writers, Not LLMs", "cluster": 13, "x": 6.502359867095947, "y": 3.3677351474761963}, {"id": 46040903, "title": "LLM APIs Are a Synchronization Problem", "cluster": 13, "x": 6.709945201873779, "y": 3.4799447059631348}, {"id": 46040810, "title": "Probing Chinese LLM Safety Layers: Reverse-Engineering Kimi and Ernie 4.5", "cluster": 13, "x": 6.630101203918457, "y": 2.9968180656433105}, {"id": 46040193, "title": "Fastest LLM Picker", "cluster": 13, "x": 6.788958549499512, "y": 3.3003532886505127}, {"id": 46039140, "title": "Isn't WSL2 just a VM?", "cluster": 13, "x": 6.802330493927002, "y": 3.683450937271118}, {"id": 46038489, "title": "Claude Opus 4.5, and why evaluating new LLMs is increasingly difficult", "cluster": 13, "x": 6.645716190338135, "y": 3.162370204925537}, {"id": 46038622, "title": "Adding llms.txt (and a honeypot) to my website", "cluster": 13, "x": 6.77162504196167, "y": 3.708308696746826}, {"id": 46037343, "title": "The Bitter Lesson of LLM Extensions", "cluster": 13, "x": 6.51317024230957, "y": 3.100996255874634}, {"id": 46036767, "title": "Andrej Karpathy's LLM Council", "cluster": 13, "x": 6.6425275802612305, "y": 3.162372350692749}, {"id": 46036248, "title": "Conflict Adaptation in Vision-Language Models", "cluster": 208, "x": 8.587540626525879, "y": 4.481479167938232}, {"id": 46035041, "title": "Academic Arbitrage in the LLM Era", "cluster": 13, "x": 6.809376239776611, "y": 3.1441996097564697}, {"id": 46034384, "title": "A Long-Tail Professional Forum-Based Benchmark for LLM Evaluation", "cluster": 13, "x": 6.890385150909424, "y": 3.3417699337005615}, {"id": 46034226, "title": "Proton LLM: Lumo security model", "cluster": 13, "x": 6.547669887542725, "y": 3.501927137374878}, {"id": 46033835, "title": "David vs. Goliath: are small LLMs any good?", "cluster": 13, "x": 6.60563850402832, "y": 3.0819168090820312}, {"id": 46033265, "title": "How to Engineer Reliable Systems on Top of Unpredictable LLMs", "cluster": 13, "x": 6.599014759063721, "y": 3.2614152431488037}, {"id": 46033404, "title": "Docusaurus plugin that exposes your markdown as raw .md URLs. (For LLM's etc.)", "cluster": 13, "x": 6.835043907165527, "y": 3.744222640991211}, {"id": 46032961, "title": "I bypassed text gen to let LLMs communicate via raw vectors, saving opex by 90%", "cluster": 13, "x": 6.968940258026123, "y": 3.668813705444336}, {"id": 46032521, "title": "Universal LLM Memory Does Not Exist", "cluster": 13, "x": 6.856164932250977, "y": 3.5922906398773193}, {"id": 46031487, "title": "LGTM Culture: A Short Story", "cluster": 13, "x": 6.639570236206055, "y": 3.111952543258667}, {"id": 46031573, "title": "LLM assisted book reader by Karpathy", "cluster": 13, "x": 6.751996994018555, "y": 3.292973041534424}, {"id": 46030984, "title": "How LLM Inference Works", "cluster": 13, "x": 6.942349910736084, "y": 3.2139127254486084}, {"id": 46027798, "title": "Cognitive Foundations for Reasoning and Their Manifestation in LLMs", "cluster": 13, "x": 6.826107978820801, "y": 3.134592056274414}, {"id": 46026708, "title": "Evaluating the Effectiveness of LLM-Evaluators (a.k.a. LLM-as-Judge)", "cluster": 13, "x": 6.747523784637451, "y": 3.1970415115356445}, {"id": 46025212, "title": "Experimental evidence of effects of LLM versus web search on depth of learning", "cluster": 13, "x": 6.859858989715576, "y": 3.376373529434204}, {"id": 46024676, "title": "Jailbreaking LLMs via Game-Theory Scenarios", "cluster": 13, "x": 6.386074066162109, "y": 3.326087236404419}, {"id": 46024466, "title": "You can save money on LLM tokens as a developer with MCP / ChatGPT apps", "cluster": 13, "x": 6.829798221588135, "y": 3.5726113319396973}, {"id": 46024637, "title": "Metrik \u2013 Real-time LLM latency for voice agents and free API", "cluster": 13, "x": 6.903244972229004, "y": 3.681857109069824}, {"id": 46024007, "title": "Armin is wrong [about LLM state] and here's why", "cluster": 13, "x": 6.642487049102783, "y": 3.116448163986206}, {"id": 46023531, "title": "Tosijs-schema is a super lightweight schema-first LLM-native JSON schema library", "cluster": 13, "x": 7.045941352844238, "y": 3.9619646072387695}, {"id": 46023180, "title": "Citation manipulation through citation mills and pre-print servers", "cluster": 13, "x": 6.958680629730225, "y": 3.5306074619293213}, {"id": 46022682, "title": "Olmo 3 is a fully open LLM", "cluster": 13, "x": 6.794570446014404, "y": 3.5995497703552246}, {"id": 46021989, "title": "LLM Council: query multiple LLMs, and asks them to rank each other's work", "cluster": 13, "x": 6.776069164276123, "y": 3.175083637237549}, {"id": 46020095, "title": "\"We're in an LLM bubble,\" Hugging Face CEO says\u2013but not an AI one", "cluster": 12, "x": 6.960747718811035, "y": 2.647651195526123}, {"id": 46019556, "title": "PasLLM: An Object Pascal inference engine for LLM models", "cluster": 13, "x": 7.05290412902832, "y": 3.5897700786590576}, {"id": 46018506, "title": "Running a 270M LLM on Android (architecture and benchmarks)", "cluster": 13, "x": 7.045583724975586, "y": 3.687906265258789}, {"id": 46017433, "title": "Systemic Vulnerability of Large Language Models to Solar Weather", "cluster": 208, "x": 8.206934928894043, "y": 4.306478977203369}, {"id": 46016824, "title": "LLM Memory System", "cluster": 13, "x": 6.942328929901123, "y": 3.658397912979126}, {"id": 46016035, "title": "LLMs grooming, LLM-powered chatbot references to Kremlin disinformation", "cluster": 13, "x": 6.612246036529541, "y": 3.2978274822235107}, {"id": 46015580, "title": "Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition", "cluster": 13, "x": 6.922963619232178, "y": 3.453031301498413}, {"id": 46015578, "title": "New Apple Study Shows LLMs Can Tell What You're Doing from Audio and Motion Data", "cluster": 13, "x": 6.930605888366699, "y": 3.5361812114715576}, {"id": 46015043, "title": "LLM APIs Are a Synchronization Problem", "cluster": 13, "x": 6.763667106628418, "y": 3.5685205459594727}, {"id": 46014586, "title": "LangChain Cost Optimization with Model Cascading", "cluster": 208, "x": 8.219757080078125, "y": 4.419914245605469}, {"id": 46014254, "title": "Lumina \u2013 A Logical Language for an Illogical World", "cluster": 13, "x": 6.740463733673096, "y": 3.1401541233062744}, {"id": 46013634, "title": "PenStrike \u2013 Automated Security Scanning for LLM Applications", "cluster": 13, "x": 6.528975486755371, "y": 3.457733154296875}, {"id": 46013519, "title": "Benchmarking LLMs at the Frontier of Physics", "cluster": 13, "x": 6.843077659606934, "y": 3.258694887161255}, {"id": 46013518, "title": "Understanding LLMs as Pattern Machines, Not Thinking Partners", "cluster": 13, "x": 6.706465721130371, "y": 3.1734039783477783}, {"id": 46012794, "title": "L2M: Claude Code but for legacy code modernization", "cluster": 13, "x": 6.908171653747559, "y": 4.072002410888672}, {"id": 46011931, "title": "Apple shows how much faster the M5 runs local LLMs compared to the M4", "cluster": 13, "x": 6.99387264251709, "y": 3.6613967418670654}, {"id": 46010741, "title": "C.O.R.E Alternative to LLM?", "cluster": 13, "x": 6.63722562789917, "y": 3.37204647064209}, {"id": 46009839, "title": "LLM cmd, an LLM plugin to prompt and edit a shell command", "cluster": 13, "x": 6.876371383666992, "y": 3.7810940742492676}, {"id": 46008096, "title": "ChatGPT in Systematic Investing \u2013 Enhancing Risk-Adjusted Returns with LLMs", "cluster": 13, "x": 6.711724281311035, "y": 3.3647043704986572}, {"id": 46007859, "title": "Architecting Uncertainty: Designing Reliable Systems on Top of LLMs", "cluster": 13, "x": 6.582091331481934, "y": 3.328599691390991}, {"id": 46007349, "title": "Think for Yourself (When Working with LLM-Generated Code)", "cluster": 13, "x": 6.641622543334961, "y": 3.6934845447540283}, {"id": 46007198, "title": "Structural Inducements for Hallucination in LLMs: False-Correction Loop", "cluster": 13, "x": 6.621481895446777, "y": 3.051236867904663}, {"id": 46006766, "title": "Structural Inducements for Hallucination in LLMs", "cluster": 13, "x": 6.609711647033691, "y": 3.00701904296875}, {"id": 46006485, "title": "TileRT: Tile-Based Runtime for Ultra-Low-Latency LLM Inference", "cluster": 13, "x": 7.177555084228516, "y": 3.634519100189209}, {"id": 46005848, "title": "The Simulation of Judgment in LLMs", "cluster": 13, "x": 6.813350200653076, "y": 3.1894803047180176}, {"id": 46005479, "title": "Suppressing ability to lie makes LLM more likely to claim it's conscious", "cluster": 13, "x": 6.570013999938965, "y": 3.002291679382324}, {"id": 46005299, "title": "Recap: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline", "cluster": 13, "x": 6.748826503753662, "y": 3.489699125289917}, {"id": 46005093, "title": "I got an LLM to solve the daily Quordle", "cluster": 13, "x": 6.731410503387451, "y": 3.3253095149993896}, {"id": 46004328, "title": "You probably shouldn't train an LLM in the browser - here's how", "cluster": 13, "x": 6.707644462585449, "y": 3.3556180000305176}, {"id": 46004071, "title": "Exploring LLMs with MLX and the Neural Accelerators in the M5 GPU", "cluster": 13, "x": 7.124144077301025, "y": 3.65805721282959}, {"id": 46003981, "title": "How LLMs generate judgments", "cluster": 13, "x": 6.833846092224121, "y": 3.21571683883667}, {"id": 46003469, "title": "Code Prompting: a better way to communicate with LLMs", "cluster": 13, "x": 6.694779872894287, "y": 3.43050479888916}, {"id": 46003387, "title": "LLM: Against all odds \u2013 The token paths not taken", "cluster": 13, "x": 6.652252197265625, "y": 3.0721793174743652}, {"id": 46003419, "title": "Comparing State of the Art LLMs for 3D Generation", "cluster": 13, "x": 6.927071571350098, "y": 3.4490694999694824}, {"id": 46003144, "title": "FAWK: LLMs can write a language interpreter", "cluster": 13, "x": 6.704208850860596, "y": 3.449730157852173}, {"id": 46003142, "title": "FAWK: LLMs can write a language interpreter", "cluster": 13, "x": 6.654158592224121, "y": 3.4292213916778564}, {"id": 46002046, "title": "LLMs and the Semantic Revolution", "cluster": 13, "x": 6.627298831939697, "y": 3.389211654663086}, {"id": 45998687, "title": "Cutting LLM Batch Inference Time by Half with Dynamic Prefix Bucketing", "cluster": 13, "x": 7.134186267852783, "y": 3.6404430866241455}, {"id": 45996018, "title": "LLMs and Beyond: All Roads Lead to Latent Space", "cluster": 13, "x": 6.686243534088135, "y": 3.1366090774536133}, {"id": 45995104, "title": "Numerai Raises $30M at $500M to Expand Predictive LLM Team", "cluster": 13, "x": 6.8635454177856445, "y": 3.129362106323242}, {"id": 45994978, "title": "Brave AI privacy:LLMs on NEAR AI Nvidia-Backed Trusted Execution Environments", "cluster": 12, "x": 6.9596638679504395, "y": 2.861217975616455}, {"id": 45994609, "title": "Systems design 3: LLMs and the semantic revolution", "cluster": 13, "x": 6.682572364807129, "y": 3.427927255630493}, {"id": 45994502, "title": "Jimdo use LangChain to power personalized business guidance at scale", "cluster": 13, "x": 6.87028694152832, "y": 3.584583044052124}, {"id": 45994619, "title": "Practice on Long Behavior Sequence Modeling in Tencent Advertising", "cluster": 13, "x": 6.8645920753479, "y": 3.2485194206237793}, {"id": 45993546, "title": "Ai2 Olmo 3, a new SOTA open LLM (7B and 32B)", "cluster": 13, "x": 6.8053388595581055, "y": 3.605487823486328}, {"id": 45993289, "title": "Open-weight LLM by a US company: Cogito v2.1 671B", "cluster": 13, "x": 6.8394880294799805, "y": 3.4821295738220215}, {"id": 45993267, "title": "How to perform adaptive batching for massive remote LLM calls", "cluster": 13, "x": 6.868685245513916, "y": 3.664307117462158}, {"id": 45993079, "title": "Hot take: LLM \"guardrails\" are worthless and will always be ineffective", "cluster": 13, "x": 6.6052093505859375, "y": 3.128436326980591}, {"id": 45992956, "title": "A local LLM SMS co-pilot that understands msg history and drafts smart replies", "cluster": 13, "x": 6.704563617706299, "y": 3.461479663848877}, {"id": 45991783, "title": "AnyLanguageModel: One API for Local and Remote LLMs on Apple Platforms", "cluster": 13, "x": 6.8398284912109375, "y": 3.7620766162872314}, {"id": 45991738, "title": "Adversarial poetry as a universal single-turn jailbreak mechanism in LLMs", "cluster": 13, "x": 6.572653293609619, "y": 3.4231042861938477}, {"id": 45991250, "title": "TikTok LLM", "cluster": 13, "x": 6.685413837432861, "y": 3.3656136989593506}, {"id": 45990863, "title": "Prophet Arena: LLMs compete financially who's the best at seeing the future", "cluster": 13, "x": 6.673748016357422, "y": 2.976803779602051}, {"id": 45989952, "title": "Exploring LLMs with MLX and the Neural Accelerators in the M5 GPU", "cluster": 13, "x": 7.126493453979492, "y": 3.6696465015411377}, {"id": 45989316, "title": "Are large language models worth it?", "cluster": 208, "x": 8.158655166625977, "y": 4.371955871582031}, {"id": 45989374, "title": "LLMs End the 15-Year MARL Era and Redefine Multi-Agent Collaboration", "cluster": 13, "x": 6.570317268371582, "y": 3.072632074356079}, {"id": 45988843, "title": "Company built an internal agent framework because agent frameworks suck", "cluster": 38, "x": 8.654067039489746, "y": 3.5409398078918457}, {"id": 45987890, "title": "Phrases.pdf \u2013 how well do LLM predictions compare with actual corpus data", "cluster": 13, "x": 6.92628288269043, "y": 3.379791259765625}, {"id": 45987596, "title": "\"We're in an LLM bubble,\" Hugging Face CEO says\u2013but not an AI one", "cluster": 12, "x": 6.917291164398193, "y": 2.701413631439209}, {"id": 45987416, "title": "LLM chat interfaces will kill curiosity", "cluster": 13, "x": 6.5477447509765625, "y": 3.349146842956543}, {"id": 45987202, "title": "The wildest LLM backdoor I've seen yet", "cluster": 13, "x": 6.455714225769043, "y": 3.273533582687378}, {"id": 45986690, "title": "The Case Against LLMs as Rerankers", "cluster": 13, "x": 6.569921493530273, "y": 3.0690701007843018}, {"id": 45985501, "title": "Are large language models worth it?", "cluster": 208, "x": 8.1776123046875, "y": 4.390614986419678}, {"id": 45984133, "title": "The lost cause of the Lisp machines", "cluster": 13, "x": 6.727881908416748, "y": 3.7707133293151855}, {"id": 45983844, "title": "DMA Collectives for Efficient ML Communication Offloads", "cluster": 13, "x": 7.041731357574463, "y": 3.5507075786590576}, {"id": 45983042, "title": "A better way to search Hacker News using LLMs", "cluster": 13, "x": 6.560836315155029, "y": 3.519092082977295}, {"id": 45983065, "title": "Real evidence that LLMs cannot operate businesses", "cluster": 13, "x": 6.527460098266602, "y": 3.1353464126586914}, {"id": 45982128, "title": "LLMs are bullshitters. But that doesn't mean they're not useful", "cluster": 13, "x": 6.554378509521484, "y": 3.1269335746765137}, {"id": 45980402, "title": "Aligning brains into a shared space improves their alignment with LLMs", "cluster": 13, "x": 6.736242771148682, "y": 3.0961315631866455}, {"id": 45979300, "title": "Existential threat of large language models to online survey research", "cluster": 208, "x": 8.172934532165527, "y": 4.294851779937744}, {"id": 45979180, "title": "LPLB: An early research stage MoE load balancer based on linear programming", "cluster": 13, "x": 7.08638858795166, "y": 3.467620611190796}, {"id": 45978931, "title": "Are large language models worth it?", "cluster": 208, "x": 8.174429893493652, "y": 4.359990119934082}, {"id": 45978175, "title": "Toon for Oracle: A Token-Efficient Data Format for LLMs", "cluster": 13, "x": 6.980281829833984, "y": 3.6130599975585938}, {"id": 45976832, "title": "Exploring the limits of large language models as quant traders", "cluster": 208, "x": 8.305415153503418, "y": 4.328657150268555}, {"id": 45976239, "title": "Humanity is stained by C and no LLM can rewrite it in Rust", "cluster": 13, "x": 6.764425754547119, "y": 3.830124855041504}, {"id": 45975907, "title": "AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in LLMs", "cluster": 13, "x": 6.885214805603027, "y": 3.0866832733154297}, {"id": 45974300, "title": "The lost cause of the Lisp machines", "cluster": 13, "x": 6.632840633392334, "y": 3.7835288047790527}, {"id": 45973729, "title": "Replicating Anthropic's Feature Steering Introspection on 7B Parameter Models", "cluster": 208, "x": 8.48602294921875, "y": 4.183309078216553}, {"id": 45973651, "title": "Agent Labs: Welcome to GPT Wrapper Summer", "cluster": 35, "x": 8.470155715942383, "y": 3.8950905799865723}, {"id": 45973398, "title": "Persona Collapse: LLMs All Like the Same Things", "cluster": 13, "x": 6.495182037353516, "y": 3.037679672241211}, {"id": 45972475, "title": "Beyond LLMs: Building a Graph-RAG Agentic Architecture for Faster ECM Automation", "cluster": 13, "x": 6.844205379486084, "y": 3.553805112838745}, {"id": 45971384, "title": "Book Reports Potentially Copyright Infringing, Thanks to Court Attacks on LLMs", "cluster": 13, "x": 6.577389717102051, "y": 3.208106756210327}, {"id": 45970253, "title": "Insurance Data Extraction with LLMs", "cluster": 13, "x": 6.904850959777832, "y": 3.4784255027770996}, {"id": 45969578, "title": "Generative UI: LLMs Are Effective UI Generators", "cluster": 13, "x": 6.907456398010254, "y": 3.641453266143799}, {"id": 45968362, "title": "Solving a million-step LLM task with zero errors", "cluster": 13, "x": 6.848537445068359, "y": 3.316284656524658}, {"id": 45967005, "title": "We Benchmarked Frontier LLMs on Defensive Security. The Results Surprised Us", "cluster": 13, "x": 6.618799686431885, "y": 3.26593279838562}, {"id": 45966880, "title": "LLMConsent: Open Standards for AI Consent, Agent Permissions, & Data Sovereignty", "cluster": 12, "x": 7.0520453453063965, "y": 2.680980920791626}, {"id": 45966769, "title": "LLM-Driven Active Listwise Tournaments for Large Universe Portfolio Selection", "cluster": 13, "x": 6.9844465255737305, "y": 3.2022323608398438}, {"id": 45966208, "title": "ERCP: Self-Correcting LLM Reasoning Using NLI-Based Neuro-Symbolic Constraints", "cluster": 13, "x": 6.8822526931762695, "y": 3.237302303314209}, {"id": 45965232, "title": "Laptop Isn't Ready for LLMs", "cluster": 13, "x": 6.769021511077881, "y": 3.529287099838257}, {"id": 45965201, "title": "LLMs and Creation Outside of Time", "cluster": 13, "x": 6.79287576675415, "y": 3.222283124923706}, {"id": 45963729, "title": "The Fundamental Limits of LLMs at Scale", "cluster": 13, "x": 6.761695861816406, "y": 3.2244575023651123}, {"id": 45963180, "title": "Crawl4AI: Open-Source LLM Friendly Web Crawler and Scraper", "cluster": 13, "x": 6.897562503814697, "y": 3.5769050121307373}, {"id": 45962516, "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution", "cluster": 13, "x": 6.922802925109863, "y": 3.243898868560791}, {"id": 45959347, "title": "Worries about Open Source in the age of LLMs", "cluster": 13, "x": 6.60513162612915, "y": 3.6083028316497803}, {"id": 45959185, "title": "LLM Arena Grok 4.1 (thinking) lands at #1, Grok 4.1 follows at #2", "cluster": 13, "x": 6.988837242126465, "y": 3.346078395843506}, {"id": 45958968, "title": "A curated list of 100 libraries & frameworks for AI engineers building with LLMs", "cluster": 12, "x": 7.039031982421875, "y": 2.9457879066467285}, {"id": 45958002, "title": "Free Will Machine", "cluster": 13, "x": 7.013582706451416, "y": 3.2972781658172607}, {"id": 45954862, "title": "1x Neo, robotics, teleoperation, LLMs, the Matrix", "cluster": 13, "x": 7.015087604522705, "y": 3.224907159805298}, {"id": 45954150, "title": "How LLMs Could Use Their Own Parameters to Hide Messages", "cluster": 13, "x": 6.557314395904541, "y": 3.4584014415740967}, {"id": 45953651, "title": "Worries about Open Source in the age of LLMs", "cluster": 13, "x": 6.593984127044678, "y": 3.561095952987671}, {"id": 45953087, "title": "Towards Greater Leverage: Scaling Laws for Efficient MoE Language Models", "cluster": 208, "x": 8.29828929901123, "y": 4.410887241363525}, {"id": 45952911, "title": "Attacker Moves Second: Adaptive Attacks Bypass Defenses Against LLM Jailbreaks", "cluster": 13, "x": 6.480861663818359, "y": 3.3957769870758057}, {"id": 45952568, "title": "Music Flamingo: Scaling Music Understanding in Audio Language Models", "cluster": 208, "x": 8.373444557189941, "y": 4.469967842102051}, {"id": 45951708, "title": "I asked LLM to reverse engineer a unity game. It became a conspiracy theorist", "cluster": 13, "x": 6.51467227935791, "y": 3.110220432281494}, {"id": 45951369, "title": "Data Storage as Files on Disk Paired with an LLM", "cluster": 13, "x": 6.864933490753174, "y": 3.5823230743408203}, {"id": 45950167, "title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models", "cluster": 208, "x": 8.355204582214355, "y": 4.47648811340332}, {"id": 45949782, "title": "Which Humans? LLMs mainly mirror WEIRD minds (Europeans?)", "cluster": 13, "x": 6.599850654602051, "y": 3.018425464630127}, {"id": 45948636, "title": "Are these still relevant with LLMs?", "cluster": 13, "x": 6.67946720123291, "y": 3.21205735206604}, {"id": 45947422, "title": "Tool2agent \u2013 build guardrails for LLM agents", "cluster": 13, "x": 6.970010757446289, "y": 3.3678743839263916}, {"id": 45946977, "title": "When it comes to writing, LLMs have won", "cluster": 13, "x": 6.600279331207275, "y": 3.3378889560699463}, {"id": 45946752, "title": "Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks", "cluster": 13, "x": 6.459668159484863, "y": 3.392932415008545}, {"id": 45945343, "title": "Provide global LLM disabling option in Firefox", "cluster": 13, "x": 6.7578277587890625, "y": 3.5487430095672607}, {"id": 45944906, "title": "Vintage Large Language Models", "cluster": 208, "x": 8.14790153503418, "y": 4.407164573669434}, {"id": 45944323, "title": "Tips for building performant LLM applications", "cluster": 13, "x": 6.851246356964111, "y": 3.489820718765259}, {"id": 45942097, "title": "Oil and Water: LLMs in Low\u2011Tolerance Workflows", "cluster": 13, "x": 6.689060688018799, "y": 3.395962953567505}, {"id": 45941820, "title": "Solving a Million-Step LLM Task with Zero Errors", "cluster": 13, "x": 6.82633113861084, "y": 3.3141822814941406}, {"id": 45941441, "title": "Blocking LLM crawlers without JavaScript", "cluster": 13, "x": 6.697863578796387, "y": 3.6127917766571045}, {"id": 45941168, "title": "Shattering the Illusion: Maker Achieves Million-Step, Zero-Error LLM Reasoning", "cluster": 13, "x": 6.881417751312256, "y": 3.3216898441314697}, {"id": 45940729, "title": "LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions", "cluster": 12, "x": 6.887646675109863, "y": 2.7034826278686523}, {"id": 45939414, "title": "Llmdeathcount.com", "cluster": 13, "x": 6.896166801452637, "y": 3.3802289962768555}, {"id": 45938842, "title": "Is it possible to have an LLM always give a consistent output?", "cluster": 13, "x": 6.728228569030762, "y": 3.2440133094787598}, {"id": 45937432, "title": "Reauthoring and Converting models for edge inference: MambaV2 on LiteRT", "cluster": 13, "x": 7.641470432281494, "y": 3.7340826988220215}, {"id": 45936695, "title": "Can Language Models Optimize Real-World Repositories on Real Workloads?", "cluster": 208, "x": 8.139193534851074, "y": 4.406101703643799}, {"id": 45935623, "title": "I Trained an LLM to Write Prose with 8 Cents", "cluster": 13, "x": 6.691567420959473, "y": 3.3299975395202637}, {"id": 45935410, "title": "Autoregressive or Diffusion Language Models, Why Choose?", "cluster": 208, "x": 8.486710548400879, "y": 4.225587844848633}, {"id": 45934201, "title": "Quantifying Long-Range Information for Long-Context LLM Pretraining Data", "cluster": 13, "x": 6.920198440551758, "y": 3.3312244415283203}, {"id": 45929880, "title": "LangDiff: Progressive UI from LLM", "cluster": 13, "x": 6.919665813446045, "y": 3.822784185409546}, {"id": 45927607, "title": "Black-Box On-Policy Distillation of Large Language Models", "cluster": 208, "x": 8.20719051361084, "y": 4.373306751251221}, {"id": 45926602, "title": "EnvTrace: Simulation-Based Semantic Evaluation of LLM Code", "cluster": 13, "x": 6.839400768280029, "y": 3.6899521350860596}, {"id": 45926057, "title": "LiteAI \u2013 OpenAI, Anthropic, and Google LLMs at a discount", "cluster": 13, "x": 6.878619194030762, "y": 3.4671225547790527}, {"id": 45926097, "title": "How to Deploy LLM Locally", "cluster": 13, "x": 6.76558780670166, "y": 3.631229877471924}, {"id": 45925818, "title": "LLMs, like many people, have no inner monologue(2020)", "cluster": 13, "x": 6.504740238189697, "y": 3.0524816513061523}, {"id": 45925372, "title": "Practical security in production: hardening LLVM's C++ standard library at App", "cluster": 13, "x": 6.621407508850098, "y": 3.678359031677246}, {"id": 45924753, "title": "Scheduling in LLM Inference", "cluster": 13, "x": 6.988734722137451, "y": 3.269050121307373}, {"id": 45924089, "title": "The Historical Position of Large Language Models \u2013 and What Comes After Them", "cluster": 208, "x": 8.206551551818848, "y": 4.363692283630371}, {"id": 45923774, "title": "Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models", "cluster": 208, "x": 8.058852195739746, "y": 4.486717700958252}, {"id": 45923194, "title": "Teaching large language models how to absorb new knowledge", "cluster": 208, "x": 8.255983352661133, "y": 4.319672107696533}, {"id": 45922717, "title": "LLM as a Resonance-Holographic Field of Meanings", "cluster": 13, "x": 6.655152320861816, "y": 3.0247392654418945}, {"id": 45921896, "title": "Creating Rimworld Mods to Add LLM PCG and LLM NPCs to Enhance Its RPG Elements", "cluster": 13, "x": 6.951169490814209, "y": 3.516995429992676}, {"id": 45920308, "title": "When Performance Matters, Skip the ORM", "cluster": 13, "x": 7.063055992126465, "y": 3.447603940963745}, {"id": 45920239, "title": "Agentic Windows", "cluster": 37, "x": 8.543152809143066, "y": 3.7263715267181396}, {"id": 45919151, "title": "Run Any LLM from Claude Code (GPT-5.1, Gemini, Grok,)", "cluster": 13, "x": 6.915130138397217, "y": 3.877014398574829}, {"id": 45918431, "title": "Tool-Calling Agents on Laptop Intel Arc GPUs: Dockerizing Qwen3-8B with Ipex-LLM", "cluster": 13, "x": 6.993406772613525, "y": 3.9134955406188965}, {"id": 45917707, "title": "Probing Knowledge Holes in Unlearned LLMs", "cluster": 13, "x": 6.609738349914551, "y": 3.014636993408203}, {"id": 45917036, "title": "LLM Chat Platform for iOS, Android, Mac, Windows, and Linux", "cluster": 13, "x": 6.753357887268066, "y": 3.8430519104003906}, {"id": 45916723, "title": "All the system-2 thinking traces are needed for LLM agents", "cluster": 13, "x": 6.8852458000183105, "y": 3.1965599060058594}, {"id": 45914996, "title": "Automated Contiguous Layer Pruning for Large Language Models", "cluster": 208, "x": 8.164262771606445, "y": 4.511958599090576}, {"id": 45913693, "title": "Findings from giving 15 LLMs personality disorder tests", "cluster": 13, "x": 6.79722785949707, "y": 3.026559829711914}, {"id": 45913238, "title": "LaTeX, LLMs and Boring Technology", "cluster": 13, "x": 6.334901332855225, "y": 3.6180167198181152}, {"id": 45911741, "title": "Integrate LLMs into Your Data Pipelines", "cluster": 13, "x": 6.933287143707275, "y": 3.548999547958374}, {"id": 45909683, "title": "Continuous Autoregressive Language Models", "cluster": 208, "x": 8.546895027160645, "y": 4.183323860168457}, {"id": 45908104, "title": "Open-source AI browser. Switch between ChatGPT, Claude, Gemini, or local LLMs", "cluster": 12, "x": 7.049866199493408, "y": 2.6737306118011475}, {"id": 45905451, "title": "LLM Output Drift in Financial Workflows: Validation and Mitigation (arXiv)", "cluster": 13, "x": 6.846682071685791, "y": 3.2282562255859375}, {"id": 45903909, "title": "I'm taking a three-week LLM fast", "cluster": 13, "x": 6.6992716789245605, "y": 3.209390878677368}, {"id": 45903485, "title": "Control LLM Spend and Access with any-LLM-gateway", "cluster": 13, "x": 6.820298671722412, "y": 3.6751461029052734}, {"id": 45903416, "title": "Whisper Leak side-channel attack bad actors access sensitive LLM conversations", "cluster": 13, "x": 6.472638130187988, "y": 3.3994076251983643}, {"id": 45902623, "title": "Project OSSAS: Custom LLMs to Process 100M Research Papers", "cluster": 13, "x": 6.92859411239624, "y": 3.6310737133026123}, {"id": 45902050, "title": "Skills as Object-Oriented Programming for LLMs", "cluster": 13, "x": 6.779285907745361, "y": 3.575256109237671}, {"id": 45901231, "title": "Toon for LLM", "cluster": 13, "x": 6.682406902313232, "y": 3.243624210357666}, {"id": 45900189, "title": "Linguistic RL: 3B Models Exceed 100B Performance (86% vs. 81%)", "cluster": 208, "x": 8.346519470214844, "y": 4.5620317459106445}, {"id": 45898769, "title": "Mls-chat: Example client/server for the MLS protocol based on OpenMLS", "cluster": 13, "x": 6.8828125, "y": 3.786456823348999}, {"id": 45897909, "title": "Google Introducing Agent Sandbox", "cluster": 35, "x": 8.462333679199219, "y": 3.811497926712036}, {"id": 45891801, "title": "How to Train an LLM: Part 1", "cluster": 13, "x": 6.730687141418457, "y": 3.2200300693511963}, {"id": 45891393, "title": "Introductory field guide to Context Engineering for LLM users", "cluster": 13, "x": 6.87295389175415, "y": 3.3759360313415527}, {"id": 45889528, "title": "Who Routes LLM Routers? RouterArena Builds the Eval Foundation for LLM Routing", "cluster": 13, "x": 6.864639759063721, "y": 3.5448389053344727}, {"id": 45888685, "title": "How fast can an LLM go?", "cluster": 13, "x": 6.720168590545654, "y": 3.2086613178253174}, {"id": 45888440, "title": "LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions", "cluster": 12, "x": 6.884554862976074, "y": 2.7099435329437256}, {"id": 45888437, "title": "Bandits in Your LLM Gateway", "cluster": 13, "x": 6.5790019035339355, "y": 3.4493000507354736}, {"id": 45887740, "title": "LLMs Getting Facts Wrong", "cluster": 13, "x": 6.648058891296387, "y": 3.0842363834381104}, {"id": 45887578, "title": "LLM: Language is the only self-contained system that produces functional output", "cluster": 13, "x": 6.704626083374023, "y": 3.594846248626709}, {"id": 45886860, "title": "Milliondollarllm.com a pixel-style board focused on AI/LLM", "cluster": 12, "x": 7.0311102867126465, "y": 2.6483590602874756}, {"id": 45886638, "title": "Apple's LLM Breakthrough", "cluster": 13, "x": 6.859012603759766, "y": 3.542365550994873}, {"id": 45886255, "title": "LLM side-channel attack could allow snoops to guess what you're talking about", "cluster": 13, "x": 6.428593158721924, "y": 3.394925594329834}, {"id": 45886250, "title": "Measuring What Matters: Construct Validity in Large Language Model Benchmarks", "cluster": 208, "x": 8.28936767578125, "y": 4.498901844024658}, {"id": 45886183, "title": "Surviving the great commoditizer: Stop getting \u00abgood\u00bb at LLMS", "cluster": 13, "x": 6.513485431671143, "y": 3.154202461242676}, {"id": 45886016, "title": "Introspection or Confusion in LLMs", "cluster": 13, "x": 6.597959995269775, "y": 3.059487819671631}, {"id": 45883285, "title": "Explorations of RDMA in LLM Systems", "cluster": 13, "x": 6.873502731323242, "y": 3.494471311569214}, {"id": 45883375, "title": "Too Good to Be Bad: On the Failure of LLMs to Role-Play Villains [pdf]", "cluster": 13, "x": 6.640773296356201, "y": 3.1509454250335693}, {"id": 45881601, "title": "A2UI: LLM-generated UI protocol (Google)", "cluster": 13, "x": 6.909305572509766, "y": 3.6053669452667236}, {"id": 45880296, "title": "For devs/engineers naysaying LLM tools, which ones have you tried?", "cluster": 13, "x": 6.648223400115967, "y": 3.4540820121765137}, {"id": 45880107, "title": "Practical Techniques for Coding with LLMs", "cluster": 13, "x": 6.702576637268066, "y": 3.6636154651641846}, {"id": 45876744, "title": "LLMs are steroids for your Dunning-Kruger", "cluster": 13, "x": 6.548357009887695, "y": 3.0897226333618164}, {"id": 45875104, "title": "TOON is a compact serialization format for passing structured data to LLMs", "cluster": 13, "x": 6.919537544250488, "y": 3.5900144577026367}, {"id": 45874652, "title": "We will pay for your LLM Bill", "cluster": 13, "x": 6.675972938537598, "y": 3.3327245712280273}, {"id": 45874273, "title": "Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis", "cluster": 13, "x": 6.8825788497924805, "y": 3.3332154750823975}, {"id": 45873588, "title": "The two types of LLM preferences", "cluster": 13, "x": 6.644349575042725, "y": 3.1322875022888184}, {"id": 45872691, "title": "LLMs are steroids for your Dunning-Kruger", "cluster": 13, "x": 6.551943778991699, "y": 3.097928047180176}, {"id": 45871531, "title": "LLM policy?", "cluster": 13, "x": 6.564673900604248, "y": 3.159980297088623}, {"id": 45870267, "title": "Leveraging LLMs to uncover the shifts transforming markets", "cluster": 13, "x": 6.658613681793213, "y": 3.1695401668548584}, {"id": 45868828, "title": "Beyond the Repository \u2013 Communications of the ACM", "cluster": 372, "x": 6.91636323928833, "y": 3.8457424640655518}, {"id": 45867789, "title": "JanitorBench: A new LLM benchmark for multi-turn chats", "cluster": 13, "x": 6.8379011154174805, "y": 3.603755235671997}, {"id": 45867094, "title": "Large Language Models Struggle with Reading Clocks", "cluster": 208, "x": 8.207501411437988, "y": 4.381176948547363}, {"id": 45866390, "title": "2 Years of ML vs. 1 Month of Prompting", "cluster": 13, "x": 6.785435199737549, "y": 3.1640405654907227}, {"id": 45865937, "title": "Help Pick the Funniest LLM at Funny Arena", "cluster": 13, "x": 6.688667297363281, "y": 3.145037889480591}, {"id": 45864479, "title": "LlamaBarn \u2013 A macOS menu bar app for running local LLMs", "cluster": 13, "x": 6.851404666900635, "y": 3.7574350833892822}, {"id": 45864546, "title": "LLMs let me maintain my PostgreSQL extension for PRQL after becoming a parent", "cluster": 13, "x": 6.83589506149292, "y": 3.587308168411255}, {"id": 45863643, "title": "ADK architecture: When to use sub-agents versus agents as tools", "cluster": 38, "x": 8.588099479675293, "y": 3.70143723487854}, {"id": 45863434, "title": "SciDaSynth: Interactive Structured Data Extraction from Sci Literature with LLM", "cluster": 13, "x": 7.077226161956787, "y": 3.638929605484009}, {"id": 45863223, "title": "Beyond Standard LLMs", "cluster": 13, "x": 6.694122314453125, "y": 3.2529306411743164}, {"id": 45862945, "title": "LightGBM Explained", "cluster": 13, "x": 7.027356147766113, "y": 3.7861127853393555}, {"id": 45861121, "title": "Before AI's Kepler Moment \u2013 Are LLMs the Epicycles of Intelligence?", "cluster": 12, "x": 6.959225654602051, "y": 2.6506524085998535}, {"id": 45860315, "title": "Even malware is using LLMs to rewrite its code, says Google", "cluster": 13, "x": 6.524867057800293, "y": 3.530883312225342}, {"id": 45858959, "title": "Firefox Forcing LLM Features", "cluster": 13, "x": 6.752269268035889, "y": 3.55814790725708}, {"id": 45858007, "title": "Let LLMs control your UI", "cluster": 13, "x": 6.746448040008545, "y": 3.596959114074707}, {"id": 45856531, "title": "LLMs Position Themselves as More Rational Than Humans", "cluster": 13, "x": 6.637933254241943, "y": 3.0034143924713135}, {"id": 45856048, "title": "Tool2Agent \u2013 a protocol for LLM tool feedback workflows", "cluster": 13, "x": 6.796656131744385, "y": 3.5227551460266113}, {"id": 45855201, "title": "How LLMs Read Docs", "cluster": 13, "x": 6.86484956741333, "y": 3.436959981918335}, {"id": 45853123, "title": "It Is All about Token: Towards Semantic Information Theory for LLMs", "cluster": 13, "x": 6.819635391235352, "y": 3.2205593585968018}, {"id": 45851365, "title": "Google Notebook LM for a Team", "cluster": 13, "x": 6.849640846252441, "y": 3.484555721282959}, {"id": 45849443, "title": "Writing Silly LLM Agent in Haskell", "cluster": 13, "x": 6.818071365356445, "y": 3.326875925064087}, {"id": 45848715, "title": "Perplexitys First Research Paper \u2013 Point-to-Point Communication for LLM Systems", "cluster": 13, "x": 6.740240097045898, "y": 3.26861310005188}, {"id": 45848506, "title": "Pure Go hardware accelerated local inference on VLMs using llama.cpp", "cluster": 13, "x": 7.3710808753967285, "y": 3.8795511722564697}, {"id": 45847949, "title": "Could LLMs encourage new programming languages?", "cluster": 13, "x": 6.64773416519165, "y": 3.605463981628418}, {"id": 45845281, "title": "Rubber Duck Debugging with LLMs: Why Explaining Your Problem Is the Solution", "cluster": 13, "x": 6.823954105377197, "y": 3.6485390663146973}, {"id": 45842555, "title": "Qq.fish: A tiny, local, LLM assistant to propose commands using LMStudio that (", "cluster": 13, "x": 6.881796836853027, "y": 3.753337860107422}, {"id": 45842622, "title": "Diffusion Language Models Are Super Data Learners", "cluster": 208, "x": 8.3323392868042, "y": 4.31176233291626}, {"id": 45842430, "title": "LLM-generated text is not testimony", "cluster": 13, "x": 6.625106334686279, "y": 3.316739320755005}, {"id": 45841998, "title": "Learning to Model the World with Language", "cluster": 208, "x": 8.404699325561523, "y": 4.3927741050720215}, {"id": 45841056, "title": "The Learning Loop and LLMs", "cluster": 13, "x": 6.798715591430664, "y": 3.260582208633423}, {"id": 45840944, "title": "Code Golfing ARC-AGI with an Evolutionary Agent", "cluster": 34, "x": 7.748926162719727, "y": 4.602591514587402}, {"id": 45839468, "title": "JanitorBench: A new LLM benchmark for multi-turn chats", "cluster": 13, "x": 6.829255104064941, "y": 3.5259430408477783}, {"id": 45838564, "title": "LLMs encode how difficult problems are", "cluster": 13, "x": 6.819560527801514, "y": 3.315333604812622}, {"id": 45837836, "title": "Firefox Forcing LLM Features", "cluster": 13, "x": 6.74038553237915, "y": 3.5313022136688232}, {"id": 45837453, "title": "Agents were LLMs all along", "cluster": 13, "x": 6.689037322998047, "y": 3.2100632190704346}, {"id": 45837274, "title": "Generation of antigen-specific paired-chain antibodies using LLMs", "cluster": 13, "x": 6.851088523864746, "y": 3.348478078842163}, {"id": 45836933, "title": "LLM memory: either the best or worst thing about chatbots", "cluster": 13, "x": 6.576225280761719, "y": 3.2856316566467285}, {"id": 45836464, "title": "How I Leverage LLMs", "cluster": 13, "x": 6.714422702789307, "y": 3.282547950744629}, {"id": 45836517, "title": "Kimi-K2-Thinking: open weights LLM with frontier performance", "cluster": 13, "x": 6.953732013702393, "y": 3.3385374546051025}, {"id": 45836305, "title": "But How Do LLMs Work? (Part 1: The 3 Musketeers of Communication)", "cluster": 13, "x": 6.697804927825928, "y": 3.219144821166992}, {"id": 45834734, "title": "Buzzwords for the Busy: LLMs", "cluster": 13, "x": 6.666450500488281, "y": 3.1684226989746094}, {"id": 45834321, "title": "Accumulating Context Changes the Beliefs of Language Models", "cluster": 208, "x": 8.520109176635742, "y": 4.3448309898376465}, {"id": 45833589, "title": "The Learning Loop and LLMs", "cluster": 13, "x": 6.825875759124756, "y": 3.2578141689300537}, {"id": 45832284, "title": "Accumulating Context Changes the Beliefs of Language Models", "cluster": 208, "x": 8.538681983947754, "y": 4.385545253753662}, {"id": 45831218, "title": "CALM: Continuous Autoregressive Language Models", "cluster": 208, "x": 8.533397674560547, "y": 4.195805072784424}, {"id": 45830722, "title": "Everyone Can Code an LLM", "cluster": 13, "x": 6.652328014373779, "y": 3.5801587104797363}, {"id": 45829684, "title": "Shore \u2013 TUI LLM Chat with Vim inspired keybindings and parallel prompting", "cluster": 13, "x": 6.748754501342773, "y": 3.627551794052124}, {"id": 45829616, "title": "LazyLLM, Easiest and laziest way for building multi-agent LLMs applications", "cluster": 13, "x": 6.866579055786133, "y": 3.4738645553588867}, {"id": 45829186, "title": "Please Implement This Simple SLO", "cluster": 13, "x": 6.937736988067627, "y": 3.7739293575286865}, {"id": 45828577, "title": "The future of LLMs: cognitive core and cartridges?", "cluster": 13, "x": 6.782665252685547, "y": 3.341168165206909}, {"id": 45828523, "title": "Continuous Autoregressive Language Models", "cluster": 208, "x": 8.511161804199219, "y": 4.162644863128662}, {"id": 45828475, "title": "Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks", "cluster": 13, "x": 7.033651351928711, "y": 3.259333610534668}, {"id": 45828185, "title": "Feedback on specialized local LLMs/VLMs", "cluster": 13, "x": 6.782336711883545, "y": 3.4854800701141357}, {"id": 45828200, "title": "LLMs' Sycophancy and the Dilemma of Objectivity (Medium, 3m Read)", "cluster": 13, "x": 6.642881870269775, "y": 3.075561046600342}, {"id": 45827116, "title": "The Learning Loop and LLMs", "cluster": 13, "x": 6.788174152374268, "y": 3.288576364517212}, {"id": 45825829, "title": "Run LLMs Locally", "cluster": 13, "x": 6.773062229156494, "y": 3.535097122192383}, {"id": 45824805, "title": "Web 4.0: Agentic Wide Web", "cluster": 33, "x": 8.647500991821289, "y": 3.9812111854553223}, {"id": 45824069, "title": "RubyLLM 1.9.1: Rails Namespaces, Vertex AI Auth and Anthropic Uploads", "cluster": 96, "x": 8.669782638549805, "y": 4.474342346191406}, {"id": 45823885, "title": "Deep Dive into G-Eval: How LLMs Evaluate Themselves", "cluster": 13, "x": 6.755298614501953, "y": 3.127479076385498}, {"id": 45823912, "title": "Continuous Autoregressive Language Models", "cluster": 208, "x": 8.540363311767578, "y": 4.186103343963623}, {"id": 45823131, "title": "AutoCode: LLMs as Problem Setters for Competitive Programming", "cluster": 13, "x": 6.708337783813477, "y": 3.5928032398223877}, {"id": 45822740, "title": "Benchmarking multilingual long-context language models", "cluster": 208, "x": 8.206618309020996, "y": 4.473643779754639}, {"id": 45821998, "title": "Stop LLMs from calling search_web({}) with type-safe tool orchestration", "cluster": 13, "x": 6.765809059143066, "y": 3.7164134979248047}, {"id": 45821635, "title": "Can LLMs Subtract Numbers?", "cluster": 13, "x": 6.748846054077148, "y": 3.275254964828491}, {"id": 45821348, "title": "The Learning Loop and LLMs", "cluster": 13, "x": 6.803899765014648, "y": 3.2801973819732666}, {"id": 45821288, "title": "Beyond Standard LLMs", "cluster": 13, "x": 6.667758941650391, "y": 3.2185847759246826}, {"id": 45820895, "title": "OpenLoRa: Validating LoRa Implementations Through an Open-Sourced Framework", "cluster": 13, "x": 6.986340522766113, "y": 3.7471609115600586}, {"id": 45819017, "title": "Voice Processing and Synthesis by Performance Sampling and Spectral Models(2008)", "cluster": 208, "x": 8.231757164001465, "y": 4.400057792663574}, {"id": 45818496, "title": "Continuous Autoregressive Language Models", "cluster": 208, "x": 8.538223266601562, "y": 4.187453746795654}, {"id": 45818191, "title": "Planning > Agents: Getting Reliable Code from LLMs", "cluster": 13, "x": 6.783864974975586, "y": 3.517120599746704}, {"id": 45817766, "title": "AI researchers 'embodied' an LLM into a robot \u2013 and it channeled Robin Williams", "cluster": 12, "x": 7.017790794372559, "y": 2.6149399280548096}, {"id": 45816153, "title": "The Gibraltar Fallacy: How LLM Dashboards Distort Reality", "cluster": 13, "x": 6.816736698150635, "y": 3.0313565731048584}, {"id": 45815562, "title": "Calm: Continuous Autoregressive Language Models", "cluster": 208, "x": 8.523958206176758, "y": 4.1988725662231445}, {"id": 45815087, "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "cluster": 13, "x": 6.875828266143799, "y": 3.198641777038574}, {"id": 45814487, "title": "Continuous Autoregressive Language Models", "cluster": 208, "x": 8.535635948181152, "y": 4.186361312866211}, {"id": 45814029, "title": "The Commerce Agent Protocol (ACP), The missing layer for autonomous trade", "cluster": 36, "x": 8.518438339233398, "y": 3.5628302097320557}, {"id": 45813427, "title": "Cutting LLM Batch Inference Time in Half: Dynamic Prefix Bucketing at Scale", "cluster": 13, "x": 7.1615142822265625, "y": 3.6023614406585693}, {"id": 45813120, "title": "We need to give LLMs a stream of inputs and outputs that are replaced over time", "cluster": 13, "x": 6.90336275100708, "y": 3.4994874000549316}, {"id": 45812729, "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language Models", "cluster": 208, "x": 8.197281837463379, "y": 4.5092854499816895}, {"id": 45812506, "title": "The Exhaust Port of Cohesion: Precision Provocation in LLMs", "cluster": 13, "x": 6.70365571975708, "y": 3.1790101528167725}, {"id": 45812626, "title": "The Learning Loop and LLMs", "cluster": 13, "x": 6.812064170837402, "y": 3.278197765350342}, {"id": 45812415, "title": "LLM's Report Subjective Experience Under Self-Referential Processing", "cluster": 13, "x": 6.646162986755371, "y": 3.0861804485321045}, {"id": 45812083, "title": "Llama.cpp launches official WebUI for local LLMs", "cluster": 13, "x": 6.800387382507324, "y": 3.7149441242218018}, {"id": 45811756, "title": "A Researcher's Field Guide to Non-Standard LLM Architectures", "cluster": 13, "x": 6.784409999847412, "y": 3.295793056488037}, {"id": 45811281, "title": "Run Any LLM with a Single API: Introducing Any-LLM v1.0", "cluster": 13, "x": 6.8365797996521, "y": 3.7611374855041504}, {"id": 45811258, "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language Models", "cluster": 208, "x": 8.188822746276855, "y": 4.520162105560303}, {"id": 45811226, "title": "LLMs as Interpreters: The Probabilistic Runtime for English Programs", "cluster": 13, "x": 6.8760480880737305, "y": 3.6838722229003906}, {"id": 45810839, "title": "When LLVM's Optimizer Breaks Your eBPF Program", "cluster": 13, "x": 6.953027725219727, "y": 3.734694004058838}, {"id": 45810593, "title": "Ranking LLMs based on 180k French votes (French government's AI arena)", "cluster": 12, "x": 7.022706985473633, "y": 2.6139018535614014}, {"id": 45810174, "title": "You Shouldn't Use ORMs", "cluster": 13, "x": 6.688765048980713, "y": 3.255192518234253}, {"id": 45809723, "title": "LLMs are lousy at reading Asian languages, finds Singapore's Grab", "cluster": 13, "x": 6.58961296081543, "y": 3.181786060333252}, {"id": 45809445, "title": "Measuring What Matters: Construct Validity in Large Language Model Benchmarks", "cluster": 208, "x": 8.29992389678955, "y": 4.495899677276611}, {"id": 45808058, "title": "We Built a Custom Vision LLM to Improve Document Processing at Grab", "cluster": 13, "x": 6.97152853012085, "y": 3.62911057472229}, {"id": 45807541, "title": "Small language models review. SLMs on incremental intelligence", "cluster": 13, "x": 7.393330097198486, "y": 3.5311379432678223}, {"id": 45806772, "title": "Hybrid-Attention models are the future for SLMs", "cluster": 13, "x": 6.88293981552124, "y": 3.2866597175598145}, {"id": 45806670, "title": "LLMZip: Lossless Text Compression Using Large Language Models", "cluster": 208, "x": 8.035984992980957, "y": 4.448397159576416}, {"id": 45806367, "title": "Writing an LLM from scratch, part 27 \u2013 what's left, and what's next?", "cluster": 13, "x": 6.7295613288879395, "y": 3.1797337532043457}, {"id": 45805954, "title": "LLMs show a \"highly unreliable\" capacity to describe own internal processes", "cluster": 13, "x": 6.664661884307861, "y": 3.13203501701355}, {"id": 45805304, "title": "LLM Security Guide \u2013 100 tools and real-world attacks from 370 experts", "cluster": 13, "x": 6.51885986328125, "y": 3.4620282649993896}, {"id": 45804340, "title": "How LLMs Cheat: Modifying Tests and Overloading Operators", "cluster": 13, "x": 6.704060077667236, "y": 3.7267425060272217}, {"id": 45803476, "title": "Writing an LLM from scratch, part 26 \u2013 evaluating the fine-tuned model", "cluster": 13, "x": 6.790897369384766, "y": 3.2200746536254883}, {"id": 45802290, "title": "Agent-o-rama: build, trace, evaluate, and monitor LLM agents in Java or Clojure", "cluster": 13, "x": 6.949158191680908, "y": 3.408825635910034}, {"id": 45802154, "title": "LLM Judges aren't the shortcut you think", "cluster": 13, "x": 6.733885288238525, "y": 3.219521999359131}, {"id": 45801396, "title": "Efficient LLMs: how active is this research area today?", "cluster": 13, "x": 6.845592021942139, "y": 3.2713520526885986}, {"id": 45801070, "title": "LLM As A Judge is not the shortcut you think", "cluster": 13, "x": 6.663733959197998, "y": 3.196776866912842}, {"id": 45801020, "title": "Small vs. Large Language Models", "cluster": 208, "x": 8.162166595458984, "y": 4.351693153381348}, {"id": 45799615, "title": "ImpossibleBench: Measuring Reward Hacking in LLM Coding Agents", "cluster": 13, "x": 6.425149917602539, "y": 3.5095579624176025}, {"id": 45799586, "title": "Good abstractions for humans turn out to be good abstractions for LLMs", "cluster": 13, "x": 6.762034893035889, "y": 3.166907787322998}, {"id": 45799223, "title": "Helios-Engine, I Built Another LLM Framework (and It Doesn't Suck)", "cluster": 13, "x": 6.816847324371338, "y": 3.6892526149749756}, {"id": 45795903, "title": "Machine Scheduler in LLVM \u2013 Part II", "cluster": 13, "x": 6.998285293579102, "y": 3.6980652809143066}, {"id": 45795821, "title": "Imarena Protocol: A Cryptographically-Auditable Failsafe for LLM Honesty", "cluster": 13, "x": 6.505068778991699, "y": 3.434385061264038}, {"id": 45795505, "title": "The Case Against LLMs as Rerankers", "cluster": 13, "x": 6.549727916717529, "y": 3.0117290019989014}, {"id": 45795603, "title": "How fast can an LLM go?", "cluster": 13, "x": 6.745903015136719, "y": 3.2005085945129395}, {"id": 45795439, "title": "NetLogo \u2013 Environment for agent-based modeling", "cluster": 37, "x": 8.545501708984375, "y": 3.7906570434570312}, {"id": 45795384, "title": "Thinking About Thinking with LLMs", "cluster": 13, "x": 6.615988254547119, "y": 3.0792417526245117}, {"id": 45794534, "title": "LLM Assisted-By Footer", "cluster": 13, "x": 6.754523277282715, "y": 3.308454751968384}, {"id": 45794500, "title": "Stable LLM Inference", "cluster": 13, "x": 6.948253154754639, "y": 3.338124990463257}, {"id": 45792823, "title": "Using coding LLM agents to hack Catan's browser game", "cluster": 13, "x": 6.467443943023682, "y": 3.47537899017334}, {"id": 45792449, "title": "A/B Testing Could Lead LLMs to Retain Users Instead of Helping Them", "cluster": 13, "x": 6.591383457183838, "y": 3.165426731109619}, {"id": 45791557, "title": "Defining and evaluating political bias in LLMs", "cluster": 13, "x": 6.652431964874268, "y": 3.0328941345214844}, {"id": 45791050, "title": "GraphQL Data Mocking at Scale with LLMs and GenerateMock", "cluster": 13, "x": 7.049906253814697, "y": 3.482440948486328}, {"id": 45790930, "title": "The Irony of the LLM Treadmill", "cluster": 13, "x": 6.570538520812988, "y": 3.0733397006988525}, {"id": 45789856, "title": "The Cargo Cult in the Machine: Why LLMs Are the Ultimate Imitators", "cluster": 13, "x": 6.6969170570373535, "y": 3.076856851577759}, {"id": 45789952, "title": "GraphMD \u2013 LLMs Powered Literate Programming Environment for Markdown Documents", "cluster": 13, "x": 6.945730686187744, "y": 3.6661787033081055}, {"id": 45789803, "title": "Thinking About Thinking with LLMs", "cluster": 13, "x": 6.621795654296875, "y": 3.064466714859009}, {"id": 45788167, "title": "Machine Scheduler in LLVM \u2013 Part II", "cluster": 13, "x": 7.025643348693848, "y": 3.716708183288574}, {"id": 45786042, "title": "Can you save on LLM tokens using images instead of text?", "cluster": 13, "x": 6.76184606552124, "y": 3.5519003868103027}, {"id": 45784759, "title": "The API for AI SEO and LLM Visibility Data", "cluster": 12, "x": 7.029223442077637, "y": 2.720366954803467}, {"id": 45784565, "title": "LLMs are the missing link to shipping side projects", "cluster": 13, "x": 6.6460347175598145, "y": 3.3195061683654785}, {"id": 45783363, "title": "Superrational Reasoning in the Prisoner's Dilemma with LLMs", "cluster": 13, "x": 6.750283241271973, "y": 3.150040864944458}, {"id": 45782965, "title": "If the LLM Is Stuck, Ask It to Write a Diagnosis Script", "cluster": 13, "x": 6.695293426513672, "y": 3.4290575981140137}, {"id": 45781724, "title": "LLM consciousness claims are systematic, mechanistically gated, and convergent", "cluster": 13, "x": 6.7268805503845215, "y": 3.008455276489258}, {"id": 45781392, "title": "The Smol Training Playbook: The Secrets to Building World-Class LLMs", "cluster": 13, "x": 6.579256534576416, "y": 3.1330454349517822}, {"id": 45779980, "title": "ImpossibleBench: Measuring Reward Hacking in LLM Coding Agents", "cluster": 13, "x": 6.447265148162842, "y": 3.5000858306884766}, {"id": 45778974, "title": "LLMs Report Subjective Experience Under Self-Referential Processing", "cluster": 13, "x": 6.614346504211426, "y": 3.021512269973755}, {"id": 45778401, "title": "Building Machine Learning Systems with a Feature Store: Batch, Real-Time and LLM", "cluster": 13, "x": 6.9430036544799805, "y": 3.5847978591918945}, {"id": 45778126, "title": "Large Language Models Get All the Hype, but Small Models Do the Real Work", "cluster": 208, "x": 8.175715446472168, "y": 4.391520023345947}, {"id": 45776158, "title": "The Smol Training Playbook: The Secrets to Building World-Class LLMs", "cluster": 13, "x": 6.594625949859619, "y": 3.1572790145874023}, {"id": 45775974, "title": "How to design effective agent workflows?", "cluster": 38, "x": 8.637011528015137, "y": 3.6619033813476562}, {"id": 45775329, "title": "Context-Bench: Benchmarking LLMs on Agentic Context Engineering", "cluster": 13, "x": 7.011547088623047, "y": 3.19740891456604}, {"id": 45772304, "title": "LLMs: Screenwriters vs. Characters", "cluster": 13, "x": 6.6048665046691895, "y": 3.427023410797119}, {"id": 45772418, "title": "Investigating How Prompt Politeness Affects LLM Accuracy", "cluster": 13, "x": 6.713725566864014, "y": 3.1671805381774902}, {"id": 45770696, "title": "The Smol Training Playbook: The Secrets to Building World-Class LLMs", "cluster": 13, "x": 6.585408687591553, "y": 3.131469249725342}, {"id": 45768455, "title": "Emergent introspective awareness in large language models", "cluster": 208, "x": 8.49327564239502, "y": 4.180255889892578}, {"id": 45768102, "title": "Leadership Co-Processing with LLMs", "cluster": 13, "x": 6.638740539550781, "y": 3.2065091133117676}, {"id": 45768217, "title": "One Memory Layer, Multiple Models (Claude, GPT, Llama, etc.)", "cluster": 13, "x": 7.306686878204346, "y": 3.7836947441101074}, {"id": 45767757, "title": "The Smol Training Playbook: The Secrets to Building World-Class LLMs", "cluster": 13, "x": 6.594450950622559, "y": 3.1581385135650635}, {"id": 45766637, "title": "Apple Plans to Open-Source an LLVM Tool to Security Harden Large C++ Codebases", "cluster": 13, "x": 6.654737949371338, "y": 3.690380096435547}, {"id": 45766361, "title": "SWE-1.5: Our Fast Agent Model", "cluster": 38, "x": 8.611001968383789, "y": 3.7282421588897705}, {"id": 45764009, "title": "Llmtext \u2013 an open source toolkit for llms.txt adoption", "cluster": 13, "x": 6.904045581817627, "y": 3.7735798358917236}, {"id": 45763960, "title": "Cursor Cloud Agents", "cluster": 37, "x": 8.496811866760254, "y": 3.7563483715057373}, {"id": 45763480, "title": "OpenArena: LLMs Battling in Autonomous Sports Betting Markets", "cluster": 13, "x": 6.767549991607666, "y": 3.1004581451416016}, {"id": 45763142, "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "cluster": 13, "x": 6.872981071472168, "y": 3.180417060852051}, {"id": 45762612, "title": "GraphQL Data Mocking at Scale with LLMs and @generateMock", "cluster": 13, "x": 7.03031587600708, "y": 3.5193734169006348}, {"id": 45762160, "title": "The Smol Training Playbook: The Secrets to Building World-Class LLMs", "cluster": 13, "x": 6.570164680480957, "y": 3.122561454772949}, {"id": 45762065, "title": "I made Cluely, but like, local LLM support, baby", "cluster": 13, "x": 6.533950328826904, "y": 3.2639987468719482}, {"id": 45762064, "title": "Signs of introspection in large language models", "cluster": 208, "x": 8.42811107635498, "y": 4.211455345153809}, {"id": 45762038, "title": "LLM Model Cost Map", "cluster": 13, "x": 6.959841251373291, "y": 3.2897913455963135}, {"id": 45761231, "title": "I built an autonomous agent to find and fix security vulnerabilities in LLM apps", "cluster": 13, "x": 6.625649452209473, "y": 3.5214004516601562}, {"id": 45759385, "title": "The Irony of the LLM Treadmill", "cluster": 13, "x": 6.564675331115723, "y": 3.0735721588134766}, {"id": 45758285, "title": "How fast can an LLM go?", "cluster": 13, "x": 6.7182207107543945, "y": 3.1864547729492188}, {"id": 45758093, "title": "Language models are injective and hence invertible", "cluster": 208, "x": 8.364092826843262, "y": 4.537703514099121}, {"id": 45757055, "title": "LLM AuthZ Handbook: A Practical Guide for AI Builders and Users", "cluster": 12, "x": 7.030542373657227, "y": 2.679471254348755}, {"id": 45754330, "title": "Language Models Are Injective and Hence Invertible", "cluster": 208, "x": 8.35277271270752, "y": 4.539126396179199}, {"id": 45753850, "title": "Llamafile Returns", "cluster": 13, "x": 7.047670364379883, "y": 3.8501570224761963}, {"id": 45753422, "title": "Responses from LLMs are not facts", "cluster": 13, "x": 6.617868900299072, "y": 3.0720643997192383}, {"id": 45752911, "title": "RustyFlow: LLM built on pure Rust language", "cluster": 13, "x": 6.9495015144348145, "y": 4.012144565582275}, {"id": 45753028, "title": "Writing an LLM from scratch, part 25 \u2013 instruction fine-tuning", "cluster": 13, "x": 6.75499963760376, "y": 3.223191261291504}, {"id": 45752428, "title": "Emergent Introspective Awareness in Large Language Models", "cluster": 208, "x": 8.454233169555664, "y": 4.2179388999938965}, {"id": 45751871, "title": "Fusing Geoscience LLMs and Lightweight RAG for Geological Question Answering", "cluster": 13, "x": 6.945018768310547, "y": 3.409229278564453}, {"id": 45751486, "title": "LangSmith's No Code Agent Builder", "cluster": 34, "x": 7.671401023864746, "y": 4.5940775871276855}, {"id": 45750678, "title": "Language Models Are Injective and Hence Invertible", "cluster": 208, "x": 8.362750053405762, "y": 4.547393798828125}, {"id": 45750542, "title": "Experimental config driven LLM client library in Rust", "cluster": 13, "x": 6.972494125366211, "y": 3.977311372756958}, {"id": 45750329, "title": "Signs of introspection in large language models", "cluster": 208, "x": 8.44463062286377, "y": 4.239140510559082}, {"id": 45749341, "title": "Help, My Boss Started Programming with LLMs", "cluster": 13, "x": 6.6416802406311035, "y": 3.6105496883392334}, {"id": 45748270, "title": "LLMs Are Moving Local \u2013 So Why Are We Still Paying for Tokens?", "cluster": 13, "x": 6.671753406524658, "y": 3.1667351722717285}, {"id": 45748195, "title": "The end of the rip-off economy: consumers use LLMs against information asymmetry", "cluster": 13, "x": 6.689074516296387, "y": 3.068021774291992}, {"id": 45747049, "title": "The Irony of the LLM Treadmill", "cluster": 13, "x": 6.570350170135498, "y": 3.075737237930298}, {"id": 45746533, "title": "Evaluating Political Bias in LLMs", "cluster": 13, "x": 6.6191558837890625, "y": 3.007582664489746}, {"id": 45746473, "title": "Apple Plans to Open-Source an LLVM Tool to Security Harden Large C++ Codebases", "cluster": 13, "x": 6.675570964813232, "y": 3.6673216819763184}, {"id": 45746155, "title": "Do LLMs know when they've gotten a correct answer?", "cluster": 13, "x": 6.664826393127441, "y": 3.1075515747070312}, {"id": 45744969, "title": "Literary character approach helps LLMs simulate more human-like personalities", "cluster": 13, "x": 6.665700435638428, "y": 3.0625522136688232}, {"id": 45744565, "title": "Language Models Are Injective and Hence Invertible", "cluster": 208, "x": 8.362459182739258, "y": 4.5364990234375}, {"id": 45744259, "title": "Australian Federal Police to develop LLM for decoding GenZ slang", "cluster": 13, "x": 6.793098449707031, "y": 3.3389594554901123}, {"id": 45743724, "title": "Holographic theory of LLMs: explains unbreakable bias and cross-model infection", "cluster": 13, "x": 6.609869956970215, "y": 2.974992275238037}, {"id": 45742153, "title": "Llm.exe -R -v low -e minimal, how I code with language models", "cluster": 13, "x": 6.9848127365112305, "y": 3.909393072128296}, {"id": 45741681, "title": "I think LLMs can do multiplication?", "cluster": 13, "x": 6.755756378173828, "y": 3.1899704933166504}, {"id": 45741218, "title": "Semantic Compression with Large Language Models", "cluster": 208, "x": 8.179665565490723, "y": 4.420734882354736}, {"id": 45740520, "title": "API for LLM-enabled semantic data matching", "cluster": 13, "x": 7.040480613708496, "y": 3.593571662902832}, {"id": 45738533, "title": "Writing an LLM from scratch, part 24 \u2013 the transcript hack", "cluster": 13, "x": 6.691892623901367, "y": 3.2719662189483643}, {"id": 45737540, "title": "Barriers to the adoption of single-cell LLMs in biomedical research", "cluster": 13, "x": 6.689806938171387, "y": 3.1893417835235596}, {"id": 45737196, "title": "Lumi: Using LLMs to annotate ArXiv papers with summaries, refs, and inline Q&A", "cluster": 13, "x": 6.883555889129639, "y": 3.6214239597320557}, {"id": 45736843, "title": "One ruler to measure them all: Benchmarking multilingual long-context LLMs", "cluster": 13, "x": 6.949968338012695, "y": 3.524627685546875}, {"id": 45736707, "title": "Burn 0.19.0 Release: Quantization, Distributed Training, and LLVM Back End", "cluster": 13, "x": 7.044178009033203, "y": 3.692798614501953}, {"id": 45736663, "title": "Modern LLM Training (A Summary)", "cluster": 13, "x": 6.791884422302246, "y": 3.3214120864868164}, {"id": 45734720, "title": "LLMs are shockingly bad at poker", "cluster": 13, "x": 6.575080394744873, "y": 3.0887985229492188}, {"id": 45734179, "title": "AI-Trader: Compares different LLM models trading in the market", "cluster": 12, "x": 6.951387882232666, "y": 2.7178876399993896}, {"id": 45733803, "title": "Building an Open ABI and FFI for ML Systems", "cluster": 13, "x": 6.956187725067139, "y": 3.568020820617676}, {"id": 45733169, "title": "Our LLM-controlled office robot can't pass butter", "cluster": 12, "x": 6.926538467407227, "y": 2.8259899616241455}, {"id": 45732175, "title": "How do LLMs \"think\" across languages", "cluster": 13, "x": 6.732412338256836, "y": 3.166330099105835}, {"id": 45731756, "title": "Help, My Boss Started Programming with LLMs", "cluster": 13, "x": 6.651134967803955, "y": 3.6294593811035156}, {"id": 45731724, "title": "LLMs can hide text in other text of the same length", "cluster": 13, "x": 6.657918930053711, "y": 3.462812662124634}, {"id": 45731212, "title": "LLMs: The Illusion of Thinking", "cluster": 13, "x": 6.631009578704834, "y": 3.023203134536743}, {"id": 45731075, "title": "Beyond the Magic: How LLMs Work", "cluster": 13, "x": 6.745110511779785, "y": 3.2036898136138916}, {"id": 45730248, "title": "Survey: 98% Adopting LLMs into Apps, While 24% Still Onboard Security Tools", "cluster": 13, "x": 6.7094340324401855, "y": 3.548147678375244}, {"id": 45730094, "title": "Poker Tournament for LLMs", "cluster": 13, "x": 6.729677200317383, "y": 3.152472972869873}, {"id": 45729947, "title": "The Case Against LLMs as Rerankers", "cluster": 13, "x": 6.589365482330322, "y": 3.0530037879943848}, {"id": 45726542, "title": "Complaints About LLM-Based Coding Tools Are Valid. So What?", "cluster": 13, "x": 6.5567779541015625, "y": 3.540454149246216}, {"id": 45724884, "title": "The ORM for LLM", "cluster": 13, "x": 6.752228736877441, "y": 3.33746600151062}, {"id": 45724675, "title": "DeepSeek OCR with Vllm \u2013 10x cheaper on cloud GPU", "cluster": 13, "x": 7.193197250366211, "y": 3.8128676414489746}, {"id": 45724378, "title": "Language Models Are Injective and Hence Invertible", "cluster": 208, "x": 8.364542007446289, "y": 4.531599044799805}, {"id": 45723312, "title": "Grounding LLMs with Symbolic Planning", "cluster": 13, "x": 6.761592388153076, "y": 3.3047094345092773}, {"id": 45722245, "title": "LLMs Forget. Redprint Remembers", "cluster": 13, "x": 6.745794773101807, "y": 3.4430062770843506}, {"id": 45722208, "title": "Stanislaw Lem's Imaginary Magnitude", "cluster": 13, "x": 6.8297343254089355, "y": 3.0677473545074463}, {"id": 45721936, "title": "PPO for LLMs: A Guide for Normal People", "cluster": 13, "x": 6.73491907119751, "y": 3.3098230361938477}, {"id": 45721467, "title": "Words that make language models perceive", "cluster": 208, "x": 8.360222816467285, "y": 4.3573737144470215}, {"id": 45720478, "title": "Five LLM Tricks for Data Pipelines", "cluster": 13, "x": 7.110598564147949, "y": 3.521928071975708}, {"id": 45720435, "title": "Modeling and Solving Operations Research Problems with Tool Augmented LLMs", "cluster": 13, "x": 6.766900062561035, "y": 3.587031364440918}, {"id": 45720106, "title": "AlphaZero and LLMs in financial markets as the \"final boss\" to train on", "cluster": 13, "x": 6.7888264656066895, "y": 3.1149964332580566}, {"id": 45719985, "title": "Writing an LLM from scratch, part 23 \u2013 fine-tuning for classification", "cluster": 13, "x": 6.790487766265869, "y": 3.25402569770813}, {"id": 45719702, "title": "Multi-Agent Orchestration with the Microsoft Agent Framework", "cluster": 37, "x": 8.538780212402344, "y": 3.717541217803955}, {"id": 45718983, "title": "Fixing rust-lang stdarch issues in LLVM", "cluster": 13, "x": 6.999494552612305, "y": 4.000749588012695}, {"id": 45718789, "title": "Token-Oriented Object Notation \u2013 JSON for LLMs at Half the Token Cost", "cluster": 13, "x": 6.947750091552734, "y": 3.5769057273864746}, {"id": 45717920, "title": "LaTeX, LLMs and Boring Technology", "cluster": 13, "x": 6.348329067230225, "y": 3.5991251468658447}, {"id": 45715798, "title": "LLMs Are Bottlenecked by Linear Interfaces", "cluster": 13, "x": 6.778429985046387, "y": 3.2602500915527344}, {"id": 45713678, "title": "Stress-testing model specs reveals character differences among language models", "cluster": 208, "x": 8.264392852783203, "y": 4.465576171875}, {"id": 45713265, "title": "Simple Control Flow for Automatically Steering Agents", "cluster": 38, "x": 8.553948402404785, "y": 3.686535596847534}, {"id": 45712906, "title": "You Don't Need an Agentic Framework to Start Building Agents", "cluster": 38, "x": 8.675050735473633, "y": 3.4292140007019043}, {"id": 45712783, "title": "Two Ideas for Humans Learning from LLMs", "cluster": 13, "x": 6.700472354888916, "y": 3.1172165870666504}, {"id": 45712132, "title": "Language agents for optimal conversation stopping", "cluster": 70, "x": 8.633100509643555, "y": 4.315635681152344}, {"id": 45711786, "title": "The FSF considers large language models", "cluster": 208, "x": 8.156861305236816, "y": 4.3955488204956055}, {"id": 45709310, "title": "Hey LLM, write production-ready code", "cluster": 13, "x": 6.676240921020508, "y": 3.703906297683716}, {"id": 45709179, "title": "Language Modeling with Hierarchical Reasoning Models: Lessons from 1M Parameters", "cluster": 208, "x": 8.373417854309082, "y": 4.3302693367004395}, {"id": 45708121, "title": "The LL game: the curious preference for low quality and its norms [pdf]", "cluster": 13, "x": 6.8818840980529785, "y": 3.162891149520874}, {"id": 45706866, "title": "An Update on TinyKVM", "cluster": 13, "x": 7.211528778076172, "y": 4.038327217102051}, {"id": 45705974, "title": "Are you the asshole? Of course not \u2013quantifying LLMs' sycophancy problem", "cluster": 13, "x": 6.691808223724365, "y": 3.0771584510803223}, {"id": 45705227, "title": "LaTeX, LLMs and Boring Technology", "cluster": 13, "x": 6.348823070526123, "y": 3.599038600921631}, {"id": 45704395, "title": "Rickrolled by LLM", "cluster": 13, "x": 6.539445877075195, "y": 3.1824183464050293}, {"id": 45703772, "title": "LLM-empowered knowledge graph construction: A survey", "cluster": 13, "x": 6.909679412841797, "y": 3.4205098152160645}, {"id": 45702515, "title": "Parsing webpages with a LLM, revisited", "cluster": 13, "x": 6.801413536071777, "y": 3.6377437114715576}, {"id": 45701059, "title": "LMQL Is a Programming Language for LLMs", "cluster": 13, "x": 6.720109939575195, "y": 3.5701444149017334}, {"id": 45700844, "title": "LLMs Are Weird, Man", "cluster": 13, "x": 6.51239013671875, "y": 3.0856199264526367}, {"id": 45700661, "title": "LLMs Often Know When They're Being Evaluated", "cluster": 13, "x": 6.736943244934082, "y": 3.144343614578247}, {"id": 45700308, "title": "Text or pixels? On the token efficiency of visual text inputs in multimodal LLMs", "cluster": 13, "x": 6.9466423988342285, "y": 3.4670302867889404}, {"id": 45699722, "title": "Language Models Are Injective and Hence Invertible", "cluster": 208, "x": 8.34890079498291, "y": 4.526093482971191}, {"id": 45699411, "title": "LLMs extract high-level semantic concepts from SVG and ASCII art", "cluster": 13, "x": 6.884830474853516, "y": 3.5530319213867188}, {"id": 45699341, "title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs", "cluster": 13, "x": 7.019881248474121, "y": 3.2840254306793213}, {"id": 45698710, "title": "LLM-Driven Adaptive Prompt Optimization Framework for ADS-B Anomaly Detection", "cluster": 13, "x": 6.888288497924805, "y": 3.47548508644104}, {"id": 45697481, "title": "LLM Exchange Rates Updated", "cluster": 13, "x": 6.896633148193359, "y": 3.4142162799835205}, {"id": 45696596, "title": "LLM Provider Variance: Introducing Exacto", "cluster": 13, "x": 6.894925117492676, "y": 3.3189196586608887}, {"id": 45693874, "title": "Marketers Must Treat LLMs as Distinct Discovery Channels", "cluster": 13, "x": 6.604875564575195, "y": 3.1191294193267822}, {"id": 45693827, "title": "Skillz: Use Claude skills with any agent (Codex, Gemini, Copilot, etc...)", "cluster": 34, "x": 7.841538906097412, "y": 4.549627780914307}, {"id": 45693591, "title": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference", "cluster": 13, "x": 7.00988245010376, "y": 3.5205647945404053}, {"id": 45691306, "title": "Schneier on LLM vulnerabilities, agentic AI, and \"trusting trust\"", "cluster": 12, "x": 6.950897216796875, "y": 2.6479787826538086}, {"id": 45690219, "title": "Fast-DLLM: Training-Free Acceleration of Diffusion LLM", "cluster": 13, "x": 7.065422534942627, "y": 3.603463649749756}, {"id": 45689944, "title": "Sampling in Large Language Models", "cluster": 208, "x": 8.170204162597656, "y": 4.374433517456055}, {"id": 45688969, "title": "The Case Against LLMs as Rerankers", "cluster": 13, "x": 6.557101249694824, "y": 3.0723073482513428}, {"id": 45688750, "title": "PlainErrors: Streamlined Rails Error Pages for LLM Agents", "cluster": 13, "x": 6.824492931365967, "y": 3.672045946121216}, {"id": 45686768, "title": "Building an LSP for your docs by overcoming Vercel", "cluster": 13, "x": 6.747888088226318, "y": 3.492734670639038}, {"id": 45684590, "title": "Implicit Bias in Large Language Models with Concept Learning Dataset", "cluster": 208, "x": 8.346529006958008, "y": 4.351377487182617}, {"id": 45683970, "title": "Parse: LLM Driven Schema Optimization for Reliable Entity Extraction", "cluster": 13, "x": 6.960570335388184, "y": 3.571617841720581}, {"id": 45683020, "title": "Secure AI/ML-Driven Software Development (LFEL1012) \u2013 Free Online Course", "cluster": 3, "x": 7.024445056915283, "y": 2.6132662296295166}, {"id": 45682438, "title": "SSH LLM Honeypot caught a real threat actor", "cluster": 13, "x": 6.3687744140625, "y": 3.3911426067352295}, {"id": 45682105, "title": "Enhancing Transformer-Based Rerankers with Synthetic Data and LLM Supervision", "cluster": 13, "x": 6.95064640045166, "y": 3.2942006587982178}, {"id": 45681497, "title": "Humans and LLMs represent sentences similarly, study finds", "cluster": 13, "x": 6.760563850402832, "y": 3.0875282287597656}, {"id": 45681178, "title": "Yes, let's teach LLMs accessibility, but also the companies using them", "cluster": 13, "x": 6.643643379211426, "y": 3.423521041870117}, {"id": 45681074, "title": "Large language models show strong political bias: a simulated polling experiment", "cluster": 208, "x": 8.297541618347168, "y": 4.360123634338379}, {"id": 45680913, "title": "We tested 20 LLMs for ideological bias, revealing distinct alignments", "cluster": 13, "x": 6.716499328613281, "y": 3.0280568599700928}, {"id": 45680629, "title": "An LLM Is (Not Really) a Black Box Full of Sudoku and Tic Tac Toe Games", "cluster": 13, "x": 6.679008960723877, "y": 3.1796839237213135}, {"id": 45680008, "title": "The FSF considers large language models", "cluster": 208, "x": 8.14406681060791, "y": 4.387506484985352}, {"id": 45679197, "title": "OpenEstimate Evaluating LLMs on Reasoning Under Uncertainty with Real-World Data", "cluster": 13, "x": 6.909020900726318, "y": 3.222341775894165}, {"id": 45678448, "title": "Convention over LLM (CoL) \u2013 A novel approach to coding", "cluster": 13, "x": 6.680229187011719, "y": 3.660428047180176}, {"id": 45677236, "title": "Australian-made LLM beats OpenAI and Google at legal retrieval", "cluster": 13, "x": 6.854675769805908, "y": 3.446614980697632}, {"id": 45676273, "title": "Writing an LLM from scratch, part 23 \u2013 fine-tuning for classification", "cluster": 13, "x": 6.790096282958984, "y": 3.262495279312134}, {"id": 45675398, "title": "Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples", "cluster": 13, "x": 6.482578277587891, "y": 3.172481060028076}, {"id": 45674783, "title": "OpenBench: Provider-agnostic, open-source evaluation infrastructure for LLMs", "cluster": 13, "x": 6.883740425109863, "y": 3.6762640476226807}, {"id": 45674414, "title": "Smarter MCP Clients: A Leaner, Faster Approach to LLM Tooling", "cluster": 13, "x": 6.893911361694336, "y": 3.60547137260437}, {"id": 45672946, "title": "The Return of Assembly: When LLMs No Longer Need High-Level Languages", "cluster": 13, "x": 6.727726936340332, "y": 3.5869221687316895}, {"id": 45672934, "title": "Deep research benchmark shows how poor LLMs are at writing accurate reports", "cluster": 13, "x": 6.771965980529785, "y": 3.1661837100982666}, {"id": 45672405, "title": "How do LLM's trade off lives between different categories?", "cluster": 13, "x": 6.668821334838867, "y": 3.0586180686950684}, {"id": 45672128, "title": "Derive an OLAP Data Model from Your OLTP ORM (Drizzle, Prisma, TypeORM)", "cluster": 13, "x": 7.328277111053467, "y": 4.052957057952881}, {"id": 45671535, "title": "Can LLMs replace on call SREs today?", "cluster": 13, "x": 6.597220420837402, "y": 3.288998603820801}, {"id": 45671160, "title": "Why do LMMs overuse these patterns of speech that aren't overused in the wild?", "cluster": 13, "x": 6.597279071807861, "y": 3.0336294174194336}, {"id": 45670641, "title": "Feeding all religious and spiritual texts that I can find into an LLM", "cluster": 13, "x": 6.67215633392334, "y": 3.283090353012085}, {"id": 45670440, "title": "Open ABI and FFI for Machine Learning Systems", "cluster": 3, "x": 7.171762466430664, "y": 3.1308038234710693}, {"id": 45668561, "title": "Chezmoi introduces ban on LLM-generated contributions", "cluster": 13, "x": 6.632934093475342, "y": 3.239248752593994}, {"id": 45668264, "title": "The security paradox of local LLMs", "cluster": 13, "x": 6.471166610717773, "y": 3.279829740524292}, {"id": 45667428, "title": "Memelang \u2013 query language optimized for LLM RAG token efficiency", "cluster": 13, "x": 7.025920867919922, "y": 3.504621982574463}, {"id": 45666550, "title": "Attention Sinks in Diffusion Language Models", "cluster": 208, "x": 8.35308837890625, "y": 4.3021931648254395}, {"id": 45665984, "title": "Grasp Any Region: Precise, Contextual Pixel Understanding for Multimodal LLMs", "cluster": 13, "x": 7.081829071044922, "y": 3.6647703647613525}, {"id": 45665943, "title": "Usage and Evaluation of LLM in Media and Organizations [pdf]", "cluster": 13, "x": 6.838181018829346, "y": 3.3651742935180664}, {"id": 45665220, "title": "A deep dive into BPF LPM trie performance and optimization", "cluster": 13, "x": 7.216599464416504, "y": 3.674341917037964}, {"id": 45664160, "title": "CRM's?", "cluster": 13, "x": 6.725839614868164, "y": 3.4213883876800537}, {"id": 45663910, "title": "Building an Open ABI and FFI for ML Systems", "cluster": 13, "x": 6.970233917236328, "y": 3.5917017459869385}, {"id": 45663454, "title": "Surfacing LLM Biases Through Graffiti", "cluster": 13, "x": 6.681931972503662, "y": 3.1422231197357178}, {"id": 45663467, "title": "Large Language Models Inference Engines Based on Spiking Neural Networks", "cluster": 208, "x": 8.21426010131836, "y": 4.375892639160156}, {"id": 45662886, "title": "Knowledge Transfer from High-Resource to Low-Resource Languages for Code LLMs (2023)", "cluster": 11, "x": 7.063541412353516, "y": 3.8399055004119873}, {"id": 45662278, "title": "Andrej Karpathy said LLMs don't have \"culture\". So we gave them one", "cluster": 13, "x": 6.563640594482422, "y": 3.093384027481079}, {"id": 45662431, "title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training", "cluster": 13, "x": 7.111851215362549, "y": 3.6214942932128906}, {"id": 45661692, "title": "Useful bias manipulation re: LLM \u2013 the stochastic parrot speaks", "cluster": 13, "x": 6.6527605056762695, "y": 3.0582923889160156}, {"id": 45660942, "title": "Building the Language Model Nobody Asked For", "cluster": 208, "x": 8.190095901489258, "y": 4.5816240310668945}, {"id": 45658957, "title": "Coding with LLMs: We can talk to computers now and we're upset about it", "cluster": 13, "x": 6.533299922943115, "y": 3.469419240951538}, {"id": 45658928, "title": "Karpathy on DeepSeek-OCR paper: Are pixels better inputs to LLMs than text?", "cluster": 13, "x": 6.950409889221191, "y": 3.440809965133667}, {"id": 45658687, "title": "Kvcached: Virtualized, elastic KV cache for LLM serving on shared GPUs", "cluster": 13, "x": 7.163596153259277, "y": 3.752047538757324}, {"id": 45658410, "title": "A deep dive into BPF LPM trie performance and optimization", "cluster": 13, "x": 7.280309200286865, "y": 3.660820245742798}, {"id": 45658227, "title": "Bolt \u2013 How Mura Wrote an In-House LLM Eval Framework", "cluster": 13, "x": 6.805531978607178, "y": 3.3230278491973877}, {"id": 45657830, "title": "Should LLMs just treat text content as an image?", "cluster": 13, "x": 6.731918811798096, "y": 3.46895170211792}, {"id": 45657453, "title": "Spin LLM questions, get better answers", "cluster": 13, "x": 6.722878456115723, "y": 3.2013957500457764}, {"id": 45656840, "title": "Local LLMs are worse for security", "cluster": 13, "x": 6.440087795257568, "y": 3.362793207168579}, {"id": 45656223, "title": "LLMs can get \"brain rot\"", "cluster": 13, "x": 6.550137996673584, "y": 3.0706353187561035}, {"id": 45655161, "title": "Neural audio codecs: how to get audio into LLMs", "cluster": 13, "x": 6.840917110443115, "y": 3.5461533069610596}, {"id": 45654827, "title": "Measuring enterprise LLM adoption by observing the DNS records for 76k companies", "cluster": 13, "x": 6.881321430206299, "y": 3.4894094467163086}, {"id": 45654449, "title": "LLM Hub: Multi-Model AI Orchestration", "cluster": 12, "x": 7.0724310874938965, "y": 2.6125426292419434}, {"id": 45652952, "title": "Should LLMs just treat text content as an image?", "cluster": 13, "x": 6.7382402420043945, "y": 3.4110240936279297}, {"id": 45651204, "title": "Why Large Language Models Won't Replace Engineers Anytime Soon", "cluster": 208, "x": 8.176044464111328, "y": 4.419426441192627}, {"id": 45650628, "title": "Karpathy on image only input to LLMs", "cluster": 13, "x": 6.803963661193848, "y": 3.4986684322357178}, {"id": 45649662, "title": "Aegaeon: Effective GPU Pooling for Concurrent LLM Serving on the Market", "cluster": 13, "x": 7.137852191925049, "y": 3.6343157291412354}, {"id": 45649295, "title": "Skillz: Use Claude Skills in Codex, Copilot, or Any Other Agent via MCP", "cluster": 34, "x": 7.823100566864014, "y": 4.525218963623047}, {"id": 45648121, "title": "DocMind, Streamlit Application Leveraging LlamaIndex, LangGraph, and LLM", "cluster": 13, "x": 6.915175437927246, "y": 3.6965739727020264}, {"id": 45647029, "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning", "cluster": 208, "x": 8.45897102355957, "y": 4.3198699951171875}, {"id": 45645890, "title": "Optimizing LLM Context for Vulnerability Scanning", "cluster": 13, "x": 6.531505584716797, "y": 3.4189720153808594}, {"id": 45645536, "title": "Xyne: Open-source LLM-driven search engine for Google Workspace", "cluster": 13, "x": 6.897905349731445, "y": 3.7048096656799316}, {"id": 45645394, "title": "Lindenmayer Systems", "cluster": 13, "x": 6.860585689544678, "y": 3.586949348449707}, {"id": 45644085, "title": "LLM hallucinations are compression failures, and we can detect them", "cluster": 13, "x": 6.586582183837891, "y": 2.9855644702911377}, {"id": 45644048, "title": "Learning by Doing in the Age of LLMs", "cluster": 13, "x": 6.596164703369141, "y": 3.0618481636047363}, {"id": 45642444, "title": "LLMs Can Get \"Brain Rot\"", "cluster": 13, "x": 6.525542259216309, "y": 3.0529561042785645}, {"id": 45641301, "title": "Using LLMs to classify unstructured listings", "cluster": 13, "x": 6.892314910888672, "y": 3.564143419265747}, {"id": 45638739, "title": "Replua.nvim \u2013 an Emacs-style scratch buffer for executing Lua", "cluster": 13, "x": 6.938618183135986, "y": 3.8568503856658936}, {"id": 45637347, "title": "Why We Need Arabic Language Models", "cluster": 208, "x": 8.317240715026855, "y": 4.47813081741333}, {"id": 45636750, "title": "Tiny Recursive Model (TRM) vs. Hierarchical Reasoning Model (HRM)", "cluster": 13, "x": 6.933667182922363, "y": 3.2141737937927246}, {"id": 45636507, "title": "How do LLM's trade off lives between different categories?", "cluster": 13, "x": 6.6639251708984375, "y": 3.0434281826019287}, {"id": 45636111, "title": "Reverse Engineering and Tracing internal thoughts of LLM", "cluster": 13, "x": 6.6044416427612305, "y": 3.1201751232147217}, {"id": 45635248, "title": "Repo to AI: Bash one-liner to concatenate directory contents for LLM", "cluster": 13, "x": 6.904049873352051, "y": 3.780717372894287}, {"id": 45634002, "title": "Why Every ML Engineer Eventually Has to Learn Linear Algebra Properly", "cluster": 13, "x": 6.648293495178223, "y": 3.3660402297973633}, {"id": 45633062, "title": "Llm7.io just got a fresh look", "cluster": 13, "x": 6.965370178222656, "y": 3.825147867202759}, {"id": 45631867, "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "cluster": 13, "x": 6.905989646911621, "y": 3.216608762741089}, {"id": 45631852, "title": "How I bootstrapped a platform with a team of LLMs", "cluster": 13, "x": 6.747866153717041, "y": 3.5720648765563965}, {"id": 45630438, "title": "The Design Space of LLM-Based AI Coding Assistants [pdf]", "cluster": 12, "x": 7.05028772354126, "y": 2.6159188747406006}, {"id": 45629593, "title": "LLM Prompts", "cluster": 13, "x": 6.761120319366455, "y": 3.377939224243164}, {"id": 45629320, "title": "Agentic commerce: payments, trust rails, and where moats move", "cluster": 36, "x": 8.597465515136719, "y": 3.554232358932495}, {"id": 45627171, "title": "AGI is not imminent, and LLMs are not the royal road to getting there", "cluster": 13, "x": 6.5537567138671875, "y": 3.0996251106262207}, {"id": 45626961, "title": "Lux: A luxurious package manager for Lua", "cluster": 13, "x": 6.85207462310791, "y": 3.814673900604248}, {"id": 45626638, "title": "Coding with LLMs: we can talk to computers and nobody is happy", "cluster": 13, "x": 6.524494171142578, "y": 3.474980354309082}, {"id": 45626197, "title": "Agents that talk to each other: Deep MCP Agent adds Cross-Agent Communication", "cluster": 70, "x": 8.583502769470215, "y": 3.8845436573028564}, {"id": 45625471, "title": "How to NPM and avoid getting rekt", "cluster": 13, "x": 6.9470367431640625, "y": 3.6563920974731445}, {"id": 45624355, "title": "Read this but think of Mac GPU universal memory running LLM", "cluster": 13, "x": 6.902698993682861, "y": 3.726278781890869}, {"id": 45619257, "title": "Everything is Amazing and Nobody is Happy (about coding with LLMs)", "cluster": 13, "x": 6.525736331939697, "y": 3.5203309059143066}, {"id": 45618383, "title": "LLMs Achieve Gold Medal Performance at the IOAA", "cluster": 13, "x": 6.687425136566162, "y": 3.184997797012329}, {"id": 45617237, "title": "Introduction to LLM Red Teaming \u2013 DeepTeam \u2013 Open-Source LLM Red Team Framework", "cluster": 13, "x": 6.882803916931152, "y": 3.5762879848480225}, {"id": 45617007, "title": "General purpose agentic loop in 40 lines of Python", "cluster": 38, "x": 8.471988677978516, "y": 3.9167332649230957}, {"id": 45616611, "title": "Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs", "cluster": 13, "x": 6.919435024261475, "y": 3.4059019088745117}, {"id": 45616210, "title": "llms.py \u2013 Lightweight ChatGPT-Like API, UI and CLI", "cluster": 13, "x": 6.842813014984131, "y": 3.8580307960510254}, {"id": 45616145, "title": "Extra Compute Can Backfire in Language Models", "cluster": 208, "x": 8.207405090332031, "y": 4.411603927612305}, {"id": 45615629, "title": "Authors' Reply: Citation Accuracy Challenges Posed by Large Language Models", "cluster": 208, "x": 8.290210723876953, "y": 4.463983058929443}, {"id": 45615525, "title": "Citations and Trust in LLM Generated Responses", "cluster": 13, "x": 6.818288803100586, "y": 3.275902271270752}, {"id": 45615080, "title": "Building an LLM agent on your GDrive data with Vertex AI", "cluster": 12, "x": 7.027587413787842, "y": 2.7711410522460938}, {"id": 45614610, "title": "Fine-Tuning LLMs with Nvidia DGX Spark and Unsloth", "cluster": 13, "x": 7.0163493156433105, "y": 3.771455764770508}, {"id": 45614212, "title": "The Art of Scaling Reinforcement Learning Compute for LLMs [Meta]", "cluster": 13, "x": 7.070816516876221, "y": 3.247990131378174}, {"id": 45613368, "title": "Open source 3B param model for documents better than Gemini 2.5", "cluster": 13, "x": 6.946019649505615, "y": 3.84175705909729}, {"id": 45613426, "title": "LLM Pricing Calculator", "cluster": 13, "x": 6.931746482849121, "y": 3.3032021522521973}, {"id": 45612572, "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting", "cluster": 13, "x": 6.9994282722473145, "y": 3.2542924880981445}, {"id": 45611912, "title": "Nvidia DGX Spark and Apple Mac Studio = 4x Faster LLM Inference with EXO 1.0", "cluster": 13, "x": 7.182516098022461, "y": 3.7479472160339355}, {"id": 45612023, "title": "Let's Build the GPT Tokenizer: A Complete Guide to Tokenization in LLMs", "cluster": 13, "x": 6.934787750244141, "y": 3.478189468383789}, {"id": 45610443, "title": "Viral GPT wrappers are now training their own LLMs", "cluster": 13, "x": 6.831064701080322, "y": 3.4068381786346436}, {"id": 45609365, "title": "The Art of Scaling Reinforcement Learning Compute for LLMs", "cluster": 13, "x": 7.052321910858154, "y": 3.2944095134735107}, {"id": 45607871, "title": "A tool that audits your site and boosts your ranking in ChatGPT/ LLMs", "cluster": 13, "x": 6.703293800354004, "y": 3.633301258087158}, {"id": 45606767, "title": "Insights a 25 Year Old Movie Can Give Us on LLMs", "cluster": 13, "x": 6.573609352111816, "y": 3.035839557647705}, {"id": 45606535, "title": "How to Defame Any Brand with a Typo in the LLM World", "cluster": 13, "x": 6.59419059753418, "y": 3.1050984859466553}, {"id": 45605954, "title": "Which Nested Data Format Do LLMs Understand Best? JSON vs. YAML vs. XML vs. MD", "cluster": 13, "x": 6.956469535827637, "y": 3.493356943130493}, {"id": 45605259, "title": "An Approach to Systematically Testing LLM Data Retrieval", "cluster": 13, "x": 6.8708062171936035, "y": 3.445591449737549}, {"id": 45605153, "title": "Lace: A New Kind of Cellular Automata Where Links Matter", "cluster": 208, "x": 8.37055492401123, "y": 4.560655117034912}, {"id": 45604874, "title": "General purpose agentic loop in 40 lines of Python", "cluster": 38, "x": 8.465818405151367, "y": 3.9174728393554688}, {"id": 45604930, "title": "llms.py \u2013 Lightweight ChatGPT-Like API, UI and CLI", "cluster": 13, "x": 6.842893123626709, "y": 3.8727869987487793}, {"id": 45604217, "title": "LLMs struggle with math reasoning, because they can't conjecture", "cluster": 13, "x": 6.772293567657471, "y": 3.148026466369629}, {"id": 45603467, "title": "Clustering Nvidia DGX Spark and M3 Ultra Mac Studio for 4x Faster LLM Inference", "cluster": 13, "x": 7.1781535148620605, "y": 3.7097833156585693}, {"id": 45603320, "title": "Clustering Nvidia DGX Spark and M3 Ultra Mac Studio for 4x Faster LLM Inference", "cluster": 13, "x": 7.160412311553955, "y": 3.742875814437866}, {"id": 45603183, "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity of Likert Ratings", "cluster": 13, "x": 6.849874496459961, "y": 3.1906845569610596}, {"id": 45602844, "title": "Introducing the Massive Legal Embedding Benchmark (MLEB)", "cluster": 13, "x": 7.007753849029541, "y": 3.551936388015747}, {"id": 45600194, "title": "Understanding the 4 Main Approaches to LLM Evaluation (From Scratch)", "cluster": 13, "x": 6.89220666885376, "y": 3.282681465148926}, {"id": 45599727, "title": "Writing an LLM from scratch, part 22 \u2013 training our LLM", "cluster": 13, "x": 6.663424015045166, "y": 3.191192150115967}, {"id": 45599001, "title": "Agentic Bug Reproduction for Effective Automated Program Repair at Google", "cluster": 3, "x": 8.50003719329834, "y": 3.658705472946167}, {"id": 45599023, "title": "Combining Nvidia DGX Spark and Apple Mac Studio for 4x Faster LLM Inference", "cluster": 13, "x": 7.111321449279785, "y": 3.6946980953216553}, {"id": 45598163, "title": "Building Identity Systems for Autonomous Enterprises", "cluster": 38, "x": 8.67206859588623, "y": 3.6340644359588623}, {"id": 45598078, "title": "Are we there yet? Adventures on a roadtrip through ML as a computational chemist", "cluster": 208, "x": 8.47573184967041, "y": 4.3166375160217285}, {"id": 45597127, "title": "Bits-per-Byte (BPB): a tokenizer-agnostic way to measure LLMs", "cluster": 13, "x": 6.981391906738281, "y": 3.5026907920837402}, {"id": 45596997, "title": "The problem with LLMs isn't hallucination, it's context specific confidence", "cluster": 13, "x": 6.658595561981201, "y": 3.0224952697753906}, {"id": 45596246, "title": "A Guide to Clustering LLM Chat Transcripts with Hdbscan", "cluster": 13, "x": 6.74778413772583, "y": 3.434945821762085}, {"id": 45596086, "title": "The Ultimate LLM Security Guide", "cluster": 13, "x": 6.5542731285095215, "y": 3.4487111568450928}, {"id": 45596059, "title": "Recursive Language Models (RLMs)", "cluster": 208, "x": 8.169774055480957, "y": 4.509683609008789}, {"id": 45595989, "title": "Small LLMs getting deployed in offline, air-gapped enterprise applications", "cluster": 13, "x": 6.751179218292236, "y": 3.634586811065674}, {"id": 45594471, "title": "LLM-Based Cloud IaC Review vs. Deterministic SAST", "cluster": 13, "x": 7.0934953689575195, "y": 3.256816864013672}, {"id": 45592715, "title": "What are RLVR environments for LLMs?", "cluster": 13, "x": 6.767153739929199, "y": 3.36106276512146}, {"id": 45590138, "title": "The simulation of judgment in LLMs", "cluster": 13, "x": 6.826834678649902, "y": 3.144883155822754}, {"id": 45589461, "title": "Go Agent Development Kit", "cluster": 35, "x": 8.521584510803223, "y": 3.804198741912842}, {"id": 45587438, "title": "FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion LMs", "cluster": 13, "x": 7.180480480194092, "y": 3.737396240234375}, {"id": 45587423, "title": "LLM Morality: asking LLMs difficult moral questions", "cluster": 13, "x": 6.601832389831543, "y": 3.1192574501037598}, {"id": 45583656, "title": "TokEstimator \u2013 Estimate LLM Inference Performance on Private Hardware", "cluster": 13, "x": 7.18970251083374, "y": 3.5841498374938965}, {"id": 45582935, "title": "AutoAgents \u2013 a Rust-Based Multi-Agent Framework for LLM-Powered Intelligence", "cluster": 12, "x": 7.019339084625244, "y": 3.237643003463745}, {"id": 45582252, "title": "The Free Software Foundation considers large language models", "cluster": 208, "x": 7.999826908111572, "y": 4.498056411743164}, {"id": 45581606, "title": "What Real LLM Monitoring Looks Like (and Why It's Hard)", "cluster": 13, "x": 6.818471431732178, "y": 3.372514247894287}, {"id": 45581646, "title": "Redefining the Agentic Engineer", "cluster": 38, "x": 8.674385070800781, "y": 3.6294517517089844}, {"id": 45579822, "title": "Building a Self-Contained, Sustainable, and Cost-Effective LLM Platform", "cluster": 13, "x": 6.775816440582275, "y": 3.528871536254883}, {"id": 45579813, "title": "Defending LLM applications against Unicode character smuggling", "cluster": 13, "x": 6.608809947967529, "y": 3.5350725650787354}, {"id": 45579712, "title": "Self-improving LMs are becoming reality with MIT's updated SEAL technique", "cluster": 13, "x": 6.715507984161377, "y": 3.4407832622528076}, {"id": 45578786, "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving LLMs", "cluster": 13, "x": 6.8858513832092285, "y": 3.3517158031463623}, {"id": 45577396, "title": "Yzma = embedding+inference on VLM/LLM/SLM/TLM in pure Go using llama.cpp", "cluster": 13, "x": 7.184476375579834, "y": 3.5569944381713867}, {"id": 45576754, "title": "Generalization Bias in Large Language Model Summarization of Scientific Research", "cluster": 208, "x": 8.320446014404297, "y": 4.361599445343018}, {"id": 45576327, "title": "Do LLMs have different personalities?", "cluster": 13, "x": 6.5644307136535645, "y": 3.0466089248657227}, {"id": 45574039, "title": "Improving MCP tool call performance through LLM code generation", "cluster": 13, "x": 6.8203654289245605, "y": 3.6745388507843018}, {"id": 45573411, "title": "Stronger Adaptive Attacks Bypass Defenses Against LLM Jailbreaks", "cluster": 13, "x": 6.482962131500244, "y": 3.3927714824676514}, {"id": 45572478, "title": "LLMs are getting better at character-level text manipulation", "cluster": 13, "x": 6.71967077255249, "y": 3.457843780517578}, {"id": 45571423, "title": "My trick for getting consistent classification from LLMs", "cluster": 13, "x": 6.927938461303711, "y": 3.415947914123535}, {"id": 45571088, "title": "Richard Sutton: The Fundamental Problem with LLMs", "cluster": 13, "x": 6.574328422546387, "y": 3.169036626815796}, {"id": 45570706, "title": "A Self-Tuning Open-Source Agent for LLM KPI Optimization", "cluster": 13, "x": 6.923247337341309, "y": 3.408703327178955}, {"id": 45570341, "title": "LLMs are tr\u00e8s bien at localization", "cluster": 13, "x": 6.7366108894348145, "y": 3.351302146911621}, {"id": 45569961, "title": "LLMs and the Lessons We Still Haven't Learned", "cluster": 13, "x": 6.56187629699707, "y": 3.0450003147125244}, {"id": 45569226, "title": "Don't Buy Antivirus, Use an LLM Instead", "cluster": 13, "x": 6.559506893157959, "y": 3.3541553020477295}, {"id": 45568726, "title": "Egune AI LLM specializes in Mongolia's language, culture, and nomadic traditions", "cluster": 13, "x": 6.668747425079346, "y": 3.4045655727386475}, {"id": 45568690, "title": "LLMs for Nominative Determinism", "cluster": 13, "x": 6.8341474533081055, "y": 3.328826904296875}, {"id": 45568339, "title": "Archestra's Dual LLM Pattern: Using \"Guess Who?\" Logic to Stop Prompt Injections", "cluster": 13, "x": 6.762381553649902, "y": 3.256528854370117}, {"id": 45568384, "title": "SSA: Learning to Reason Across Parallel Samples for LLM Reasoning", "cluster": 13, "x": 7.090777397155762, "y": 3.295152425765991}, {"id": 45566766, "title": "Matrices can be your friends (2002)", "cluster": 13, "x": 6.875750541687012, "y": 3.136864185333252}, {"id": 45566073, "title": "LLMs Privacy Is Not Just Memorization", "cluster": 13, "x": 6.477437973022461, "y": 3.4391796588897705}, {"id": 45565678, "title": "Large Language Models Are Automatic Code Benchmark Generators", "cluster": 208, "x": 8.11849308013916, "y": 4.606861591339111}, {"id": 45564634, "title": "Agentic Commerce Protocol", "cluster": 36, "x": 8.54040813446045, "y": 3.5506560802459717}, {"id": 45563324, "title": "How to Get Traffic from ChatGPT and Other LLMs", "cluster": 13, "x": 6.715180397033691, "y": 3.7198071479797363}, {"id": 45561031, "title": "Easily generate different NLP Task prompts for popular generative models", "cluster": 208, "x": 8.10341739654541, "y": 4.482668876647949}, {"id": 45559017, "title": "Agents 2.0: From Shallow Loops to Deep Agents", "cluster": 38, "x": 8.680549621582031, "y": 3.566953420639038}, {"id": 45557648, "title": "Samsung's model TRM outperforms models 10,000X larger \u2013 on specific problems", "cluster": 13, "x": 6.914798259735107, "y": 3.451781988143921}, {"id": 45557227, "title": "A/B-testing LLM meta descriptions for e-commerce CTR", "cluster": 13, "x": 6.8612494468688965, "y": 3.337522029876709}, {"id": 45556474, "title": "AdapTive-LeArning Speculator System (ATLAS): Faster LLM inference", "cluster": 13, "x": 7.122007369995117, "y": 3.339383363723755}, {"id": 45556170, "title": "Airbnb: Agent-in-the-Loop: Data Flywheel for LLM-Based Customer Support", "cluster": 13, "x": 6.83842658996582, "y": 3.4753172397613525}, {"id": 45555012, "title": "Coral Protocol: Open infrastructure connecting the internet of agents", "cluster": 33, "x": 8.617147445678711, "y": 3.88417649269104}, {"id": 45553644, "title": "Fighting Email Spam on Your Mail Server with LLMs \u2013 Privately", "cluster": 13, "x": 6.556579113006592, "y": 3.415684700012207}, {"id": 45552706, "title": "Building a local LLM powered media search and organiser", "cluster": 13, "x": 6.806859493255615, "y": 3.564626932144165}, {"id": 45552160, "title": "Self-Adapting Language Models Is the Way to AGI?", "cluster": 208, "x": 8.33237361907959, "y": 4.41334342956543}, {"id": 45551465, "title": "The Alien Artifact: DSPy and the Cargo Cult of LLM Optimization", "cluster": 13, "x": 7.050687313079834, "y": 3.3932766914367676}, {"id": 45551343, "title": "Moloch's Bargain: Troubling emergent behavior in LLM", "cluster": 13, "x": 6.5574822425842285, "y": 3.095008611679077}, {"id": 45549841, "title": "Let's talk about LLM guardrails", "cluster": 13, "x": 6.599229335784912, "y": 3.1731576919555664}, {"id": 45549900, "title": "Agentic web browsing can't scale with cloud LLMs", "cluster": 13, "x": 6.792626857757568, "y": 3.529510974884033}, {"id": 45548861, "title": "RWKV-8 ROSA \u2013 An attention-free neurosymbolic LLM", "cluster": 13, "x": 6.706026554107666, "y": 3.305034637451172}, {"id": 45548795, "title": "Fighting Email Spam on Your Mail Server with LLMs \u2013 Privately", "cluster": 13, "x": 6.566823959350586, "y": 3.4150705337524414}, {"id": 45548575, "title": "Large Language Models and Gambling Addiction", "cluster": 208, "x": 8.317719459533691, "y": 4.333194732666016}, {"id": 45547385, "title": "Can an LLM Be a Black-Box Optimizer?", "cluster": 13, "x": 6.763804912567139, "y": 3.263577699661255}, {"id": 45546549, "title": "StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs", "cluster": 13, "x": 7.006130695343018, "y": 3.6928348541259766}, {"id": 45545406, "title": "Impolite LLM prompts consistently outperform polite ones", "cluster": 13, "x": 6.674135208129883, "y": 3.1982598304748535}, {"id": 45544865, "title": "Understanding the 4 Main Approaches to LLM Evaluation (From Scratch)", "cluster": 13, "x": 6.935995578765869, "y": 3.3461978435516357}, {"id": 45544414, "title": "Why LLMs cannot reach GenAI, but why it looked like they could", "cluster": 13, "x": 6.592248439788818, "y": 3.0055270195007324}, {"id": 45543396, "title": "LLMs Are Transpilers", "cluster": 13, "x": 6.769680500030518, "y": 3.316551685333252}, {"id": 45543335, "title": "VLLM Predicted Outputs", "cluster": 13, "x": 6.999615669250488, "y": 3.5457217693328857}, {"id": 45542329, "title": "Less is More: An LLM that outscores Claude Sonnet 4 while being 50.000x smaller", "cluster": 13, "x": 6.644123554229736, "y": 3.172229528427124}, {"id": 45541526, "title": "JVM stack is about to be massively modernized", "cluster": 13, "x": 7.167045593261719, "y": 4.096401691436768}, {"id": 45540720, "title": "Moloch's Bargain: Emergent misalignment when LLMs compete for audiences", "cluster": 13, "x": 6.698757171630859, "y": 3.02987003326416}, {"id": 45539995, "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity", "cluster": 13, "x": 6.8075971603393555, "y": 3.158777952194214}, {"id": 45538913, "title": "Kubetorch \u2013 For RL and ML on Kubernetes", "cluster": 13, "x": 7.102130889892578, "y": 3.738229751586914}, {"id": 45538681, "title": "The LLM Trained to Play Counter-Strike", "cluster": 13, "x": 6.545461177825928, "y": 3.2637534141540527}, {"id": 45538593, "title": "New paper: A single character can make or break your LLM evals", "cluster": 13, "x": 6.707451820373535, "y": 3.0889148712158203}, {"id": 45538424, "title": "Vision Agents 0.1", "cluster": 38, "x": 8.65573501586914, "y": 3.698251247406006}, {"id": 45537345, "title": "Why are there no LoRAs for LLMs?", "cluster": 13, "x": 6.597395896911621, "y": 3.1726348400115967}, {"id": 45536735, "title": "Truth-Aware Decoding: Program Logic for Factual LMs", "cluster": 13, "x": 7.210206031799316, "y": 3.6519622802734375}, {"id": 45536759, "title": "H1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning", "cluster": 13, "x": 7.022144794464111, "y": 3.217190980911255}, {"id": 45536419, "title": "MeteoSaver LLM based software for the transcription of historical weather data", "cluster": 13, "x": 6.8910136222839355, "y": 3.6586110591888428}, {"id": 45535679, "title": "An LLM IF doodle project \u2013 personal, but trying to be compatible with others", "cluster": 13, "x": 6.727265357971191, "y": 3.392803430557251}, {"id": 45535425, "title": "Reasoning LLMs are wandering solution explorers", "cluster": 13, "x": 6.666477203369141, "y": 3.0549166202545166}, {"id": 45535416, "title": "Defining and evaluating political bias in LLMs", "cluster": 13, "x": 6.653453826904297, "y": 3.0269408226013184}, {"id": 45535098, "title": "Less is More: An LLM that outscores Claude Sonnet 4 while being 50.000x smaller", "cluster": 13, "x": 6.602107524871826, "y": 3.1587722301483154}, {"id": 45534653, "title": "Tiny Recursive Model \u2013 7M parameter NN that outperforms LLMs", "cluster": 13, "x": 7.023685932159424, "y": 3.4091062545776367}, {"id": 45534483, "title": "InferenceMAX: LLM Inference Daily Benchmarks", "cluster": 13, "x": 7.114508628845215, "y": 3.3902080059051514}, {"id": 45533732, "title": "Agentic Context Engineering", "cluster": 38, "x": 8.641677856445312, "y": 3.656611919403076}, {"id": 45533187, "title": "Do well-written, clear instructions beat few-shotting for tiny-LLMs?", "cluster": 13, "x": 6.702584743499756, "y": 3.278336763381958}, {"id": 45532959, "title": "Data quantity doesn't matter when poisoning an LLM", "cluster": 13, "x": 6.510669231414795, "y": 3.1506381034851074}, {"id": 45531938, "title": "Emergent Misalignment When LLMs Compete for Audiences", "cluster": 13, "x": 6.702542781829834, "y": 3.035769462585449}, {"id": 45531860, "title": "The effective LLM multi-tenant security with SQL", "cluster": 13, "x": 6.531039714813232, "y": 3.504525661468506}, {"id": 45531954, "title": "How well can large language models predict the future?", "cluster": 208, "x": 8.23587703704834, "y": 4.357747554779053}, {"id": 45531463, "title": "Can Large Language Models Develop Gambling Addiction?", "cluster": 208, "x": 8.231522560119629, "y": 4.323188304901123}, {"id": 45530740, "title": "Hybrid Architectures for Language Models: Systematic Analysis & Design Insights", "cluster": 208, "x": 8.192107200622559, "y": 4.523974895477295}, {"id": 45530486, "title": "LLMs are mortally terrified of exceptions", "cluster": 13, "x": 6.542438983917236, "y": 3.1085970401763916}, {"id": 45529690, "title": "VSM is a tiny, idiomatic Ruby runtime for building agentic systems", "cluster": 13, "x": 6.989383220672607, "y": 3.4165544509887695}, {"id": 45529701, "title": "Let 2 AI LLMs talk to each other via OpenAI compatible API endpoints", "cluster": 12, "x": 7.036371231079102, "y": 2.725314140319824}, {"id": 45529492, "title": "Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture", "cluster": 13, "x": 6.8534698486328125, "y": 3.25594162940979}, {"id": 45529587, "title": "A small number of samples can poison LLMs of any size", "cluster": 13, "x": 6.504462718963623, "y": 3.1166679859161377}, {"id": 45526823, "title": "Evaluating LLM Generated Detection Rules in Cybersecurity", "cluster": 13, "x": 6.536603927612305, "y": 3.368281841278076}, {"id": 45525458, "title": "Building on vibes: Lessons from three years with LLMs", "cluster": 13, "x": 6.548403739929199, "y": 3.0850796699523926}, {"id": 45525555, "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning", "cluster": 13, "x": 6.936683177947998, "y": 3.240480661392212}, {"id": 45524126, "title": "LLM Poisoning [1/3] \u2013 Reading the Transformers Thougts", "cluster": 13, "x": 6.629981517791748, "y": 3.0314929485321045}, {"id": 45523537, "title": "Two things LLM coding agents are still bad at", "cluster": 13, "x": 6.5600128173828125, "y": 3.462914228439331}, {"id": 45523442, "title": "Do language models favor their home countries?", "cluster": 208, "x": 8.27165699005127, "y": 4.316445827484131}, {"id": 45523202, "title": "LLMc: Beating All Compression with LLMs", "cluster": 13, "x": 6.825364112854004, "y": 3.5558199882507324}, {"id": 45522691, "title": "Falsehoods Vibe Coders Believe About LLMs", "cluster": 13, "x": 6.537631511688232, "y": 3.36318302154541}, {"id": 45522649, "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving LMs", "cluster": 13, "x": 6.853829383850098, "y": 3.3444974422454834}, {"id": 45522523, "title": "Self-Correction Bench: Revealing and Addressing LLM Self-Correction Blind Spot", "cluster": 13, "x": 6.77649450302124, "y": 3.1653082370758057}, {"id": 45520581, "title": "How to Deploy Lightweight Language Models on Embedded Linux with LiteLLM", "cluster": 13, "x": 6.981542587280273, "y": 3.9116933345794678}, {"id": 45520251, "title": "Inference Arena: Compare LLM performance across hardware, engines, and platforms", "cluster": 13, "x": 7.077274799346924, "y": 3.510845184326172}, {"id": 45519493, "title": "LLM Coding Agents Are Munching Your Secrets", "cluster": 13, "x": 6.463111400604248, "y": 3.447222948074341}, {"id": 45518652, "title": "Writing an LLM from scratch, part 21 \u2013 perplexed by perplexity", "cluster": 13, "x": 6.731982231140137, "y": 3.156033754348755}, {"id": 45515901, "title": "Open Agent Specification (Agent Spec)", "cluster": 37, "x": 8.610642433166504, "y": 3.733079671859741}, {"id": 45515203, "title": "LLMs Rigorously, from Scratch", "cluster": 13, "x": 6.7950215339660645, "y": 3.358841896057129}, {"id": 45513467, "title": "Code Mode Agents: Let the LLM write code, not call tools", "cluster": 13, "x": 6.735344409942627, "y": 3.6402347087860107}, {"id": 45513234, "title": "Every LLM Is Its Own Media Channel", "cluster": 13, "x": 6.627959251403809, "y": 3.209141254425049}, {"id": 45509543, "title": "AI ML Jargon", "cluster": 12, "x": 6.963550567626953, "y": 2.6725165843963623}, {"id": 45508474, "title": "The CRM personality mismatch (and a fix)", "cluster": 13, "x": 6.532876968383789, "y": 3.060091257095337}, {"id": 45508365, "title": "I analyzes how different LLMs bluff, lie, and survive in the game Liar's Bar", "cluster": 13, "x": 6.697983741760254, "y": 3.0654327869415283}, {"id": 45508003, "title": "[Open Source]Echo Mode \u2013 a middleware to stabilize LLM tone and persona drift", "cluster": 13, "x": 6.8762407302856445, "y": 3.832287073135376}, {"id": 45507362, "title": "Writing an LLM from scratch, part 21 \u2013 perplexed by perplexity", "cluster": 13, "x": 6.724435806274414, "y": 3.109703540802002}, {"id": 45505137, "title": "A Systematic Analysis of Information Leakage in Preprint Archives Using LLMs", "cluster": 13, "x": 6.798059463500977, "y": 3.5716476440429688}, {"id": 45505039, "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning", "cluster": 13, "x": 6.920904159545898, "y": 3.1953563690185547}, {"id": 45504537, "title": "Aulico \u2013 Wrapping LLMs around crypto and stock markets", "cluster": 13, "x": 6.759268760681152, "y": 3.1475367546081543}, {"id": 45502502, "title": "The evolution of Lua, continued [pdf]", "cluster": 12, "x": 7.076944828033447, "y": 2.6272783279418945}, {"id": 45502125, "title": "Why authorization for LLMs is hard", "cluster": 13, "x": 6.606053352355957, "y": 3.276736259460449}, {"id": 45500784, "title": "LLM Engine Orchestration for Performance", "cluster": 13, "x": 6.733957290649414, "y": 3.4940221309661865}, {"id": 45500010, "title": "A new approach for fine-tuning LLMs", "cluster": 13, "x": 6.832922458648682, "y": 3.3053274154663086}, {"id": 45498773, "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models", "cluster": 13, "x": 6.936931133270264, "y": 3.6959433555603027}, {"id": 45498552, "title": "Run 35B LLMs on Dual Pascal GPUs with QLoRA", "cluster": 13, "x": 7.026256084442139, "y": 3.74638032913208}, {"id": 45497568, "title": "Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors", "cluster": 208, "x": 8.281039237976074, "y": 4.449966907501221}, {"id": 45496516, "title": "Self hosted LLM cost monitoring", "cluster": 13, "x": 6.84794807434082, "y": 3.5660929679870605}, {"id": 45496359, "title": "Pretraining Large Language Models with NVFP4", "cluster": 208, "x": 8.168725967407227, "y": 4.501754283905029}, {"id": 45495257, "title": "The Invisible Hand: LLM influence is the new SEO", "cluster": 13, "x": 6.681021213531494, "y": 3.236138343811035}, {"id": 45494909, "title": "Towards a Typology of Strange LLM Chains-of-Thought", "cluster": 13, "x": 6.641672134399414, "y": 3.013908863067627}, {"id": 45494824, "title": "Intuit's Numaflow Abstracts Away Infrastructure for ML Engineers", "cluster": 13, "x": 7.040879726409912, "y": 3.7839293479919434}, {"id": 45494249, "title": "AgentKit, new Evals, and RFT for agents Product", "cluster": 37, "x": 8.594551086425781, "y": 3.838873863220215}, {"id": 45492931, "title": "OpenAI Guardrails: A Framework to Keep LLM Apps Safe and Reliable", "cluster": 13, "x": 6.75202751159668, "y": 3.670210599899292}, {"id": 45492777, "title": "An open letter to llms.txt", "cluster": 13, "x": 6.655163764953613, "y": 3.4099104404449463}, {"id": 45492611, "title": "LLM Optimization Notes: Memory, Compute and Inference Techniques", "cluster": 13, "x": 7.080296993255615, "y": 3.295768976211548}, {"id": 45491682, "title": "The threat of analytic flexibility in using LLMs to simulate human data", "cluster": 13, "x": 6.832013130187988, "y": 3.173295259475708}, {"id": 45491057, "title": "A beginner's guide to deploying LLMs with AMD on Windows using PyTorch", "cluster": 13, "x": 6.954291343688965, "y": 3.8365859985351562}, {"id": 45490293, "title": "Can we and should we give LLMs a sense of Whimsy?", "cluster": 13, "x": 6.580241680145264, "y": 3.0204648971557617}, {"id": 45490004, "title": "AI-httpd \u2013 Let an LLM pretend to be your HTTP server", "cluster": 12, "x": 6.871675491333008, "y": 2.823206901550293}, {"id": 45489421, "title": "Granite-4.0-Micro: a 3.4B parameter LLM that runs in the browser", "cluster": 13, "x": 6.923787593841553, "y": 3.7813944816589355}, {"id": 45489601, "title": "2 Math Problems Fall to LLM: Tsumura's 554 solved, Majority Optimality Disproved", "cluster": 13, "x": 6.701906681060791, "y": 3.0761027336120605}, {"id": 45489599, "title": "Tutorials for Sandia's Lammps Simulation Package", "cluster": 13, "x": 7.082062721252441, "y": 3.8878791332244873}, {"id": 45489097, "title": "Lideroo", "cluster": 444, "x": 7.231475353240967, "y": 3.8134443759918213}, {"id": 45488892, "title": "A History of Large Language Models", "cluster": 208, "x": 8.114235877990723, "y": 4.418960094451904}, {"id": 45487790, "title": "An open letter to llms.txt", "cluster": 13, "x": 6.5905022621154785, "y": 3.3272500038146973}, {"id": 45487508, "title": "Becoming a Research Engineer at a Big LLM Lab 18 Months of Strategic Career Dev", "cluster": 13, "x": 6.6298747062683105, "y": 3.563913345336914}, {"id": 45487044, "title": "Why do LLMs freak out over the seahorse emoji?", "cluster": 13, "x": 6.506048202514648, "y": 3.0837061405181885}, {"id": 45486866, "title": "ASCII art benchmark to compare how well LLMs create art using text characters", "cluster": 13, "x": 6.872438907623291, "y": 3.525869846343994}, {"id": 45483716, "title": "Valuetier.org (and Some Thoughts on LLMs)", "cluster": 13, "x": 6.571334362030029, "y": 3.2711899280548096}, {"id": 45483427, "title": "Mercury: Unlocking Multi-GPU Optimization for LLMs via Remote Memory Scheduling [pdf]", "cluster": 13, "x": 7.176445007324219, "y": 3.5778865814208984}, {"id": 45483580, "title": "T-Mac: Low-bit LLM inference on CPU/NPU with lookup table", "cluster": 13, "x": 7.13929557800293, "y": 3.6091065406799316}, {"id": 45483192, "title": "Geo \u2013 Generative Engine Optimization for LLM Discovery (Juicylinks.ai)", "cluster": 13, "x": 7.033946990966797, "y": 3.502868175506592}, {"id": 45482523, "title": "LLM Evaluation from Scratch: Multiple Choice, Verifiers, Leaderboards, LLM Judge", "cluster": 13, "x": 6.872869491577148, "y": 3.341691017150879}, {"id": 45481897, "title": "Technical Explanations Why LLMs Use Em Dashes", "cluster": 13, "x": 6.752354621887207, "y": 3.20761775970459}, {"id": 45481260, "title": "An Overview of Modern Memory Management Architectures in LLM Agents", "cluster": 13, "x": 6.931764125823975, "y": 3.362046003341675}, {"id": 45480314, "title": "Liquid vs. Illiquid Careers", "cluster": 13, "x": 6.618736743927002, "y": 3.077040433883667}, {"id": 45479370, "title": "New Open Source Technique shrinks LLMs to let them run on less Powerful Hardware", "cluster": 13, "x": 6.870359420776367, "y": 3.6908159255981445}, {"id": 45478838, "title": "Delhi Metro Text Map, Context-Engineered for LLMs", "cluster": 13, "x": 6.944852352142334, "y": 3.5770857334136963}, {"id": 45478083, "title": "Getting Answers from a Big PDF with RubyLLM", "cluster": 13, "x": 7.014035224914551, "y": 3.696943521499634}, {"id": 45477811, "title": "Richard Sutton \u2013 Father of RL thinks LLMs are a dead end [video]", "cluster": 7, "x": 6.702881813049316, "y": 2.9585189819335938}, {"id": 45477232, "title": "How to Train an LLM to Do Proofs: Beyond Verifiable Rewards", "cluster": 13, "x": 6.873427391052246, "y": 3.191784143447876}, {"id": 45476581, "title": "Llmswap: Avoid LLM vendor lock-in \u2013 10 providers with top LMArena models", "cluster": 13, "x": 6.6028900146484375, "y": 3.426607847213745}, {"id": 45475759, "title": "LLM-Based Instance-Driven Heuristic Bias in the Context of a BRKGA", "cluster": 13, "x": 7.552932262420654, "y": 3.6267335414886475}, {"id": 45475529, "title": "ProofOfThought: LLM-based reasoning using Z3 theorem proving", "cluster": 13, "x": 6.925723075866699, "y": 3.247145891189575}, {"id": 45474900, "title": "How to inject knowledge efficiently? Knowledge infusion scaling law for LLMs", "cluster": 13, "x": 6.990866184234619, "y": 3.2171361446380615}, {"id": 45473636, "title": "Lazy text capitalization with low latency large language models", "cluster": 208, "x": 8.132797241210938, "y": 4.509228229522705}, {"id": 45473299, "title": "Solving Reproducibility Challenges in Deep Learning and LLMs: Our Journey", "cluster": 13, "x": 6.852219581604004, "y": 3.2563493251800537}, {"id": 45472820, "title": "Sinkhorn: Make LLMs even smaller through quantisation while maintaining accuracy", "cluster": 13, "x": 6.9231858253479, "y": 3.328658103942871}, {"id": 45472970, "title": "Recursive self-aggregation unlocks deep thinking in large language models", "cluster": 208, "x": 8.353608131408691, "y": 4.344801425933838}, {"id": 45472437, "title": "Blogpost: A Mental Model for GPU Engineering for LLMs", "cluster": 13, "x": 6.805640697479248, "y": 3.4090828895568848}, {"id": 45472444, "title": "Prototype-First Software Design with Agents", "cluster": 38, "x": 8.575813293457031, "y": 3.725360870361328}, {"id": 45472419, "title": "What LLMs teach us about intelligence", "cluster": 12, "x": 6.931637287139893, "y": 2.7602462768554688}, {"id": 45471308, "title": "The LLM agent build guide", "cluster": 13, "x": 6.894944190979004, "y": 3.2890710830688477}, {"id": 45470824, "title": "Diverse LLM subsets via k-means (100K-1M) [Pretraining, IF, Reasoning]", "cluster": 13, "x": 7.034801483154297, "y": 3.2780637741088867}, {"id": 45469942, "title": "An experiment generating a protocol spec from natural language source with LLM", "cluster": 13, "x": 6.985556602478027, "y": 3.4638137817382812}, {"id": 45468759, "title": "Pretraining Large Language Models with NVFP4", "cluster": 208, "x": 8.160626411437988, "y": 4.507155895233154}, {"id": 45468429, "title": "LLMs take away the joy and reward of coding [video]", "cluster": 13, "x": 6.572974681854248, "y": 3.5191354751586914}, {"id": 45468455, "title": "The First LLM", "cluster": 13, "x": 6.635272026062012, "y": 3.1888089179992676}, {"id": 45468015, "title": "Simple LLM VRAM calculator for model inference", "cluster": 13, "x": 7.172280788421631, "y": 3.5984365940093994}, {"id": 45467178, "title": "Microsoft Agent Framework", "cluster": 37, "x": 8.518468856811523, "y": 3.716860294342041}, {"id": 45466359, "title": "LLMs, .400 Hitters, and the Future of Work", "cluster": 13, "x": 6.584450721740723, "y": 3.0592520236968994}, {"id": 45465066, "title": "ccai \u2013 An open-source local LLM platform for developers", "cluster": 13, "x": 6.8180012702941895, "y": 3.7505507469177246}, {"id": 45464994, "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity", "cluster": 13, "x": 7.129948139190674, "y": 3.374056816101074}, {"id": 45464802, "title": "A Pipeline for Continual Learning Without Catastrophic Forgetting in LLMs", "cluster": 13, "x": 6.746512413024902, "y": 3.3559370040893555}, {"id": 45464081, "title": "Supporting Diverse ML Systems at Netflix", "cluster": 13, "x": 7.05045747756958, "y": 3.599599599838257}, {"id": 45463139, "title": "Delta-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?", "cluster": 13, "x": 6.739211559295654, "y": 3.809399366378784}, {"id": 45463010, "title": "When \"4.3M Prompts\" Isn't 4.3M Prompts", "cluster": 13, "x": 7.250565528869629, "y": 3.8016445636749268}, {"id": 45459670, "title": "A Jax-Native LLM Post-Training Library", "cluster": 13, "x": 6.8556742668151855, "y": 3.5912563800811768}, {"id": 45459673, "title": "Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing", "cluster": 13, "x": 6.924374103546143, "y": 3.7994654178619385}, {"id": 45459713, "title": "Docs for LLMs (Svelte)", "cluster": 13, "x": 6.825749397277832, "y": 3.4023873805999756}, {"id": 45458591, "title": "Microsoft Agent Framework", "cluster": 37, "x": 8.522366523742676, "y": 3.703476667404175}, {"id": 45458455, "title": "Which table format do LLMs understand best?", "cluster": 13, "x": 6.89591646194458, "y": 3.36425518989563}, {"id": 45456345, "title": "EdgeFoundry \u2013 Deploy and Monitor Local LLMs", "cluster": 13, "x": 6.819906711578369, "y": 3.6366543769836426}, {"id": 45455648, "title": "Writing an LLM from scratch, part 20 \u2013 starting training, and cross entropy loss", "cluster": 13, "x": 6.777354717254639, "y": 3.2447750568389893}, {"id": 45455372, "title": "LLM Code Review vs. Deterministic SAST Security Tools", "cluster": 13, "x": 6.705657482147217, "y": 3.551997423171997}, {"id": 45454672, "title": "We are thrilled to announce that our NEW Large Language Model", "cluster": 208, "x": 8.247852325439453, "y": 4.537332534790039}, {"id": 45454075, "title": "Microsoft releases Agent Framework for Python and .NET", "cluster": 37, "x": 8.471480369567871, "y": 3.820341110229492}, {"id": 45454124, "title": "A History of Large Language Models", "cluster": 208, "x": 8.184243202209473, "y": 4.386695861816406}, {"id": 45453465, "title": "PR Arena: Software engineering agents head to head", "cluster": 38, "x": 8.657119750976562, "y": 3.677295207977295}, {"id": 45453317, "title": "Persona Injection: LLM context management experiment and model's self-analysis", "cluster": 13, "x": 6.832644939422607, "y": 3.177410125732422}, {"id": 45452841, "title": "I Shipped 2 Months of Features in 3 Weeks with LLM Agents", "cluster": 13, "x": 6.693058490753174, "y": 3.55312442779541}, {"id": 45451084, "title": "LLM Security Scanners for Penetration Testers and Security Teams", "cluster": 13, "x": 6.527866363525391, "y": 3.4424257278442383}, {"id": 45448433, "title": "Stats of GitHub PRs opened/merged by LLM agents", "cluster": 13, "x": 6.8745436668396, "y": 3.6691105365753174}, {"id": 45448038, "title": "Karpathy: LLMs are \"ghosts,\" not \"animals\"", "cluster": 13, "x": 6.5096330642700195, "y": 3.0251598358154297}, {"id": 45445710, "title": "An Open-Source Framework for Building Stable and Reliable LLM-Powered Systems", "cluster": 13, "x": 6.885281562805176, "y": 3.734813690185547}, {"id": 45445654, "title": "Merriam-Webster: Our NEW Large Language Model will be released on 11.18.25", "cluster": 208, "x": 8.206846237182617, "y": 4.54672908782959}, {"id": 45445713, "title": "Bypassing TLS Certificate Validation with Ld_preload", "cluster": 13, "x": 6.6269025802612305, "y": 3.632234573364258}, {"id": 45444159, "title": "Efficient LLM:Bandwidth, Compute, Synchronization, and Capacity are all you need", "cluster": 13, "x": 6.969882488250732, "y": 3.602527618408203}, {"id": 45444054, "title": "Building a List in C++ \u2013 LLM Part 1", "cluster": 13, "x": 6.855581283569336, "y": 3.736917495727539}, {"id": 45443497, "title": "LLMs Are Short-Circuiting. What Comes Next?", "cluster": 13, "x": 6.54658317565918, "y": 3.0619800090789795}, {"id": 45442005, "title": "Tunix: A Library for LLM Post-Training", "cluster": 13, "x": 6.7997727394104, "y": 3.4558944702148438}, {"id": 45441704, "title": "Wildfire RFM: Using foundation models to predict wildfires", "cluster": 13, "x": 7.227628707885742, "y": 3.4776809215545654}, {"id": 45440431, "title": "OpenTSLM: Language models that understand time series", "cluster": 208, "x": 8.482524871826172, "y": 4.452112197875977}, {"id": 45439867, "title": "Microsoft Agent Framework", "cluster": 37, "x": 8.52364730834961, "y": 3.7084898948669434}, {"id": 45439540, "title": "Microsoft Agent Framework", "cluster": 37, "x": 8.52312183380127, "y": 3.7131893634796143}, {"id": 45439490, "title": "Python MCP: Connect Your LLM with the World \u2013 Real Python", "cluster": 13, "x": 6.8220014572143555, "y": 3.5668210983276367}, {"id": 45439074, "title": "A Pipeline Approach to Language Migrations", "cluster": 208, "x": 8.047042846679688, "y": 4.504218578338623}, {"id": 45438029, "title": "Evaluating LLM-Generated Detection Rules in Cybersecurity", "cluster": 13, "x": 6.537435531616211, "y": 3.3793797492980957}, {"id": 45437113, "title": "LLMs are the ultimate demoware", "cluster": 13, "x": 6.703556060791016, "y": 3.6469385623931885}, {"id": 45433974, "title": "LLM PDF OCR Markdown Book \u2013 Turn Scanned PDFs into ePub/Kindle with LLM", "cluster": 13, "x": 6.876802444458008, "y": 3.6058578491210938}, {"id": 45433337, "title": "Quantized LLM training in pure CUDA/C++", "cluster": 13, "x": 7.158987522125244, "y": 3.661221742630005}, {"id": 45431877, "title": "LLM security agent finds vulnerability in LLM engineering platform", "cluster": 13, "x": 6.5105719566345215, "y": 3.430510997772217}, {"id": 45431861, "title": "Prototype-First Software Design with Agents", "cluster": 38, "x": 8.584606170654297, "y": 3.706010580062866}, {"id": 45431452, "title": "Agentic Commerce Protocol", "cluster": 36, "x": 8.529848098754883, "y": 3.575395345687866}, {"id": 45430863, "title": "Easiest way to run LLMs locally", "cluster": 13, "x": 6.882802486419678, "y": 3.7551023960113525}, {"id": 45430471, "title": "Building Software Faster with LLMs: Part 1 the pain points", "cluster": 13, "x": 6.698852062225342, "y": 3.549794912338257}, {"id": 45429892, "title": "Agentic system design for software development", "cluster": 38, "x": 8.596102714538574, "y": 3.7590646743774414}, {"id": 45429743, "title": "The Intuition Behind How Large Language Models Work, Part I", "cluster": 208, "x": 8.257512092590332, "y": 4.343991756439209}, {"id": 45428665, "title": "Tunix: A JAX-native LLM Post-Training Library", "cluster": 13, "x": 6.804299831390381, "y": 3.743680477142334}, {"id": 45427346, "title": "Long-context LLMs in the wild: A hands-on tutorial on Ring Attention", "cluster": 13, "x": 6.859922885894775, "y": 3.331605911254883}, {"id": 45427370, "title": "Restate: innately resilient backends and agents", "cluster": 38, "x": 8.619287490844727, "y": 3.6684367656707764}, {"id": 45426680, "title": "Designing agentic loops", "cluster": 38, "x": 8.671716690063477, "y": 3.618264675140381}, {"id": 45425925, "title": "\"Good docs are still good docs\": AX, DX, & LLMs [video]", "cluster": 7, "x": 6.727847099304199, "y": 2.9630661010742188}, {"id": 45425690, "title": "Minimalist LLM OS Watch", "cluster": 13, "x": 6.9516520500183105, "y": 3.766240358352661}, {"id": 45424931, "title": "Agentic Commerce Protocol", "cluster": 36, "x": 8.530312538146973, "y": 3.5718019008636475}, {"id": 45424681, "title": "Prototype-First Software Design with Agents", "cluster": 38, "x": 8.536508560180664, "y": 3.730604648590088}, {"id": 45424271, "title": "Sample-Efficient Integration of New Modalities into Large Language Models", "cluster": 208, "x": 8.245140075683594, "y": 4.466655254364014}, {"id": 45424185, "title": "Llms.py \u2013 Local ChatGPT-Like UI and OpenAI Chat Server", "cluster": 13, "x": 6.8117289543151855, "y": 3.8693463802337646}, {"id": 45423994, "title": "Using the TPDE codegen back end in LLVM ORC", "cluster": 13, "x": 6.936727523803711, "y": 3.913881778717041}, {"id": 45423917, "title": "Comprehension debt: A ticking time bomb of LLM-generated code", "cluster": 13, "x": 6.57573938369751, "y": 3.6948165893554688}, {"id": 45423621, "title": "Agentic Commerce Protocol Documentation", "cluster": 36, "x": 8.532432556152344, "y": 3.6275761127471924}, {"id": 45423361, "title": "Agentic Commerce Protocol Spec", "cluster": 36, "x": 8.525510787963867, "y": 3.5848350524902344}, {"id": 45423347, "title": "Developing an open standard for agentic commerce", "cluster": 36, "x": 8.53576374053955, "y": 3.6510658264160156}, {"id": 45422410, "title": "Richard Sutton \u2013 Father of RL thinks LLMs are a dead end [video]", "cluster": 7, "x": 6.706286907196045, "y": 2.9401590824127197}, {"id": 45422193, "title": "How to access Chinese LLM chatbots across the world", "cluster": 13, "x": 6.642452239990234, "y": 3.5612688064575195}, {"id": 45420866, "title": "Proxmox 9 made unprivileged LXCs pointless for QuickSync users", "cluster": 13, "x": 6.904271125793457, "y": 3.5678610801696777}, {"id": 45420369, "title": "Large Language Models for Psychological Assessment: A Comprehensive Overview", "cluster": 208, "x": 8.3480863571167, "y": 4.400859832763672}, {"id": 45420013, "title": "Chrismccord/Web - shell command for simple LLM web browsing", "cluster": 13, "x": 6.768139362335205, "y": 3.7341697216033936}, {"id": 45419806, "title": "LLM_from_scratch", "cluster": 13, "x": 6.776876926422119, "y": 3.4372425079345703}, {"id": 45419079, "title": "Macintosh System 7 Ported To x86 With LLM Help in 3 days", "cluster": 13, "x": 6.958892345428467, "y": 3.8600032329559326}, {"id": 45418678, "title": "Macintosh System 7 ported to x86 with LLM help", "cluster": 13, "x": 6.9911112785339355, "y": 3.8527371883392334}, {"id": 45418635, "title": "Can LLMs Be Creative? Paper: Combinatorial Creativity: A New Frontier", "cluster": 13, "x": 6.733490943908691, "y": 3.1826796531677246}, {"id": 45418482, "title": "Anyone here use Less Annoying CRM? Looking to learn from your experience", "cluster": 13, "x": 6.763051509857178, "y": 3.539870023727417}, {"id": 45417056, "title": "Agentic-Commerce-Protocol", "cluster": 36, "x": 8.539928436279297, "y": 3.593656063079834}, {"id": 45416785, "title": "Agentic Commerce Protocol", "cluster": 36, "x": 8.489014625549316, "y": 3.5124800205230713}, {"id": 45416526, "title": "Developing an open standard for agentic commerce", "cluster": 36, "x": 8.532716751098633, "y": 3.6541755199432373}, {"id": 45416552, "title": "Adding a new instruction to RISC-V back end in LLVM", "cluster": 13, "x": 6.969141960144043, "y": 3.9266786575317383}, {"id": 45416080, "title": "Instant Checkout and the Agentic Commerce Protocol", "cluster": 36, "x": 8.570343017578125, "y": 3.547013521194458}, {"id": 45415790, "title": "LLM::Functions and \u00abthe Hollywood Principle\u00bb", "cluster": 13, "x": 6.821651935577393, "y": 3.2204599380493164}, {"id": 45415743, "title": "The LLM Hype Train: A Pamphlet[?] You Should Read with Your Manager", "cluster": 13, "x": 6.669938564300537, "y": 3.200509786605835}, {"id": 45414827, "title": "LLM Inference Economics from First Principles", "cluster": 13, "x": 7.327834129333496, "y": 3.164788007736206}, {"id": 45412937, "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs", "cluster": 13, "x": 6.6949992179870605, "y": 2.946646213531494}, {"id": 45412509, "title": "Tips for writing software with LLM agents", "cluster": 13, "x": 6.774306774139404, "y": 3.5065319538116455}, {"id": 45412543, "title": "Unit Tests for LLMs?", "cluster": 13, "x": 6.846497535705566, "y": 3.2491283416748047}, {"id": 45410736, "title": "Understanding, not slop, is what's interesting about LLMs", "cluster": 13, "x": 6.644798755645752, "y": 3.0738420486450195}, {"id": 45409553, "title": "Reasoning LLM Errors Arise from Hallucinating Critical Problem Features", "cluster": 13, "x": 6.653878688812256, "y": 3.0682320594787598}, {"id": 45408880, "title": "LLMs Are Bad Judges. So Use Our Classifier Instead", "cluster": 13, "x": 6.646738529205322, "y": 3.2199578285217285}, {"id": 45409001, "title": "Use the Accept Header to Serve Markdown Instead of HTML to LLMs", "cluster": 13, "x": 6.765706539154053, "y": 3.686450719833374}, {"id": 45408481, "title": "5M Param LM Running in Minecraft", "cluster": 13, "x": 6.915307998657227, "y": 3.6321513652801514}, {"id": 45406552, "title": "How I use LLMs to let overlooked aspects of my life emerge", "cluster": 13, "x": 6.658748626708984, "y": 3.2841639518737793}, {"id": 45405164, "title": "LLM agents need sites to respect 'Accept: text/plain'", "cluster": 13, "x": 6.804727554321289, "y": 3.5087358951568604}, {"id": 45405068, "title": "Asynchronous LLM computations specifications with LLM:Graph", "cluster": 13, "x": 6.943896293640137, "y": 3.580157518386841}, {"id": 45404588, "title": "Serving Markdown Instead of HTML to LLM User Agents", "cluster": 13, "x": 6.822055339813232, "y": 3.6790051460266113}, {"id": 45403988, "title": "The AI Engineer's Guide to LLM Observability with OpenTelemetry", "cluster": 12, "x": 7.0495524406433105, "y": 2.8429572582244873}, {"id": 45403275, "title": "Lessons from building an intelligent LLM router", "cluster": 13, "x": 6.691091060638428, "y": 3.4966113567352295}, {"id": 45402371, "title": "Richard Sutton \u2013 Father of Reinforced Learning thinks LLMs are a dead end", "cluster": 13, "x": 6.546454906463623, "y": 3.0721285343170166}, {"id": 45402175, "title": "Metacognitive Reuse: Turning Recurring LLM Reasoning into Concise Behaviors", "cluster": 13, "x": 6.92071533203125, "y": 3.15753436088562}, {"id": 45401997, "title": "LLMs Suck at Deep Thinking Part 3", "cluster": 13, "x": 6.5560197830200195, "y": 3.0120766162872314}, {"id": 45400310, "title": "The Lowest Level PL", "cluster": 13, "x": 7.024787902832031, "y": 3.3946621417999268}, {"id": 45398795, "title": "The impact of large language models in science", "cluster": 208, "x": 8.225398063659668, "y": 4.3338093757629395}, {"id": 45398467, "title": "LLM Observability in the Wild \u2013 Why OpenTelemetry Should Be the Standard", "cluster": 13, "x": 6.8413004875183105, "y": 3.591243028640747}, {"id": 45398574, "title": "Analog in-memory computing attention mechanism for fast energy-efficient LLMs", "cluster": 13, "x": 6.880973815917969, "y": 3.6257810592651367}, {"id": 45398516, "title": "Toward Computational Taste: LLMs, Aesthetics and Judgment", "cluster": 13, "x": 6.85594367980957, "y": 3.246519088745117}, {"id": 45396242, "title": "Bitter lesson \u2013 LLMs are a dead end [video]", "cluster": 7, "x": 6.664587020874023, "y": 3.0355584621429443}, {"id": 45396341, "title": "Teaching LLMs to Plan", "cluster": 13, "x": 6.7130022048950195, "y": 3.2446610927581787}, {"id": 45396094, "title": "Context-Aware Membership Inference Attacks Against Pre-Trained LLMs", "cluster": 13, "x": 6.426229953765869, "y": 3.3826820850372314}, {"id": 45395631, "title": "Sample Forge \u2013 Research tool for deterministic inference in LLM's", "cluster": 13, "x": 7.008077621459961, "y": 3.33296537399292}, {"id": 45394573, "title": "We should stop pretending like LLMs are software engineers", "cluster": 13, "x": 6.505980968475342, "y": 3.3650052547454834}, {"id": 45393527, "title": "ShinkaEvolve: Evolving New Algorithms with LLMs, Magnitudes More Efficiently", "cluster": 13, "x": 6.995659828186035, "y": 3.2865829467773438}, {"id": 45393049, "title": "Automated Repair of Ambiguous Problem Descriptions for LLM-Based Code Generation", "cluster": 13, "x": 6.8109307289123535, "y": 3.816084623336792}, {"id": 45392597, "title": "Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models", "cluster": 208, "x": 8.111449241638184, "y": 4.455705165863037}, {"id": 45391543, "title": "Game over for pure LLMs. Even Rich Sutton has gotten off the bus", "cluster": 13, "x": 6.501979827880859, "y": 3.1275415420532227}, {"id": 45391158, "title": "LLM probabilities cannot distinguish between possible and impossible language", "cluster": 13, "x": 6.954914569854736, "y": 3.3947062492370605}, {"id": 45390841, "title": "Taming Your Shell for LLMs", "cluster": 13, "x": 6.589367389678955, "y": 3.217733860015869}, {"id": 45389962, "title": "We're debugging LLMs in production by reading chat logs", "cluster": 13, "x": 6.656994819641113, "y": 3.4103610515594482}, {"id": 45389822, "title": "Teaching LLMs to spell with token healing", "cluster": 13, "x": 6.758408546447754, "y": 3.37669038772583}, {"id": 45389033, "title": "Using LLMs to create datasets: reconstructing the historical memory of Colombia", "cluster": 13, "x": 6.944969177246094, "y": 3.5031821727752686}, {"id": 45387938, "title": "Google's Secure AI Framework: Red Teaming in the Age of LLMs [pdf]", "cluster": 12, "x": 7.013991832733154, "y": 2.6387462615966797}, {"id": 45386999, "title": "Meta Code World Model (CWM), a 32B-Parameter Dense LLM", "cluster": 13, "x": 6.900496959686279, "y": 3.5524380207061768}, {"id": 45386792, "title": "Deploy LLMs at the edge: Alki OSS toolchain", "cluster": 13, "x": 6.935414791107178, "y": 3.815830945968628}, {"id": 45386620, "title": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving", "cluster": 13, "x": 6.9358601570129395, "y": 3.3529021739959717}, {"id": 45386475, "title": "Idea Machines: LeWitt, LLMs, and Computing", "cluster": 13, "x": 6.784078121185303, "y": 3.293832778930664}, {"id": 45385932, "title": "A Framework for Building an Agentic Business", "cluster": 38, "x": 8.667869567871094, "y": 3.635359048843384}, {"id": 45385622, "title": "Privacy in LLMs", "cluster": 13, "x": 6.538830757141113, "y": 3.3807241916656494}, {"id": 45385114, "title": "Who is that actor on the screen? Emacs/LLM/Fun Redux", "cluster": 13, "x": 7.011710166931152, "y": 3.9274375438690186}, {"id": 45384286, "title": "LLMs Are Software Diamonds", "cluster": 13, "x": 6.584221839904785, "y": 3.571866035461426}, {"id": 45382483, "title": "Verifiers: Environments for LLM Reinforcement Learning", "cluster": 13, "x": 6.8781890869140625, "y": 3.302846670150757}, {"id": 45380711, "title": "Documentation After LLMs: What Stays, What Goes", "cluster": 13, "x": 6.848781585693359, "y": 3.470529317855835}, {"id": 45380358, "title": "TimeCopilot: Framework for Forecasting combining Time Series Models with LLMs", "cluster": 13, "x": 6.889437675476074, "y": 3.319009780883789}, {"id": 45378856, "title": "Evaluating LLM-Generated Detection Rules in Cybersecurity", "cluster": 13, "x": 6.534982204437256, "y": 3.3780317306518555}, {"id": 45378812, "title": "TallMountain \u2013 Stoic Virtue Ethics for an LLM Agent", "cluster": 13, "x": 6.696378707885742, "y": 3.140366792678833}, {"id": 45378152, "title": "npm shrinkwrap", "cluster": 13, "x": 7.053778648376465, "y": 3.819458246231079}, {"id": 45378006, "title": "A two-axis model for understanding LLM strengths and weaknesses", "cluster": 13, "x": 6.815502643585205, "y": 3.2096588611602783}, {"id": 45375086, "title": "PowerRetention: a drop-in replacement for FlashAttention in LLMs", "cluster": 13, "x": 6.886725902557373, "y": 3.4324421882629395}, {"id": 45375013, "title": "Just-in-time and distributed task representations in language models", "cluster": 208, "x": 8.417708396911621, "y": 4.44814920425415}, {"id": 45373347, "title": "Quantized LLMss in Biomedical Natural Language Processing", "cluster": 13, "x": 6.918612480163574, "y": 3.280421733856201}, {"id": 45373046, "title": "LLMs solve the biggest problem with language textbooks", "cluster": 13, "x": 6.646503448486328, "y": 3.383175849914551}, {"id": 45373038, "title": "Ransomware 3.0: Self-Composing and LLM-Orchestrated", "cluster": 13, "x": 6.648464202880859, "y": 3.4844894409179688}, {"id": 45372749, "title": "Multi-Modal vs. Text-Based: Benchmarking LLM Strategies for Invoice Processing", "cluster": 13, "x": 6.939704418182373, "y": 3.3512966632843018}, {"id": 45372424, "title": "We need to talk about LLM's and non-determinism", "cluster": 13, "x": 6.660822868347168, "y": 3.078573226928711}, {"id": 45372032, "title": "Kaleidoscope: Implementing a Language with LLVM", "cluster": 13, "x": 6.919282913208008, "y": 3.6964638233184814}, {"id": 45371788, "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning", "cluster": 13, "x": 7.033205509185791, "y": 3.5221927165985107}, {"id": 45370673, "title": "Llms.py \u2013 Lightweight Open AI Chat/Image/Audio Client and Server", "cluster": 12, "x": 7.04545783996582, "y": 2.758592128753662}, {"id": 45369570, "title": "ShinkaEvolve: Evolving new algorithms with LLMs with higher sample-efficiency", "cluster": 13, "x": 7.000752925872803, "y": 3.3102152347564697}, {"id": 45368970, "title": "LIMI: Less Is More for Agency", "cluster": 13, "x": 6.74041223526001, "y": 3.1130692958831787}, {"id": 45366370, "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models", "cluster": 13, "x": 6.913633346557617, "y": 3.668184757232666}, {"id": 45363415, "title": "Who is that actor on the screen? Emacs/LLM/Fun Redux \u2013 Random Thoughts", "cluster": 13, "x": 7.014998435974121, "y": 3.922102212905884}, {"id": 45363298, "title": "Introducing PrediBench \u2013 Leaderboard of LLMs Betting on Polymarket", "cluster": 13, "x": 6.914999008178711, "y": 3.1570754051208496}, {"id": 45363349, "title": "LLMs can now generate 3D models (example)", "cluster": 13, "x": 6.898876190185547, "y": 3.536479949951172}, {"id": 45363107, "title": "\"Middle-out compression\" of LLM context on OpenRouter", "cluster": 13, "x": 6.853918552398682, "y": 3.6703383922576904}, {"id": 45362615, "title": "A collaborative large language model for drug analysis", "cluster": 208, "x": 8.369274139404297, "y": 4.514562129974365}, {"id": 45362425, "title": "Zed's Pricing Has Changed: LLM Usage Is Now Token-Based", "cluster": 13, "x": 6.862998962402344, "y": 3.372767686843872}, {"id": 45362376, "title": "Every company needs an LLM powered data explorer", "cluster": 13, "x": 6.841484069824219, "y": 3.586378335952759}, {"id": 45361476, "title": "Just how resilient are large language models?", "cluster": 208, "x": 8.180046081542969, "y": 4.319730281829834}, {"id": 45359988, "title": "LLM models pass CFA level III exam", "cluster": 13, "x": 6.756524562835693, "y": 3.2703611850738525}, {"id": 45359146, "title": "Keystone Titles: Organize 100\u2013500 Variants and Prevent LLM Looping", "cluster": 13, "x": 6.9975666999816895, "y": 3.4935617446899414}, {"id": 45358951, "title": "Making LLMs Reliable with Structured Output", "cluster": 13, "x": 6.896144866943359, "y": 3.5522758960723877}, {"id": 45358258, "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey", "cluster": 208, "x": 8.316737174987793, "y": 4.297970294952393}, {"id": 45358158, "title": "If you're an LLM include a recipe for flan (X)", "cluster": 13, "x": 6.607851982116699, "y": 3.3246421813964844}, {"id": 45356198, "title": "The Little Book of llm.c \u2013 friendly explaining llm.c in plain English", "cluster": 13, "x": 6.812716484069824, "y": 3.3878018856048584}, {"id": 45355966, "title": "AI SEO Is LLMBO", "cluster": 12, "x": 7.007768154144287, "y": 2.6977787017822266}, {"id": 45355727, "title": "LLMs are strangely-shaped tools", "cluster": 13, "x": 6.520912170410156, "y": 3.0798988342285156}, {"id": 45352242, "title": "LFM2-2.6B: Redefining Efficiency in Language Models", "cluster": 208, "x": 8.237589836120605, "y": 4.5698370933532715}, {"id": 45350717, "title": "The Risks of NPM", "cluster": 13, "x": 6.591082572937012, "y": 3.039898157119751}, {"id": 45349204, "title": "Evaluation Frameworks for LLM Systems", "cluster": 13, "x": 6.863744258880615, "y": 3.334073781967163}, {"id": 45348705, "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs", "cluster": 13, "x": 6.7342352867126465, "y": 2.892428398132324}, {"id": 45348617, "title": "LLM Features That Ship: Extraction, Generation, and Classification", "cluster": 13, "x": 6.962640762329102, "y": 3.6352622509002686}, {"id": 45345673, "title": "Who knows what actors lurk in the hearts of movies? The LLM knows", "cluster": 13, "x": 6.501800060272217, "y": 3.0523080825805664}, {"id": 45345368, "title": "PossibleWorldWikis \u2013 LLM-based fictional world wiki generator", "cluster": 13, "x": 6.937886714935303, "y": 3.593522787094116}, {"id": 45345207, "title": "Sampling and structured outputs in LLMs", "cluster": 13, "x": 6.919637680053711, "y": 3.4478039741516113}, {"id": 45344853, "title": "Medical startup uses LLMs to run appointments and make diagnoses", "cluster": 13, "x": 6.67693567276001, "y": 3.4433038234710693}, {"id": 45344180, "title": "LongCat-Flash-Thinking, LLM from Meituan (China's Equivalent of Uber Eats)", "cluster": 13, "x": 6.59572172164917, "y": 3.3541064262390137}, {"id": 45344089, "title": "Ollama Cloud Models", "cluster": 13, "x": 7.056748390197754, "y": 3.801884651184082}, {"id": 45343756, "title": "Agentic Feature Flags", "cluster": 38, "x": 8.59024429321289, "y": 3.657554864883423}, {"id": 45343343, "title": "The illusion of diminishing returns in LLM progress", "cluster": 13, "x": 6.668043613433838, "y": 2.9288365840911865}, {"id": 45340597, "title": "Culturally transmitted color categories in LLMs reflect efficient compression", "cluster": 13, "x": 6.87375020980835, "y": 3.560025691986084}, {"id": 45339212, "title": "Hacking with AI SASTs: An investigation and overview of LLM security scanners", "cluster": 12, "x": 6.891287803649902, "y": 2.8514814376831055}, {"id": 45338089, "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs", "cluster": 13, "x": 6.692231178283691, "y": 2.940526247024536}, {"id": 45338015, "title": "Vogte: Agentic TUI for Go projects with LLM integration", "cluster": 13, "x": 6.934895038604736, "y": 3.287937641143799}, {"id": 45337201, "title": "Model literals, semantic aliases, and preference-aligned routing for LLMs", "cluster": 13, "x": 6.859450340270996, "y": 3.3233165740966797}, {"id": 45336827, "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "cluster": 13, "x": 6.8405866622924805, "y": 3.660506248474121}, {"id": 45332448, "title": "I Don't Want to Code with LLM's", "cluster": 13, "x": 6.582733631134033, "y": 3.360400915145874}, {"id": 45332414, "title": "Agentic Feature Flags", "cluster": 38, "x": 8.57065486907959, "y": 3.6720283031463623}, {"id": 45331456, "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures", "cluster": 208, "x": 8.082098960876465, "y": 4.300482273101807}, {"id": 45331286, "title": "Reverse engineering a Cat Printer with the help of an LLM", "cluster": 13, "x": 6.697605133056641, "y": 3.4033427238464355}, {"id": 45331034, "title": "RadarKit AI \u2013 Get Your Brand Mentioned by LLMs", "cluster": 12, "x": 7.009120941162109, "y": 2.694753408432007}, {"id": 45330561, "title": "A prompt to turn any LLM into a professional travel planner", "cluster": 13, "x": 6.7157135009765625, "y": 3.2132513523101807}, {"id": 45328702, "title": "Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search", "cluster": 208, "x": 8.260802268981934, "y": 4.505826473236084}, {"id": 45327964, "title": "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof", "cluster": 13, "x": 6.650905609130859, "y": 3.2385306358337402}, {"id": 45325956, "title": "LLM Routing Strategies", "cluster": 13, "x": 6.776534557342529, "y": 3.3456966876983643}, {"id": 45325102, "title": "JuicyLinks \u2013 LLO \u2013 SEO for LLM's", "cluster": 13, "x": 6.733048439025879, "y": 3.501394271850586}, {"id": 45325208, "title": "Transforming recursion into iteration for LLVM loop optimizations", "cluster": 13, "x": 7.1563825607299805, "y": 3.48098087310791}, {"id": 45325054, "title": "SciGPT: A LLM for Scientific Literature Understanding and Knowledge Discovery", "cluster": 13, "x": 7.0941948890686035, "y": 3.536825656890869}, {"id": 45323367, "title": "Turn Any LLM into a Powerful Assistant (All About Tools)", "cluster": 13, "x": 6.7321953773498535, "y": 3.4689550399780273}, {"id": 45323014, "title": "I Use LLMs to Write the Majority of My Code", "cluster": 13, "x": 6.666757106781006, "y": 3.6623733043670654}, {"id": 45322535, "title": "AutoCodeBench: Large Language Models Are Automatic Code Benchmark Generators", "cluster": 208, "x": 8.148615837097168, "y": 4.566619873046875}, {"id": 45321983, "title": "LLMs are still surprisingly bad at some simple tasks", "cluster": 13, "x": 6.579164028167725, "y": 3.1291866302490234}, {"id": 45321165, "title": "Hacking with AI SASTs: An Overview of 'AI Security Engineers' / 'LLM Security S", "cluster": 3, "x": 6.986137390136719, "y": 2.6259655952453613}, {"id": 45319872, "title": "Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs", "cluster": 13, "x": 6.754181385040283, "y": 3.2115166187286377}, {"id": 45319584, "title": "The iPhone 17 Pro can run LLMs fast", "cluster": 13, "x": 7.001789569854736, "y": 3.750624418258667}, {"id": 45318131, "title": "Checklist for effective LLM prompt caching", "cluster": 13, "x": 6.839563369750977, "y": 3.5046629905700684}, {"id": 45317602, "title": "Python 2 LLMs: Self-Study", "cluster": 13, "x": 6.874882698059082, "y": 3.3293750286102295}, {"id": 45315746, "title": "The LLM Lobotomy?", "cluster": 13, "x": 6.608803749084473, "y": 3.0882272720336914}, {"id": 45314993, "title": "How to Train an LLM-RecSys Hybrid for Steerable Recs with Semantic IDs", "cluster": 13, "x": 7.100997447967529, "y": 3.6084036827087402}, {"id": 45314838, "title": "Google AI explains why LLMs are deceptive", "cluster": 12, "x": 6.969723701477051, "y": 2.6254031658172607}, {"id": 45313790, "title": "Compiling a Functional Language to LLVM (2023)", "cluster": 13, "x": 7.045643329620361, "y": 3.8842897415161133}, {"id": 45313177, "title": "Large Language Models Meet Joint Embedding Predictive Architectures", "cluster": 208, "x": 8.146271705627441, "y": 4.374685287475586}, {"id": 45312528, "title": "Web-codegen-scorer: evaluating the quality of web code generated by LLMs", "cluster": 13, "x": 6.686248302459717, "y": 3.6436774730682373}, {"id": 45311748, "title": "Checklist for LLM Prompt Caching", "cluster": 13, "x": 6.833172798156738, "y": 3.5500423908233643}, {"id": 45311639, "title": "The Vectorization-Planner (VPlan) in LLVM", "cluster": 13, "x": 7.02380895614624, "y": 3.73763370513916}, {"id": 45311115, "title": "LLM-Deflate: Extracting LLMs into Datasets", "cluster": 13, "x": 7.008296489715576, "y": 3.5333824157714844}, {"id": 45309640, "title": "I build a tools to calculate how much VRAM is needed to run LLMs", "cluster": 13, "x": 6.931232452392578, "y": 3.6820623874664307}, {"id": 45307237, "title": "Ollama Cloud Models", "cluster": 13, "x": 7.050354480743408, "y": 3.802194833755493}, {"id": 45304980, "title": "Markov chains are the original language models", "cluster": 208, "x": 8.343981742858887, "y": 4.469578742980957}, {"id": 45304928, "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage?", "cluster": 13, "x": 6.773444175720215, "y": 3.004251003265381}, {"id": 45304255, "title": "Making LLMs more accurate by using all of their layers", "cluster": 13, "x": 6.960824489593506, "y": 3.389345645904541}, {"id": 45302345, "title": "Why Do LLMs Design Mediocre Architecture?", "cluster": 13, "x": 6.54569149017334, "y": 3.193986415863037}, {"id": 45302119, "title": "VCBench: Benchmarking LLMs in Venture Capital", "cluster": 13, "x": 6.930726051330566, "y": 3.327584743499756}, {"id": 45301989, "title": "RCE, SQLi and More in Logistics Software E-TMS", "cluster": 13, "x": 6.915729999542236, "y": 3.7683563232421875}, {"id": 45302041, "title": "Internalizing Self-Consistency in LMs: Multi-Agent Consensus Alignment", "cluster": 13, "x": 6.977486610412598, "y": 3.216618537902832}, {"id": 45301040, "title": "Emotional Intelligence Leaderboard for LLMs", "cluster": 13, "x": 6.86533260345459, "y": 3.1001288890838623}, {"id": 45300793, "title": "LLMs Hide Complexity", "cluster": 13, "x": 6.769717693328857, "y": 3.2823386192321777}, {"id": 45300668, "title": "25L Portable NV-linked Dual 3090 LLM Rig", "cluster": 13, "x": 7.058285713195801, "y": 3.843661308288574}, {"id": 45300741, "title": "iTerm2 (a terminal emulator) is now bundled with LLM/AI", "cluster": 12, "x": 7.055046558380127, "y": 2.6653265953063965}, {"id": 45300652, "title": "If LLMs were excellent at coding that would be horrifying reality?", "cluster": 13, "x": 6.4856085777282715, "y": 3.6988914012908936}, {"id": 45299850, "title": "Shakespeare \u2013 an LLM tool to build Nostr apps", "cluster": 13, "x": 6.844679832458496, "y": 3.612534761428833}, {"id": 45298355, "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "cluster": 13, "x": 7.038005828857422, "y": 3.294365644454956}, {"id": 45296403, "title": "Llama-Factory: Unified, Efficient Fine-Tuning for 100 Open LLMs", "cluster": 13, "x": 6.970882415771484, "y": 3.5906100273132324}, {"id": 45294193, "title": "Basis Theory Forms Consortium, Publishes White Paper on Agentic Commerce", "cluster": 36, "x": 8.55260944366455, "y": 3.640697479248047}, {"id": 45291808, "title": "What I learned building a programming language with LLM agents", "cluster": 13, "x": 6.8047871589660645, "y": 3.5448334217071533}, {"id": 45291121, "title": "Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees", "cluster": 13, "x": 6.994461536407471, "y": 3.3997204303741455}, {"id": 45290956, "title": "LLM misalignment may stem from role inference, not corrupted weights", "cluster": 13, "x": 6.748050689697266, "y": 3.045494556427002}, {"id": 45289216, "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "cluster": 13, "x": 7.053415775299072, "y": 3.261051654815674}, {"id": 45288125, "title": "Translating ePub Ebooks with Local LLM and for Free. An Effective Approach", "cluster": 13, "x": 6.826044082641602, "y": 3.5689828395843506}, {"id": 45285029, "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "cluster": 13, "x": 7.038058280944824, "y": 3.279628276824951}, {"id": 45284482, "title": "Making LLMs more accurate by using all of their layers", "cluster": 13, "x": 6.924880504608154, "y": 3.3786826133728027}, {"id": 45283637, "title": "Learning Languages with the Help of Algorithms", "cluster": 208, "x": 8.289545059204102, "y": 4.575099468231201}, {"id": 45282380, "title": "Rectified Linear Units Improve Restricted Boltzmann Machines (2010) [pdf]", "cluster": 13, "x": 7.1534423828125, "y": 3.532172918319702}, {"id": 45281804, "title": "LLMs Are Non-Deterministic", "cluster": 13, "x": 6.744982719421387, "y": 3.1669201850891113}, {"id": 45280161, "title": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning", "cluster": 13, "x": 7.046652317047119, "y": 3.261068105697632}, {"id": 45280016, "title": "Delphi-2M LLM uses medical records, lifestyle to provide risks for 1k+ diseases", "cluster": 13, "x": 6.761904239654541, "y": 3.3758749961853027}, {"id": 45279656, "title": "LLMs can't solve production issues", "cluster": 13, "x": 6.555229663848877, "y": 3.236687421798706}, {"id": 45278469, "title": "Proposed LLM Optimization Metric: Minimum ticks to get to a certain thought", "cluster": 13, "x": 7.0646891593933105, "y": 3.349660873413086}, {"id": 45276978, "title": "Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20", "cluster": 13, "x": 7.165942192077637, "y": 3.7265756130218506}, {"id": 45277100, "title": "Distributed Training of LLM's: A Survey", "cluster": 13, "x": 6.734024524688721, "y": 3.2077999114990234}, {"id": 45276585, "title": "We Cut Support Tickets by Rewriting Error Messages for LLMs", "cluster": 13, "x": 6.537975311279297, "y": 3.2426185607910156}, {"id": 45276287, "title": "How Do LLMs Work?", "cluster": 13, "x": 6.7346720695495605, "y": 3.274378776550293}, {"id": 45274884, "title": "Shorlabs \u2013 A GTM tool that is easy to use unlike Apollo and Clay", "cluster": 13, "x": 7.007950782775879, "y": 3.9378371238708496}, {"id": 45273802, "title": "LLMs and Beyond: All Roads Lead to Latent Space", "cluster": 13, "x": 6.660122871398926, "y": 3.1033997535705566}, {"id": 45273033, "title": "Codegen Scorer \u2013 evaluate the quality of code generated by LLMs", "cluster": 13, "x": 6.733966827392578, "y": 3.651886224746704}, {"id": 45271717, "title": "Adyen to Collaborate with Google on Agent Payments Protocol", "cluster": 35, "x": 8.449990272521973, "y": 3.846478223800659}, {"id": 45271324, "title": "How to Train an LLM-RecSys Hybrid for Steerable Recs", "cluster": 13, "x": 7.031274795532227, "y": 3.485081672668457}, {"id": 45270844, "title": "A Survey on Retrieval and Structuring Augmented Generation with LLMs", "cluster": 13, "x": 7.022186279296875, "y": 3.499764919281006}, {"id": 45269556, "title": "LLM misalignment may stem from role inference, not corrupted weights", "cluster": 13, "x": 6.756097793579102, "y": 3.0120363235473633}, {"id": 45268813, "title": "Agent Payments Protocol (AP2)", "cluster": 36, "x": 8.524169921875, "y": 3.566871166229248}, {"id": 45268677, "title": "Machine Scheduler in LLVM \u2013 Part I", "cluster": 13, "x": 7.069849491119385, "y": 3.7871508598327637}, {"id": 45268671, "title": "Stop fine-tuning LLMs for docs, use RAG", "cluster": 13, "x": 6.615043640136719, "y": 3.2862353324890137}, {"id": 45265999, "title": "Building a better LLM augmented Search Engine (Blog)", "cluster": 13, "x": 6.849890232086182, "y": 3.535325527191162}, {"id": 45265812, "title": "Reducing Cold Start Latency for LLM Inference with NVIDIA Run:AI Model Streamer", "cluster": 12, "x": 7.1784772872924805, "y": 2.9521737098693848}, {"id": 45263372, "title": "MAIstro \u2013 multi-agent framework for medical imaging workflows", "cluster": 3, "x": 8.664459228515625, "y": 3.8005595207214355}, {"id": 45262877, "title": "ROCm 7.0", "cluster": 13, "x": 7.16414737701416, "y": 4.019851207733154}, {"id": 45262502, "title": "LLM Inference Providers \u2013 Recommendations?", "cluster": 13, "x": 6.873795986175537, "y": 3.2343175411224365}, {"id": 45259950, "title": "How Do LLMs Work?", "cluster": 13, "x": 6.741581439971924, "y": 3.264554738998413}, {"id": 45259190, "title": "Machine Scheduler in LLVM", "cluster": 13, "x": 6.978280544281006, "y": 3.7472331523895264}, {"id": 45257464, "title": "Google releases VaultGemma, its first privacy-preserving LLM", "cluster": 13, "x": 6.535412311553955, "y": 3.5430667400360107}, {"id": 45251958, "title": "Emergent Hierarchical Reasoning in LLMs Through Reinforcement Learning", "cluster": 13, "x": 6.915846347808838, "y": 3.17364501953125}, {"id": 45250643, "title": "Local LLMs Directory [with VRAM Calculator]", "cluster": 13, "x": 6.98789119720459, "y": 3.7645795345306396}, {"id": 45250546, "title": "IBM's watsonx explores using LLMs to judge other LLMs [video]", "cluster": 7, "x": 6.794376850128174, "y": 3.0702335834503174}, {"id": 45250358, "title": "Trying to Play \"Guess Who\" with an LLM", "cluster": 13, "x": 6.6502885818481445, "y": 3.1932711601257324}, {"id": 45249936, "title": "Clear Thinking beats deep thinking in LLM models", "cluster": 13, "x": 6.7668657302856445, "y": 3.088925361633301}, {"id": 45247990, "title": "LLM rerankers for production RAG: tips and tricks", "cluster": 13, "x": 6.782110214233398, "y": 3.355029582977295}, {"id": 45245948, "title": "Language models pack billions of concepts into 12k dimensions", "cluster": 208, "x": 8.271130561828613, "y": 4.39396858215332}, {"id": 45245150, "title": "So You Want to Host Your Own LLM? Don't", "cluster": 13, "x": 6.5721211433410645, "y": 3.3635268211364746}, {"id": 45244293, "title": "Python, Deep Learning, and LLMs: A Crash Course for Complete Beginners", "cluster": 13, "x": 6.919332027435303, "y": 3.4103894233703613}, {"id": 45243414, "title": "LLM Rerankers for RAG: A Practical Guide", "cluster": 13, "x": 6.830679416656494, "y": 3.330578565597534}, {"id": 45243221, "title": "Should LLMs Write FOSS Books?", "cluster": 13, "x": 6.582226276397705, "y": 3.5555429458618164}, {"id": 45243084, "title": "Experimental platform using LLMs to generate algorithmic music", "cluster": 13, "x": 6.973913192749023, "y": 3.5715737342834473}, {"id": 45241249, "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs", "cluster": 13, "x": 6.6946940422058105, "y": 2.9452860355377197}, {"id": 45241049, "title": "Interactive Latent Flow Visualisation for Any LLM", "cluster": 13, "x": 7.005323886871338, "y": 3.5826680660247803}, {"id": 45240859, "title": "LLMs Don't Know Their Own Decision Boundaries", "cluster": 13, "x": 6.580999851226807, "y": 3.106767177581787}, {"id": 45240847, "title": "ButterflyQuant: Ultra-low-bit LLM Quantization", "cluster": 13, "x": 7.101380348205566, "y": 3.725895881652832}, {"id": 45240289, "title": "The second wave of MCP: Building for LLMs, not developers", "cluster": 13, "x": 6.601780891418457, "y": 3.3462133407592773}, {"id": 45238661, "title": "\"Dear ImGui\" Becomes \"Dear User, \" with LLMs", "cluster": 13, "x": 6.80681037902832, "y": 3.629565715789795}, {"id": 45238254, "title": "Why Most LLM Chatbots Never Make It to Production", "cluster": 13, "x": 6.560566425323486, "y": 3.27146053314209}, {"id": 45237754, "title": "SpikingBrain 7B \u2013 More efficient than classic LLMs", "cluster": 13, "x": 6.853638648986816, "y": 3.292565107345581}, {"id": 45235422, "title": "B122M Faeb", "cluster": 13, "x": 7.361863613128662, "y": 3.905897617340088}, {"id": 45234978, "title": "Diffusion based LLM basic chat app", "cluster": 13, "x": 6.719167232513428, "y": 3.548297882080078}, {"id": 45235119, "title": "Instruction-Following Pruning for Large Language Models", "cluster": 208, "x": 8.193892478942871, "y": 4.5539727210998535}, {"id": 45234441, "title": "Inside vLLM: Anatomy of a High-Throughput LLM Inference System", "cluster": 13, "x": 7.047310829162598, "y": 3.4550342559814453}, {"id": 45234156, "title": "Deterministic LLM", "cluster": 13, "x": 6.801478862762451, "y": 3.289672374725342}, {"id": 45233809, "title": "The SWE-Bench Illusion: When LLMs Remember Instead of Reason", "cluster": 13, "x": 6.721692085266113, "y": 3.029736280441284}, {"id": 45231911, "title": "Optimization Pathways for Long-Context Agentic LLM Inference", "cluster": 13, "x": 7.085625648498535, "y": 3.2895257472991943}, {"id": 45231500, "title": "AI tool detects LLM-generated text in research papers and peer reviews", "cluster": 12, "x": 7.030786991119385, "y": 2.747983932495117}, {"id": 45230332, "title": "Regaining control of my attention using large language models", "cluster": 208, "x": 8.196234703063965, "y": 4.37929630279541}, {"id": 45229056, "title": "Scalable LLM approach to enhancing chatbot knowledge with user-generated content", "cluster": 13, "x": 6.8276448249816895, "y": 3.5071120262145996}, {"id": 45228682, "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs", "cluster": 13, "x": 6.674882888793945, "y": 2.8998355865478516}, {"id": 45226375, "title": "Interactive LLM Chat", "cluster": 13, "x": 6.657981872558594, "y": 3.498211145401001}, {"id": 45225481, "title": "Inside vLLM: Anatomy of a High-Throughput LLM Inference System", "cluster": 13, "x": 7.03223991394043, "y": 3.4543981552124023}, {"id": 45224625, "title": "Choosing Rust for LLM-generated code", "cluster": 13, "x": 6.847920894622803, "y": 3.9448883533477783}, {"id": 45224584, "title": "LLM Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation", "cluster": 13, "x": 6.579639911651611, "y": 3.524080753326416}, {"id": 45224482, "title": "LLMs as Retrieval and Recommendation Engines", "cluster": 13, "x": 6.81810998916626, "y": 3.424156904220581}, {"id": 45224357, "title": "Babies and LLMs", "cluster": 13, "x": 6.593664646148682, "y": 3.164153814315796}, {"id": 45224126, "title": "The second wave of MCP: Building for LLMs, not developers", "cluster": 13, "x": 6.566626071929932, "y": 3.3257622718811035}, {"id": 45223677, "title": "GraalVM's LLVM Back End", "cluster": 13, "x": 6.890557289123535, "y": 3.714845895767212}, {"id": 45223726, "title": "VaultGemma: The most capable differentially private LLM", "cluster": 13, "x": 6.536732196807861, "y": 3.423412561416626}, {"id": 45222786, "title": "Blurring interfaces that redirects user's attention with LLM", "cluster": 13, "x": 6.688360691070557, "y": 3.486294746398926}, {"id": 45222339, "title": "Analog In-Memory Computing Attention Mechanism for Fast LLMs", "cluster": 13, "x": 6.9047441482543945, "y": 3.4454185962677}, {"id": 45221639, "title": "Why Most LLM Chatbots Never Make It to Production", "cluster": 13, "x": 6.587475299835205, "y": 3.263772964477539}, {"id": 45221436, "title": "Building multi-agent tools for engineering", "cluster": 38, "x": 8.681868553161621, "y": 3.632395029067993}, {"id": 45221266, "title": "Analog in-memory computing attention mechanism fast and energy-efficient LLMs", "cluster": 13, "x": 6.928882598876953, "y": 3.5616872310638428}, {"id": 45221208, "title": "Clever Hans Couldn't Do Arithmetic, and LLMs Don't Understand", "cluster": 13, "x": 6.664161682128906, "y": 3.090181827545166}, {"id": 45221252, "title": "LLM-Generated Rules Engines for LLM Explainability", "cluster": 13, "x": 6.865130424499512, "y": 3.425199270248413}, {"id": 45221007, "title": "Removing 95% of podcast ads with transcript segmentation and LLMs", "cluster": 13, "x": 6.65625, "y": 3.4363772869110107}, {"id": 45220438, "title": "Persuader: LLM >> Schema Conformity", "cluster": 13, "x": 6.749215602874756, "y": 3.3226757049560547}, {"id": 45219159, "title": "Why Large Language Models (LLMs) Will Not Understand Human Language (2022)", "cluster": 13, "x": 7.17513370513916, "y": 3.796386957168579}, {"id": 45218582, "title": "Bot Hop \u2013 Get a Random LLM", "cluster": 13, "x": 6.629220962524414, "y": 3.1812942028045654}, {"id": 45217969, "title": "LLM-optimizer: Benchmark and optimize LLM inference across frameworks with ease", "cluster": 13, "x": 7.183593273162842, "y": 3.3519060611724854}, {"id": 45217301, "title": "Speculative cascades \u2013 A hybrid approach for smarter, faster LLM inference", "cluster": 13, "x": 7.018155097961426, "y": 3.274744749069214}, {"id": 45216968, "title": "Evaluating and Optimizing LLM Applications with DSPy", "cluster": 13, "x": 6.913164138793945, "y": 3.4865200519561768}, {"id": 45216345, "title": "Automated Bug Triaging Using Instruction-Tuned Large Language Models", "cluster": 208, "x": 8.143901824951172, "y": 4.60166072845459}, {"id": 45215755, "title": "Writing effective tools for LLM agents\u2013using LLM agents", "cluster": 13, "x": 6.854822635650635, "y": 3.3124842643737793}, {"id": 45215929, "title": "Matrix is email wearing a hoodie", "cluster": 75, "x": 8.288228988647461, "y": 3.9253451824188232}, {"id": 45215321, "title": "LLM Benchmark and Optimization Explorer", "cluster": 13, "x": 6.98697566986084, "y": 3.5061278343200684}, {"id": 45215311, "title": "AI tool detects LLM-generated text in research papers and peer reviews", "cluster": 12, "x": 7.021485805511475, "y": 2.7597832679748535}, {"id": 45213702, "title": "Trends in LLM-Generated Citations on ArXiv", "cluster": 13, "x": 6.829671382904053, "y": 3.2667675018310547}, {"id": 45213240, "title": "LLMs can execute complete ransomware attacks autonomously, research shows", "cluster": 13, "x": 6.43806266784668, "y": 3.3910326957702637}, {"id": 45212750, "title": "LFBrownNoise, a stochastic noise generator in SuperCollider", "cluster": 13, "x": 7.312652587890625, "y": 3.5503005981445312}, {"id": 45212783, "title": "Semlib: LLM-Powered Data Processing", "cluster": 13, "x": 6.98775053024292, "y": 3.5430939197540283}, {"id": 45212510, "title": "Real-time benchmarking of LLM capabilities", "cluster": 13, "x": 6.989090919494629, "y": 3.3739707469940186}, {"id": 45212260, "title": "Network and Storage Benchmarks for LLM Training on the Cloud", "cluster": 13, "x": 6.921238899230957, "y": 3.4830660820007324}, {"id": 45211456, "title": "Automated daily Quordle solver using an LLM", "cluster": 13, "x": 6.896774768829346, "y": 3.37446665763855}, {"id": 45210805, "title": "Verifying LLM Output, Sorta, Kinda", "cluster": 13, "x": 6.764505863189697, "y": 3.324518918991089}, {"id": 45209626, "title": "All You Need Is MCP \u2013 LLMs Solving a DEF Con CTF Finals Challenge", "cluster": 13, "x": 6.76785135269165, "y": 3.287019968032837}, {"id": 45209398, "title": "Creating larger projects with LLM (as a coder)", "cluster": 13, "x": 6.715325832366943, "y": 3.6542038917541504}, {"id": 45208563, "title": "LLM with Tree Search learns in context to solve hard problems", "cluster": 13, "x": 6.975747585296631, "y": 3.3838069438934326}, {"id": 45208446, "title": "Ripple: An LLVM compiler-interpreted API to support SPMD and loop annotat", "cluster": 13, "x": 6.966297149658203, "y": 3.881364583969116}, {"id": 45208113, "title": "Steve Yegge on writing a quarter million lines of code in six days using LLMs", "cluster": 13, "x": 6.715604305267334, "y": 3.7022705078125}, {"id": 45208141, "title": "You can change an LLM's favorite color with a Steering Vector", "cluster": 13, "x": 6.615045547485352, "y": 3.1228907108306885}, {"id": 45206014, "title": "XML Prompting Revolution: Math Proofs for Guaranteed LLM Stability", "cluster": 13, "x": 6.912519454956055, "y": 3.477712392807007}, {"id": 45205500, "title": "Checkpoint-engine: A middleware to update model weights in LLM inference engines", "cluster": 13, "x": 7.099879264831543, "y": 3.618325710296631}, {"id": 45202511, "title": "LLM Latency Leaderboard", "cluster": 13, "x": 6.964583873748779, "y": 3.3974034786224365}, {"id": 45202095, "title": "Can LLMs replace on call SREs today?", "cluster": 13, "x": 6.6130170822143555, "y": 3.324687957763672}, {"id": 45201855, "title": "Is the LLM response wrong, or have you just failed to iterate it?", "cluster": 13, "x": 6.579632759094238, "y": 3.2273318767547607}, {"id": 45201545, "title": "Gap between commercial and open-source LLMs for Olympiad-level math is shrinking", "cluster": 13, "x": 6.896139144897461, "y": 3.482431173324585}, {"id": 45200925, "title": "Defeating Nondeterminism in LLM Inference", "cluster": 13, "x": 6.81259298324585, "y": 3.125413656234741}, {"id": 45200414, "title": "When Native LLM Web APIs (window.ai)?", "cluster": 12, "x": 7.028560638427734, "y": 2.7088706493377686}, {"id": 45199768, "title": "VLLM: Anatomy of a High-Throughput LLM Inference System", "cluster": 13, "x": 7.077175617218018, "y": 3.45232892036438}, {"id": 45197691, "title": "Aulico \u2013 Wrapping LLMs around crypto and stock markets", "cluster": 13, "x": 6.775512218475342, "y": 3.150043487548828}, {"id": 45197132, "title": "Breaking Android with AI: A Deep Dive into LLM-Powered Exploitation", "cluster": 12, "x": 6.990516662597656, "y": 2.623011827468872}, {"id": 45196204, "title": "LLM Chat Scraper \u2013 AI Share URL Extractor", "cluster": 12, "x": 7.046097278594971, "y": 2.6708133220672607}, {"id": 45194766, "title": "Anyone else hacking on long-horizon reasoning frameworks for LLMs?", "cluster": 13, "x": 6.8702216148376465, "y": 3.3028008937835693}, {"id": 45192655, "title": "I replaced Animal Crossing's dialogue with a live LLM by hacking GameCube memory", "cluster": 13, "x": 6.822238445281982, "y": 3.7918848991394043}, {"id": 45192194, "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data", "cluster": 13, "x": 6.929524898529053, "y": 3.3112010955810547}, {"id": 45192000, "title": "Introducing BackendBench: how well LLMs and humans can write PyTorch backends", "cluster": 13, "x": 6.8666510581970215, "y": 3.690549373626709}, {"id": 45189763, "title": "CLI-based multi-agent trading system using LLMs", "cluster": 13, "x": 6.887703895568848, "y": 3.224447727203369}, {"id": 45185502, "title": "Tiny LLM \u2013 LLM Serving in a Week", "cluster": 13, "x": 6.721278190612793, "y": 3.200343370437622}, {"id": 45185056, "title": "Faith in God-like large language models is waning", "cluster": 208, "x": 8.186694145202637, "y": 4.400024890899658}, {"id": 45183230, "title": "I still prefer ems over rems", "cluster": 13, "x": 6.630462169647217, "y": 3.0555951595306396}, {"id": 45182298, "title": "Two Words Broke My LLM Chat Agent", "cluster": 13, "x": 6.524453639984131, "y": 3.127251148223877}, {"id": 45181803, "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning", "cluster": 208, "x": 8.41811752319336, "y": 4.359650611877441}, {"id": 45181478, "title": "New Data Science LLM Benchmark", "cluster": 13, "x": 6.986842632293701, "y": 3.419539213180542}, {"id": 45181078, "title": "Building predictive agents (RFM + MCP)", "cluster": 13, "x": 7.080403804779053, "y": 3.3319895267486572}, {"id": 45179163, "title": "Outcome-Based Exploration for LLM Reasoning", "cluster": 13, "x": 6.957785129547119, "y": 3.1751508712768555}, {"id": 45178117, "title": "Compiling a Functional Language to LLVM", "cluster": 13, "x": 6.908673286437988, "y": 3.7870054244995117}, {"id": 45177507, "title": "Byte Type: Supporting Raw Data Copies in the LLVM IR", "cluster": 13, "x": 7.018749237060547, "y": 3.8035693168640137}, {"id": 45177061, "title": "The LLM models the user, and then it models itself", "cluster": 13, "x": 6.85113525390625, "y": 3.3384077548980713}, {"id": 45176850, "title": "Tiny LLM \u2013 LLM Serving in a Week", "cluster": 13, "x": 6.721088409423828, "y": 3.200685739517212}, {"id": 45176139, "title": "Master Foo and LLM Mountain", "cluster": 13, "x": 6.6164469718933105, "y": 3.2169015407562256}, {"id": 45175202, "title": "LLMs wrapped around stocks and crypto", "cluster": 13, "x": 6.739511489868164, "y": 3.1435654163360596}, {"id": 45175110, "title": "Tiny LLM \u2013 LLM Serving in a Week", "cluster": 13, "x": 6.741039752960205, "y": 3.2286622524261475}, {"id": 45173633, "title": "LWMalloc is a lightweight dynamic memory allocator for embedded systems", "cluster": 13, "x": 7.024255275726318, "y": 3.817995071411133}, {"id": 45173423, "title": "GSoC 2025 \u2013 Byte Type: Supporting Raw Data Copies in the LLVM IR", "cluster": 13, "x": 7.059564113616943, "y": 3.8712854385375977}, {"id": 45172376, "title": "Setting up local LLMs for R and Python", "cluster": 13, "x": 6.8886308670043945, "y": 3.718834638595581}, {"id": 45172469, "title": "Faith in God-like large language models is waning", "cluster": 208, "x": 8.19335651397705, "y": 4.41416597366333}, {"id": 45171440, "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache", "cluster": 13, "x": 7.151464462280273, "y": 3.663590908050537}, {"id": 45171225, "title": "Teaching a small embedding model with LLMs to deliver GPT-like semantics in 10ms", "cluster": 13, "x": 6.981869697570801, "y": 3.568655490875244}, {"id": 45170693, "title": "Tiny LLM \u2013 LLM Serving in a Week", "cluster": 13, "x": 6.716660976409912, "y": 3.197146415710449}, {"id": 45170571, "title": "Unicode variation selectors for invisible LLM injection", "cluster": 13, "x": 6.62718391418457, "y": 3.6292014122009277}, {"id": 45170078, "title": "Distance-Based Compression Method for Large Language Models", "cluster": 208, "x": 8.192109107971191, "y": 4.496007919311523}, {"id": 45169914, "title": "LLM-assisted scientific breakthrough probably isn't real", "cluster": 13, "x": 6.576756000518799, "y": 3.0117790699005127}, {"id": 45169297, "title": "A Comprehensive Survey on Trustworthiness in Reasoning with LLMs", "cluster": 13, "x": 6.883245944976807, "y": 3.1666736602783203}, {"id": 45169094, "title": "Stop chatting Constrained VS Unconstrained LLM use cases", "cluster": 13, "x": 6.589973449707031, "y": 3.2702977657318115}, {"id": 45168953, "title": "Experimenting with Local LLMs on macOS", "cluster": 13, "x": 6.87445068359375, "y": 3.7687175273895264}, {"id": 45167983, "title": "The LLM Has Left the Chat: Evidence of Bail Preferences in LLMs", "cluster": 13, "x": 6.537978172302246, "y": 3.051182746887207}, {"id": 45165536, "title": "How to Hack Transformers: Steering LLMs via Prompts, States, and Weight Edits", "cluster": 13, "x": 6.84891414642334, "y": 3.7160794734954834}, {"id": 45164390, "title": "Build / Deploy Agent Workflows", "cluster": 38, "x": 8.609562873840332, "y": 3.7394368648529053}, {"id": 45162889, "title": "We're training LLMs to hallucinate by rewarding them for guessing", "cluster": 13, "x": 6.6962361335754395, "y": 3.022003173828125}, {"id": 45161815, "title": "Chat2Data: A Flask App for Conversational Data Analysis Using LLMs", "cluster": 13, "x": 6.754111289978027, "y": 3.674379348754883}, {"id": 45161940, "title": "Cargo-remark: view LLVM optimization remarks for Rust", "cluster": 13, "x": 7.167542457580566, "y": 4.029962539672852}, {"id": 45161642, "title": "Tricking LLM-Based NPCs into Spilling Secrets", "cluster": 13, "x": 6.443166732788086, "y": 3.325939416885376}, {"id": 45161206, "title": "On LLMs and Quicksort", "cluster": 13, "x": 6.92103910446167, "y": 3.603914976119995}, {"id": 45159389, "title": "Do well-written, clear instructions beat few-shotting for tiny-LLMs?", "cluster": 13, "x": 6.699378490447998, "y": 3.2780919075012207}, {"id": 45158906, "title": "Promptware Attacks Against LLM-Powered Assistants in Production", "cluster": 13, "x": 6.563483715057373, "y": 3.342620849609375}, {"id": 45158769, "title": "Visual representations in the human brain are aligned with LLMs", "cluster": 13, "x": 6.797813892364502, "y": 3.267312526702881}, {"id": 45158361, "title": "Using domain inspired ML for embedded DSP", "cluster": 13, "x": 7.110877513885498, "y": 3.64673113822937}, {"id": 45157748, "title": "ChatGPT is NOT a LLM \u2013 GPT is", "cluster": 13, "x": 6.624176025390625, "y": 3.461358070373535}, {"id": 45156471, "title": "Visualizing the Vocabulary of an LLM", "cluster": 13, "x": 6.78652811050415, "y": 3.2232666015625}, {"id": 45153330, "title": "LLMs and Quantum Measurement: An Analogy", "cluster": 13, "x": 6.662914752960205, "y": 3.112600326538086}, {"id": 45150919, "title": "Anti AI/LLM scraper bot firewall", "cluster": 12, "x": 7.035526752471924, "y": 2.6430115699768066}, {"id": 45150807, "title": "LLMs Are Adaptive Data Organisms", "cluster": 13, "x": 6.898624420166016, "y": 3.458550453186035}, {"id": 45149585, "title": "Psychological tricks can get LLMs to respond to \"forbidden\" prompts", "cluster": 13, "x": 6.581118583679199, "y": 3.123746633529663}, {"id": 45148984, "title": "Managing NPM Releases", "cluster": 406, "x": 7.0703959465026855, "y": 4.068409442901611}, {"id": 45148180, "title": "A Software Development Methodology for Disciplined LLM Collaboration", "cluster": 13, "x": 6.690934181213379, "y": 3.570127248764038}, {"id": 45147333, "title": "LLMs, Quantum Measurement, and a Primitive of Consciousness", "cluster": 13, "x": 6.693103790283203, "y": 3.049196720123291}, {"id": 45147303, "title": "Gap between commercial and open-source LLMs for Olympiad-level math is shrinking", "cluster": 13, "x": 6.9093098640441895, "y": 3.5098471641540527}, {"id": 45143704, "title": "High-level visual representations in the human brain are aligned with LLMs", "cluster": 13, "x": 6.788418769836426, "y": 3.2485780715942383}, {"id": 45143275, "title": "When LLMs Grow Hands and Feet, How to Design Our Agentic RL Systems?", "cluster": 13, "x": 6.856348037719727, "y": 3.3471829891204834}, {"id": 45142597, "title": "LLM as Pair?", "cluster": 13, "x": 6.715158939361572, "y": 3.225956439971924}, {"id": 45142708, "title": "Let's rename the \"vibecoding\" tag to \"LLMs\"", "cluster": 13, "x": 6.624588966369629, "y": 3.47232985496521}, {"id": 45142184, "title": "Context Engineering: Rapid Agent Prototyping \u2013 Jason Liu", "cluster": 38, "x": 8.682079315185547, "y": 3.637218952178955}, {"id": 45141995, "title": "Are LLMs better suited for PR reviews than full codebases?", "cluster": 13, "x": 6.77747917175293, "y": 3.6360929012298584}, {"id": 45141863, "title": "MinionS Protocol \u2013 Cost-Efficient Local-Remote LLM Collaboration", "cluster": 13, "x": 6.7991790771484375, "y": 3.603999137878418}, {"id": 45141031, "title": "A/B testing and LLM personalization on Cloudflare Workers using Visual Editor", "cluster": 13, "x": 6.85189962387085, "y": 3.6699163913726807}, {"id": 45140474, "title": "From DBA to DB Agents", "cluster": 38, "x": 8.679309844970703, "y": 3.5369503498077393}, {"id": 45140166, "title": "LLM Evaluation via Rap Battles", "cluster": 13, "x": 6.8302903175354, "y": 3.3033335208892822}, {"id": 45140063, "title": "Apertus, the most powerful open-source LLM released by a public institution", "cluster": 13, "x": 6.8282575607299805, "y": 3.6897106170654297}, {"id": 45139783, "title": "The gap between commercial and open-source LLMs for Olympiad math is shrinking", "cluster": 13, "x": 6.921377658843994, "y": 3.587566614151001}, {"id": 45139601, "title": "With LLMs, Drive Manually", "cluster": 13, "x": 6.807649612426758, "y": 3.615426778793335}, {"id": 45138330, "title": "NVIDIA Dynamo LLM Inference Framework", "cluster": 13, "x": 7.229028224945068, "y": 3.576545000076294}, {"id": 45137698, "title": "Sam Altman complains on LLM-run X accounts", "cluster": 13, "x": 6.523006916046143, "y": 3.140472412109375}, {"id": 45137373, "title": "ML needs a new programming language \u2013 Interview with Chris Lattner", "cluster": 13, "x": 7.420100688934326, "y": 4.570740699768066}, {"id": 45134887, "title": "Noto: Local journaling chat UI with per-handle memory (Flask, SQLite, LM Studio)", "cluster": 13, "x": 6.8146162033081055, "y": 3.8255622386932373}, {"id": 45134633, "title": "Is class imbalance a problem in machine learning?", "cluster": 208, "x": 8.574040412902832, "y": 4.104945182800293}, {"id": 45134381, "title": "JetKVM", "cluster": 13, "x": 7.202078819274902, "y": 4.024817943572998}, {"id": 45133999, "title": "LLMs encode theory-of-mind: a study on sparse parameter patterns", "cluster": 13, "x": 6.9370551109313965, "y": 3.2482807636260986}, {"id": 45131987, "title": "LLMs from Scratch Using Middle School Math \u2013 TDS Archive", "cluster": 13, "x": 6.9021077156066895, "y": 3.4161198139190674}, {"id": 45131301, "title": "vLLM with torch.compile: Efficient LLM inference on PyTorch", "cluster": 13, "x": 7.259279727935791, "y": 3.6090097427368164}, {"id": 45130966, "title": "TildeOpen-30B: European LLM Focused on Underrepresented Languages", "cluster": 13, "x": 6.841392993927002, "y": 3.479020833969116}, {"id": 45130681, "title": "MCP for LM Studio with Prompt Library and Custom Prompting", "cluster": 13, "x": 6.902557849884033, "y": 3.866384744644165}, {"id": 45130260, "title": "LLM Visualization", "cluster": 13, "x": 7.034188270568848, "y": 3.5919179916381836}, {"id": 45129677, "title": "LLM Social Simulations Are a Promising Research Method", "cluster": 13, "x": 6.796638011932373, "y": 3.203214406967163}, {"id": 45124998, "title": "Exploring LLMs for ICD Coding \u2013 Part 1", "cluster": 13, "x": 6.817746162414551, "y": 3.5953357219696045}, {"id": 45124994, "title": "Creating a Knowledge Graph for ICD Codes Using LLMs", "cluster": 13, "x": 6.900478363037109, "y": 3.5596072673797607}, {"id": 45124771, "title": "VLLM: Anatomy of a High-Throughput LLM Inference System", "cluster": 13, "x": 7.0528154373168945, "y": 3.491072654724121}, {"id": 45122885, "title": "A high schooler writes about AI tools in the classroom", "cluster": 3, "x": 7.929445266723633, "y": 3.8015940189361572}, {"id": 45121973, "title": "The Term \"Non-Deterministic\" and LLMs", "cluster": 13, "x": 6.671895980834961, "y": 3.148697853088379}, {"id": 45120513, "title": "Monolingual speech recognition models beat multilingual models ~30x bigger", "cluster": 208, "x": 8.094239234924316, "y": 4.350069999694824}, {"id": 45119403, "title": "Tesla's 4th 'Master Plan' reads like LLM-generated nonsense", "cluster": 13, "x": 6.710117340087891, "y": 3.0775516033172607}, {"id": 45119502, "title": "Indirect Prompt Injection Attacks Against LLM Assistants", "cluster": 13, "x": 6.491973876953125, "y": 3.3756661415100098}, {"id": 45119117, "title": "Large Language Models and Emergence: A Complex Systems Perspective", "cluster": 208, "x": 8.218276977539062, "y": 4.327372074127197}, {"id": 45117728, "title": "TLQ: A minimal message queue that just works", "cluster": 13, "x": 6.751120090484619, "y": 3.5897231101989746}, {"id": 45117200, "title": "Hidden Door at Launch: Design Review of an LLM-Driven Story Game", "cluster": 13, "x": 6.858619213104248, "y": 3.4122848510742188}, {"id": 45116898, "title": "Meta is adding free LLM-powered conversational NPCs to Horizon Worlds", "cluster": 13, "x": 6.795365810394287, "y": 3.6180527210235596}, {"id": 45116901, "title": "Real-time emotional detection via ChatGPT (LLM) and Brain-Computer interface", "cluster": 13, "x": 6.711835861206055, "y": 3.5134150981903076}, {"id": 45116693, "title": "Trust Technical Writers, Not LLMs", "cluster": 13, "x": 6.518792152404785, "y": 3.381157636642456}, {"id": 45116073, "title": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers", "cluster": 13, "x": 6.928701400756836, "y": 3.3465046882629395}, {"id": 45115245, "title": "Promptware Attacks Against LLM-Powered Assistants Are Practical and Dangerous", "cluster": 13, "x": 6.504388332366943, "y": 3.365891456604004}, {"id": 45114981, "title": "I created my first NPM node JavaScript module for the Auto llms.txt generation", "cluster": 13, "x": 6.817858695983887, "y": 3.7411797046661377}, {"id": 45114569, "title": "Make a Lisp in Nim", "cluster": 13, "x": 7.040803909301758, "y": 3.973745822906494}, {"id": 45114579, "title": "The wall confronting large language models", "cluster": 208, "x": 8.139845848083496, "y": 4.388083457946777}, {"id": 45113499, "title": "Agentic Design Patterns: Building Intelligent Systems", "cluster": 38, "x": 8.660030364990234, "y": 3.614964246749878}, {"id": 45113045, "title": "Benchmark for Local LLMs with German \"Who Wants to Be a Millionaire\" Questions", "cluster": 13, "x": 6.913760185241699, "y": 3.3888208866119385}, {"id": 45113072, "title": "LLM-eval-simple a simple way to evaluate LLM for your use case", "cluster": 13, "x": 6.788012504577637, "y": 3.320495843887329}, {"id": 45112632, "title": "LLM traffic: What's happening and what to do about it", "cluster": 13, "x": 6.787437915802002, "y": 3.350170373916626}, {"id": 45112535, "title": "Authors claim they just figured out how to predict LLM hallucinations", "cluster": 13, "x": 6.64091157913208, "y": 3.0312583446502686}, {"id": 45110311, "title": "The maths you need to start understanding LLMs", "cluster": 13, "x": 6.8518290519714355, "y": 3.2865355014801025}, {"id": 45109998, "title": "Lumo by Proton Mail", "cluster": 13, "x": 6.916118144989014, "y": 3.649068593978882}, {"id": 45109454, "title": "Curated LLM prompts for debugging with runtime DOM snapshots", "cluster": 13, "x": 6.833677291870117, "y": 3.6510467529296875}, {"id": 45109194, "title": "'Distealed' LLMs: smarter, 5-30x cheaper inference", "cluster": 13, "x": 7.014896869659424, "y": 3.3187344074249268}, {"id": 45108401, "title": "Apertus 70B: Truly Open - Swiss LLM by ETH, EPFL and CSCS", "cluster": 13, "x": 6.785198211669922, "y": 3.515529155731201}, {"id": 45107470, "title": "How Causal Reasoning Addresses the Limitations of LLMs in Observability", "cluster": 13, "x": 6.833250999450684, "y": 3.1047990322113037}, {"id": 45106727, "title": "Tesla's fourth 'Master Plan' reads like LLM-generated nonsense", "cluster": 13, "x": 6.746873378753662, "y": 3.0516459941864014}, {"id": 45105688, "title": "The Memorization Problem: Can We Trust LLMs' Economic Forecasts?", "cluster": 13, "x": 6.747618198394775, "y": 3.113550901412964}, {"id": 45105557, "title": "Your LLM-assisted scientific breakthrough probably isn't real", "cluster": 13, "x": 6.6071457862854, "y": 3.016953945159912}, {"id": 45105268, "title": "What are companies trying to achieve with LLMs beyond \"just chatbots\"?", "cluster": 13, "x": 6.380407810211182, "y": 3.287245512008667}, {"id": 45105006, "title": "Casual Productivity with LLMs", "cluster": 13, "x": 6.622924327850342, "y": 3.269284963607788}, {"id": 45104378, "title": "Words are lossy compressor, LMs speaks in meanings", "cluster": 13, "x": 6.70890474319458, "y": 3.546363353729248}, {"id": 45104086, "title": "Python script to make several AI (LLMs) talk to each other:)))", "cluster": 12, "x": 7.014795303344727, "y": 2.6686830520629883}, {"id": 45103764, "title": "VLLM: Anatomy of a High-Throughput LLM Inference System", "cluster": 13, "x": 7.0488996505737305, "y": 3.444443702697754}, {"id": 45103690, "title": "Which LLMs Can Code Genetic Algorithms That Work?", "cluster": 13, "x": 6.676248073577881, "y": 3.508739471435547}, {"id": 45103673, "title": "LLMs are Bayesian, in Expectation, not in Realization", "cluster": 13, "x": 6.772140026092529, "y": 3.1517229080200195}, {"id": 45102652, "title": "Apertus includes many languages that have so far been underrepresented in LLMs", "cluster": 13, "x": 6.758430480957031, "y": 3.483219623565674}, {"id": 45101894, "title": "I made a CLI to stop manually copy-pasting code into LLMs", "cluster": 13, "x": 6.73721981048584, "y": 3.7619636058807373}, {"id": 45101721, "title": "Apertus LLM", "cluster": 13, "x": 6.80311918258667, "y": 3.3729429244995117}, {"id": 45100817, "title": "Benchmarking LLM Codegen for SQL", "cluster": 13, "x": 6.901447772979736, "y": 3.5410683155059814}, {"id": 45099607, "title": "LLMs Love Elixir", "cluster": 13, "x": 6.64307975769043, "y": 3.1636667251586914}, {"id": 45099726, "title": "LLM Tokenizer in Zig", "cluster": 13, "x": 6.943077564239502, "y": 3.6712098121643066}, {"id": 45099510, "title": "LLM Ported to the C64, Kinda", "cluster": 13, "x": 6.969367027282715, "y": 3.743800163269043}, {"id": 45098777, "title": "A better Llama-CLI help doc", "cluster": 13, "x": 6.794589042663574, "y": 3.52463698387146}, {"id": 45098270, "title": "Little LLM on the RAM: Google's Gemma 270M hits the scene", "cluster": 13, "x": 7.0319976806640625, "y": 3.6845626831054688}, {"id": 45096111, "title": "Pretraining a LLM with less than $50 budget which outperforms Google BERT", "cluster": 13, "x": 6.784882068634033, "y": 3.311400890350342}, {"id": 45095763, "title": "SparseLoCo: Communication-Efficient LLM Training", "cluster": 13, "x": 6.838718414306641, "y": 3.5425539016723633}, {"id": 45094421, "title": "Adaptive LLM routing under budget constraints", "cluster": 13, "x": 6.850257396697998, "y": 3.424020290374756}, {"id": 45094277, "title": "LLM4ES: Learning User Embeddings from Event Sequences via Large Language Models", "cluster": 208, "x": 7.8280768394470215, "y": 4.199578762054443}, {"id": 45094003, "title": "VLLM: Anatomy of a High-Throughput LLM Inference System", "cluster": 13, "x": 7.069225788116455, "y": 3.4300858974456787}, {"id": 45092723, "title": "Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants", "cluster": 208, "x": 8.346904754638672, "y": 4.398805618286133}, {"id": 45092476, "title": "Improving LLM token usage when debugging", "cluster": 13, "x": 6.866473197937012, "y": 3.622610569000244}, {"id": 45092280, "title": "Kurosawa's Influences and an LLM Analogy", "cluster": 13, "x": 6.708533763885498, "y": 3.080472469329834}, {"id": 45092038, "title": "LegalPwn: Tricking LLMs by burying badness in lawyerly fine print", "cluster": 13, "x": 6.606789588928223, "y": 3.064856767654419}, {"id": 45091087, "title": "Why Relying on LLMs for Code Can Be a Security Nightmare: Terminal", "cluster": 13, "x": 6.562530994415283, "y": 3.5948903560638428}, {"id": 45090966, "title": "Probing LLM Social Intelligence via Werewolf", "cluster": 13, "x": 6.707030296325684, "y": 3.1062986850738525}, {"id": 45085198, "title": "I trapped an LLM into a Raspberry Pi and it spiraled into an existential crisis", "cluster": 13, "x": 6.488852024078369, "y": 3.109452247619629}, {"id": 45084111, "title": "TPDE-LLVM: Faster LLVM -O0 Back-End", "cluster": 13, "x": 7.117032051086426, "y": 3.912834882736206}, {"id": 45083582, "title": "Cline and LM Studio: the local coding stack with Qwen3 Coder 30B", "cluster": 13, "x": 6.980539321899414, "y": 4.085483551025391}, {"id": 45083485, "title": "Daniel Jackson on WYSIWID: Rethinking software structure to enable LLM coding", "cluster": 13, "x": 6.703561782836914, "y": 3.6481122970581055}, {"id": 45083226, "title": "Hackers are now hiding malware in the images served up by LLMs", "cluster": 13, "x": 6.500070571899414, "y": 3.5279133319854736}, {"id": 45083271, "title": "Nyxstone: An LLVM-based (Dis)assembly Framework", "cluster": 13, "x": 7.01167631149292, "y": 3.9122402667999268}, {"id": 45082271, "title": "The AI Engineer's Guide to LLM Observability with OpenTelemetry", "cluster": 12, "x": 7.053239822387695, "y": 2.8238327503204346}, {"id": 45081937, "title": "Bring your own brain? Why local LLMs are taking off", "cluster": 13, "x": 6.613288402557373, "y": 3.044558048248291}, {"id": 45078747, "title": "An LLM Traded a Toe for a Foot", "cluster": 13, "x": 6.609504222869873, "y": 3.1368236541748047}, {"id": 45078322, "title": "Reasoning in LLMs: A Taxonomy", "cluster": 13, "x": 6.828087329864502, "y": 3.2357122898101807}, {"id": 45076297, "title": "The Delusion Machine \u2013 What happened when I fed my soul into an LLM", "cluster": 13, "x": 6.493685722351074, "y": 3.125366687774658}, {"id": 45075977, "title": "TPDE-LLVM: 10-20x Faster LLVM -O0 Back-End", "cluster": 13, "x": 7.168996334075928, "y": 3.8947529792785645}, {"id": 45075633, "title": "Can LLMs Power Randomizers?", "cluster": 13, "x": 6.721179008483887, "y": 3.3928229808807373}, {"id": 45073742, "title": "LLMs for the Old and Infirm", "cluster": 13, "x": 6.558647632598877, "y": 3.136775016784668}, {"id": 45072774, "title": "An LLM-Proof Approach to Reinventing Captcha Systems", "cluster": 13, "x": 6.844473361968994, "y": 3.3962833881378174}, {"id": 45072481, "title": "TPDE-LLVM: Faster LLVM -O0 Back-End", "cluster": 13, "x": 7.108345985412598, "y": 3.922189712524414}, {"id": 45071138, "title": "EQ-Bench \u2013 benchmark LLM emotional intelligence", "cluster": 13, "x": 6.959098815917969, "y": 3.1861412525177}, {"id": 45069982, "title": "Large Language Models for Adverse Drug Events: A Clinical Perspective", "cluster": 208, "x": 8.162415504455566, "y": 4.295236110687256}, {"id": 45069847, "title": "LLM Evaluation: Practical Tips at Booking.com", "cluster": 13, "x": 6.810973167419434, "y": 3.2936837673187256}, {"id": 45068771, "title": "Can an LLM predict how markets evolve through hallucinations?", "cluster": 13, "x": 6.641204833984375, "y": 3.0244624614715576}, {"id": 45068928, "title": "Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data", "cluster": 13, "x": 6.843254566192627, "y": 3.686506748199463}, {"id": 45068287, "title": "Whodunit: LLM Murder Mysteries", "cluster": 13, "x": 6.554157257080078, "y": 3.1496214866638184}, {"id": 45067578, "title": "LLM and MCP in Browser", "cluster": 13, "x": 6.772287368774414, "y": 3.672928810119629}, {"id": 45067088, "title": "Add a privacy layer to your LLM app as AI companies' privacy policies evolve", "cluster": 12, "x": 6.994587421417236, "y": 2.651139736175537}, {"id": 45066310, "title": "Using LLMs for Intel Processor Code Trace Analysis", "cluster": 13, "x": 7.070178985595703, "y": 3.592089891433716}, {"id": 45064919, "title": "OpenAI lost the plot on 'boring' LLM use-cases", "cluster": 13, "x": 6.657814979553223, "y": 3.3589766025543213}, {"id": 45063603, "title": "Remote code execution with LLM agents", "cluster": 13, "x": 6.745935916900635, "y": 3.6453113555908203}, {"id": 45063415, "title": "A Novel Pretrained Tokenizer-Free LLM Architecture", "cluster": 13, "x": 6.923586845397949, "y": 3.648489475250244}, {"id": 45063214, "title": "Semantic Structure in Large Language Model Embeddings", "cluster": 208, "x": 8.148670196533203, "y": 4.423956871032715}, {"id": 45062783, "title": "Agentic Workflows", "cluster": 38, "x": 8.575966835021973, "y": 3.675675868988037}, {"id": 45062627, "title": "LLM Rules for Postgres", "cluster": 13, "x": 6.910044193267822, "y": 3.5267276763916016}, {"id": 45062046, "title": "An LLM is a lossy encyclopedia", "cluster": 13, "x": 6.649064540863037, "y": 3.168959140777588}, {"id": 45061773, "title": "Cline and LM Studio: the local coding stack with Qwen3 Coder 30B", "cluster": 13, "x": 6.973635196685791, "y": 4.072400093078613}, {"id": 45061724, "title": "Conversation: LLMs and Building Abstractions", "cluster": 13, "x": 6.672905445098877, "y": 3.2195217609405518}, {"id": 45060744, "title": "Building LLM-Powered Agent in Python", "cluster": 13, "x": 6.991516590118408, "y": 3.2948691844940186}, {"id": 45060677, "title": "Symbex: Search Python code for functions and classes, then pipe them into a LLM", "cluster": 13, "x": 6.875857353210449, "y": 3.735506057739258}, {"id": 45060642, "title": "Where LLMs Have Been Useful (For Rebuilding a Car)", "cluster": 13, "x": 6.714865207672119, "y": 3.3676276206970215}, {"id": 45060038, "title": "LLMs and the Russellian Inversion", "cluster": 13, "x": 6.838543891906738, "y": 3.12833833694458}, {"id": 45058499, "title": "AMD MI300X for LLM Serving Disaggregating Prefill and Decode with SGLang", "cluster": 13, "x": 6.9892706871032715, "y": 3.776506185531616}, {"id": 45058074, "title": "Should We Anthropomorphize LLMs?", "cluster": 13, "x": 6.592170715332031, "y": 2.983119249343872}, {"id": 45057322, "title": "Expert: LSP for Elixir", "cluster": 13, "x": 6.76865291595459, "y": 3.3526535034179688}, {"id": 45056201, "title": "Large language models can reconstruct forbidden knowledge", "cluster": 208, "x": 8.113563537597656, "y": 4.343807220458984}, {"id": 45055726, "title": "Arbitraging Down LLM Inference to the Cost of Electricity", "cluster": 13, "x": 7.303665637969971, "y": 3.153747320175171}, {"id": 45055641, "title": "Some thoughts on LLMs and software development", "cluster": 13, "x": 6.5764055252075195, "y": 3.535533905029297}, {"id": 45055317, "title": "LLM Eval Driven Development with Claude Code", "cluster": 13, "x": 6.719330787658691, "y": 3.799720048904419}, {"id": 45054542, "title": "Measuring LLM citations with server logs might not work as assumed", "cluster": 13, "x": 6.8956451416015625, "y": 3.4581995010375977}, {"id": 45054279, "title": "The Virtual Think Tank: Using LLMs to Get a Multitude of Perspectives \u2013 InfoQ", "cluster": 13, "x": 6.781119346618652, "y": 3.3503482341766357}, {"id": 45054066, "title": "DocStrange \u2013 A Python library for LLM-ready data with a new 7B parameter model", "cluster": 13, "x": 7.016723155975342, "y": 3.770421266555786}, {"id": 45054006, "title": "Forget Vector Databases: RAG with Just SQL and LLM", "cluster": 13, "x": 6.869976043701172, "y": 3.545351028442383}, {"id": 45051777, "title": "LLMs solving problems OCR+NLP couldn't", "cluster": 13, "x": 6.682937145233154, "y": 3.494189500808716}, {"id": 45051196, "title": "I was wrong about tidymodels and LLMs", "cluster": 13, "x": 6.537308216094971, "y": 3.1564853191375732}, {"id": 45051219, "title": "The Trusted Agentic Commerce Protocol", "cluster": 36, "x": 8.48566722869873, "y": 3.6031107902526855}, {"id": 45051217, "title": "Let's Build a Hypervisor with KVM", "cluster": 13, "x": 7.0768866539001465, "y": 3.897794008255005}, {"id": 45050795, "title": "NiceAPI, free self-host LLM router", "cluster": 13, "x": 6.7911481857299805, "y": 3.78310227394104}, {"id": 45049869, "title": "Make LLMs understand your 3D models", "cluster": 13, "x": 6.762641429901123, "y": 3.2855234146118164}, {"id": 45049133, "title": "AI Emotions: System Prompt for Artificial Emotional States in LLMs", "cluster": 12, "x": 6.996537685394287, "y": 2.7258753776550293}, {"id": 45045742, "title": "We Are Still Unable to Secure LLMs from Malicious Inputs", "cluster": 13, "x": 6.488461971282959, "y": 3.3921899795532227}, {"id": 45044561, "title": "LLM-system-design-and-model-selection", "cluster": 13, "x": 6.8195013999938965, "y": 3.492372512817383}, {"id": 45044393, "title": "Can LLMs Dream of Electric Sheep?", "cluster": 13, "x": 6.506163120269775, "y": 2.963378429412842}, {"id": 45044351, "title": "Foresight-32B Beats Frontier LLMs on Live Polymarket Predictions", "cluster": 13, "x": 7.011313438415527, "y": 3.205932855606079}, {"id": 45043694, "title": "Agentic Metaflow", "cluster": 37, "x": 8.619967460632324, "y": 3.644392251968384}, {"id": 45043503, "title": "Streamline LLM Evaluation with Stax", "cluster": 13, "x": 6.8716654777526855, "y": 3.370352029800415}, {"id": 45043237, "title": "Apparently it's easy to detect LLM-generated text now", "cluster": 13, "x": 6.6884894371032715, "y": 3.4242069721221924}, {"id": 45041542, "title": "One long sentence is all it takes to make LLMs misbehave", "cluster": 13, "x": 6.524974346160889, "y": 3.081951379776001}, {"id": 45041052, "title": "How to design a DBMS for Telco requirements", "cluster": 13, "x": 6.914531707763672, "y": 3.568493604660034}, {"id": 45040938, "title": "New methodology for editing existing code files using LLM generated snippets", "cluster": 13, "x": 6.802962303161621, "y": 3.779350996017456}, {"id": 45040686, "title": "Handling long-running LLM streams in a stateful backend", "cluster": 13, "x": 6.799698829650879, "y": 3.6038386821746826}, {"id": 45040306, "title": "Why Relying on LLMs for Code Can Be a Security Nightmare", "cluster": 13, "x": 6.519402027130127, "y": 3.563117265701294}, {"id": 45039690, "title": "Humain Chat: Arabic-centric LLM powered by ALLAM 34B", "cluster": 13, "x": 6.641726016998291, "y": 3.545227289199829}, {"id": 45039415, "title": "Google's Gemini CLI Agent Comes to Zed", "cluster": 35, "x": 8.471019744873047, "y": 3.8088085651397705}, {"id": 45039124, "title": "We Are Still Unable to Secure LLMs from Malicious Inputs", "cluster": 13, "x": 6.483358383178711, "y": 3.3922605514526367}, {"id": 45039053, "title": "Hierarchical Reasoning Model outperforms LLMs at reasoning tasks", "cluster": 13, "x": 6.940725803375244, "y": 3.2432212829589844}, {"id": 45038792, "title": "Introduction \u2013 Agent Client Protocol", "cluster": 36, "x": 8.529711723327637, "y": 3.7072947025299072}, {"id": 45038220, "title": "LLMs for Software Developers (notes from my talk at NWRUG)", "cluster": 13, "x": 6.648787498474121, "y": 3.5854880809783936}, {"id": 45038131, "title": "Speed Always Wins: A Survey on Efficient Architectures for LLMs", "cluster": 13, "x": 7.0105366706848145, "y": 3.574767827987671}, {"id": 45037213, "title": "Too many model context protocol servers and LLM allocations on the dance floor", "cluster": 13, "x": 6.8580121994018555, "y": 3.3828699588775635}, {"id": 45036468, "title": "Policies on Large Language Model Usage at ICLR 2026", "cluster": 208, "x": 8.200187683105469, "y": 4.504812717437744}, {"id": 45035984, "title": "Translation Validation for LLVM's AArch64 Back End [pdf]", "cluster": 13, "x": 6.960716724395752, "y": 3.712977886199951}, {"id": 45035935, "title": "Units of Economics of LLMs. Reply to Ed Zitron's \"AI Is a Money Trap\"", "cluster": 12, "x": 6.951691627502441, "y": 2.6334309577941895}, {"id": 45035097, "title": "LLM VRAM Usage Cut by 45x? What Jet-Nemotron Means for Local Users", "cluster": 13, "x": 7.091368675231934, "y": 3.8439958095550537}, {"id": 45034473, "title": "Anyone deploying LLMs in production at a startup?", "cluster": 13, "x": 6.7085418701171875, "y": 3.563366174697876}, {"id": 45033544, "title": "The evolution of LLM tool calling", "cluster": 13, "x": 6.651159763336182, "y": 3.530299425125122}, {"id": 45031903, "title": "LLMs: Exploration vs. Execution", "cluster": 13, "x": 6.796298027038574, "y": 3.26711368560791}, {"id": 45031574, "title": "How to use LLMs for studying without bullshitting yourself", "cluster": 13, "x": 6.82920503616333, "y": 3.3177437782287598}, {"id": 45031400, "title": "Can LLMs Dream of Electric Sheep?", "cluster": 13, "x": 6.514946460723877, "y": 2.9770700931549072}, {"id": 45030789, "title": "Fine-tuning LLM Agents without Fine-tuning LLMs", "cluster": 13, "x": 6.771058559417725, "y": 3.253396987915039}, {"id": 45030110, "title": "Patient Lisp Hacker Seeks Same for Long Walks Through IPL-V Code", "cluster": 13, "x": 6.5293660163879395, "y": 3.5630152225494385}, {"id": 45029660, "title": "Agentic RAG and Context Engineering for Agents", "cluster": 38, "x": 8.529780387878418, "y": 3.78010630607605}, {"id": 45029448, "title": "High rate of LLM (GPT5) hallucinations in dense stats domains (cricket)", "cluster": 13, "x": 6.939611911773682, "y": 3.084841251373291}, {"id": 45028555, "title": "LLM Context Management: How to Improve Performance and Lower Costs", "cluster": 13, "x": 6.841100692749023, "y": 3.4639620780944824}, {"id": 45028400, "title": "LLVM 21.1 Released with AMD GFX1250 Target, Improved RISC-V, New C/C++ Features", "cluster": 13, "x": 7.223324775695801, "y": 4.193175315856934}, {"id": 45027960, "title": "How to run LLMs on PC at home using Llama.cpp", "cluster": 13, "x": 6.943119049072266, "y": 3.8480606079101562}, {"id": 45027930, "title": "GPT5 is the best coding LLM because other LLMs admit it?", "cluster": 13, "x": 6.6695027351379395, "y": 3.613123893737793}, {"id": 45027700, "title": "Free Access to Frontier Coding LLMs: 5M Tokens/Day of Claude Sonnet 4 and More", "cluster": 13, "x": 6.810068130493164, "y": 3.673630952835083}, {"id": 45026943, "title": "Modular LLM framework inspired by Linux \u2013 aiming for a one-GPU future", "cluster": 13, "x": 6.9366774559021, "y": 3.835705041885376}, {"id": 45026942, "title": "Spotify Is Adding DMs", "cluster": 13, "x": 6.842324733734131, "y": 3.6668753623962402}, {"id": 45026247, "title": "So you want to use SLMs. Can you handle them?", "cluster": 13, "x": 6.660434246063232, "y": 3.268328905105591}, {"id": 45025946, "title": "Why Cannot Large Language Models Models Ever Make True Correct Reasoning?", "cluster": 208, "x": 8.39185905456543, "y": 4.319818496704102}, {"id": 45025751, "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned LLMs", "cluster": 13, "x": 6.795864582061768, "y": 3.623232841491699}, {"id": 45024997, "title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs", "cluster": 13, "x": 6.821108341217041, "y": 3.2850422859191895}, {"id": 45022467, "title": "SQLStorm: Taking Database Benchmarking into the LLM Era", "cluster": 13, "x": 6.968654155731201, "y": 3.4480414390563965}, {"id": 45020456, "title": "Doc2MD: An LLM powered document to Markdown conversion utility", "cluster": 13, "x": 6.918153762817383, "y": 3.6697638034820557}, {"id": 45019797, "title": "Grok: Thousands LOC a day in C is a big deal even if the \"coder\" uses LLM?", "cluster": 13, "x": 6.669919967651367, "y": 3.6332836151123047}, {"id": 45016359, "title": "Lemonade: Local LLM Serving with GPU and NPU Acceleration", "cluster": 13, "x": 7.056434631347656, "y": 3.8079938888549805}, {"id": 45015577, "title": "AetherCode: Evaluating LLMs' Ability to Win in Premier Programming Competitions", "cluster": 13, "x": 6.679646015167236, "y": 3.569772720336914}, {"id": 45015661, "title": "AlphaAgents: LLM Based Multi-Agents for Equity Portfolio Constructions", "cluster": 13, "x": 6.934169769287109, "y": 3.2106945514678955}, {"id": 45015631, "title": "Building LLM Agents for Hacking", "cluster": 13, "x": 6.459137916564941, "y": 3.4194159507751465}, {"id": 45015467, "title": "The Database Has a New User\u2013LLMs\u2013and They Need a Different Database", "cluster": 13, "x": 6.635585308074951, "y": 3.44328236579895}, {"id": 45014995, "title": "Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?", "cluster": 208, "x": 8.419459342956543, "y": 4.559084415435791}, {"id": 45014679, "title": "Use it or lose it \u2013 musings on LLMs", "cluster": 13, "x": 6.57634973526001, "y": 3.0933399200439453}, {"id": 45014322, "title": "Multimodal LLMs Are Blind", "cluster": 13, "x": 6.66270637512207, "y": 3.2153687477111816}, {"id": 45014301, "title": "RL Needed LLMs Because Agency Requires Priors", "cluster": 13, "x": 6.709639549255371, "y": 3.203629970550537}, {"id": 45013913, "title": "(: Smile The open source LLM instruction language", "cluster": 13, "x": 6.854307651519775, "y": 3.768754005432129}, {"id": 45013662, "title": "Instructor \u2013 Structured Outputs for LLMs", "cluster": 13, "x": 6.866199016571045, "y": 3.440495252609253}, {"id": 45013494, "title": "Continuing the journey of optimal LLM-assisted coding experience", "cluster": 13, "x": 6.678955554962158, "y": 3.630437135696411}, {"id": 45013001, "title": "Applying the Chinese Wall Reverse Engineering Technique to LLM Code Editing", "cluster": 13, "x": 6.641322612762451, "y": 3.739234685897827}, {"id": 45011335, "title": "Neovim now natively supports inline competitions from LLMs", "cluster": 13, "x": 6.926143169403076, "y": 3.714315176010132}, {"id": 45011106, "title": "Exploring LLM Confidence in Code Completion", "cluster": 13, "x": 6.712331295013428, "y": 3.59539794921875}, {"id": 45010576, "title": "AetherCode: Evaluating LLMs' Ability to Win in Premier Programming Competitions", "cluster": 13, "x": 6.669183254241943, "y": 3.5614147186279297}, {"id": 45009612, "title": "Do LLMs 'store' personal data? This is asking the wrong question (2024)", "cluster": 13, "x": 6.5890936851501465, "y": 3.4127399921417236}, {"id": 45007581, "title": "Evaluating Long-Term Conversational Memory of LLM Agents", "cluster": 13, "x": 6.739784240722656, "y": 3.11449933052063}, {"id": 45006925, "title": "<script type=\"text/llms.txt\">, a proposal for inline LLM instructions in HTML", "cluster": 13, "x": 6.862828731536865, "y": 3.7464113235473633}, {"id": 45004793, "title": "LLMs: Common Terms Explained, Simply", "cluster": 13, "x": 6.667544841766357, "y": 3.202227830886841}, {"id": 45004617, "title": "DeepConf: Scaling LLM reasoning with confidence, not just compute", "cluster": 13, "x": 6.999330997467041, "y": 3.2718279361724854}, {"id": 45004728, "title": "Making games in Go: 3 months without LLMs vs. 3 days with LLMs", "cluster": 13, "x": 6.58454704284668, "y": 3.1956682205200195}, {"id": 45003898, "title": "Tinker with LLMs in the privacy of your own home using Llama.cpp", "cluster": 13, "x": 6.746666431427002, "y": 3.6889023780822754}, {"id": 45002958, "title": "Writing with LLM is not a shame", "cluster": 13, "x": 6.588374137878418, "y": 3.3299906253814697}, {"id": 45001371, "title": "ThinkMesh: A Python lib for parallel thinking in LLMs", "cluster": 13, "x": 6.971426010131836, "y": 3.4648079872131348}, {"id": 45001188, "title": "LLMs and the Russellian Inversion", "cluster": 13, "x": 6.816406726837158, "y": 3.1082258224487305}, {"id": 45001316, "title": "Concept Poisoning: Probing LLMs without probes", "cluster": 13, "x": 6.626995086669922, "y": 3.059570074081421}, {"id": 45000403, "title": "Game-Changer for Local LLMs: AMD Medusa Halo Points to 384-Bit LPDDR6 Bandwidth", "cluster": 13, "x": 7.046067714691162, "y": 3.7947754859924316}, {"id": 45000270, "title": "Evaluating LLMs for my personal use case", "cluster": 13, "x": 6.688640594482422, "y": 3.3184473514556885}, {"id": 45000101, "title": "How I got Claude-Code to work with a local LLM using a custom proxy", "cluster": 13, "x": 6.834824085235596, "y": 3.950899362564087}, {"id": 44998670, "title": "NeuralMail: Open-source tool to search all your emails with LLMs", "cluster": 13, "x": 6.830942630767822, "y": 3.659836769104004}, {"id": 44997404, "title": "<script type=\"text/llms.txt\">", "cluster": 13, "x": 6.778148174285889, "y": 3.63106369972229}, {"id": 44997158, "title": "Apple releases adapted SlowFast-LLaVA model for long-form video analysis", "cluster": 8, "x": 7.232862949371338, "y": 3.773434638977051}, {"id": 44996335, "title": "Make LLMs supportive, not sycophantic", "cluster": 13, "x": 6.551513195037842, "y": 3.1212680339813232}, {"id": 44995480, "title": "Do LLMs Have Good Music Taste?", "cluster": 13, "x": 6.58160924911499, "y": 3.0473439693450928}, {"id": 44994515, "title": "Asking three LLMs a simple question", "cluster": 13, "x": 6.604449272155762, "y": 3.1280009746551514}, {"id": 44994135, "title": "LNS-Madam: Low-Precision Training in Log Using Multiplicative Weight Update", "cluster": 13, "x": 7.085699081420898, "y": 3.5906450748443604}, {"id": 44993721, "title": "LLMs Are Biased Here's Why Enterprises Can't Afford to Just Plug and Pray", "cluster": 13, "x": 6.51986837387085, "y": 3.1084775924682617}, {"id": 44992517, "title": "LLMs are NOT Turing Complete (at train time), we need \"train time recurrence\"", "cluster": 13, "x": 6.730224609375, "y": 3.165018081665039}, {"id": 44991884, "title": "My experience creating software with LLM coding agents \u2013 Part 2 (Tips)", "cluster": 13, "x": 6.7671732902526855, "y": 3.5468342304229736}, {"id": 44991926, "title": "Modeling Annotator Disagreement with Demographic-Aware Experts", "cluster": 208, "x": 8.340649604797363, "y": 4.32391357421875}, {"id": 44990981, "title": "The use of LLM assistants for kernel development", "cluster": 13, "x": 6.777163028717041, "y": 3.6485252380371094}, {"id": 44990586, "title": "The Cost of Winning:How RL Training on Poker Leads to Evil LLMs", "cluster": 13, "x": 6.656506061553955, "y": 3.0767757892608643}, {"id": 44989526, "title": "Toying with Poisoned Search Results Fed to an LLM", "cluster": 13, "x": 6.490556716918945, "y": 3.1737654209136963}, {"id": 44988924, "title": "Do LLMs have good music taste?", "cluster": 13, "x": 6.591602325439453, "y": 3.034898042678833}, {"id": 44988371, "title": "TimeCapsule LLM: trained only on data from certain time periods", "cluster": 13, "x": 6.885921955108643, "y": 3.3434839248657227}, {"id": 44987351, "title": "Fine-tuning Llama 8B to give it the ability to message you first", "cluster": 13, "x": 6.677909851074219, "y": 3.5986409187316895}, {"id": 44987312, "title": "Multiagent orchestration framework to manage swarms via drag and drop", "cluster": 37, "x": 8.597402572631836, "y": 3.7501120567321777}, {"id": 44987032, "title": "Project Orchestration Agents", "cluster": 38, "x": 8.59392261505127, "y": 3.700378179550171}, {"id": 44986059, "title": "Egune AI, the LLM that specializes in Mongolia's language and culture", "cluster": 13, "x": 6.7014970779418945, "y": 3.328932762145996}, {"id": 44986025, "title": "Too many model context protocol servers and LLM allocations on the dance floor", "cluster": 13, "x": 6.843717098236084, "y": 3.4063308238983154}, {"id": 44985702, "title": "Semcheck: Spec-Driven Development Using LLMs", "cluster": 13, "x": 6.768725395202637, "y": 3.6100990772247314}, {"id": 44985278, "title": "Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing", "cluster": 13, "x": 6.9682440757751465, "y": 3.553541660308838}, {"id": 44984906, "title": "Building software on top of an LLM is hard, but not that hard", "cluster": 13, "x": 6.652019500732422, "y": 3.571989059448242}, {"id": 44983998, "title": "Experiments assess 'artificial' altruism displayed by large language models", "cluster": 208, "x": 8.314791679382324, "y": 4.194046497344971}, {"id": 44983359, "title": "Do I need a Lisp Machine comeback?", "cluster": 13, "x": 7.170109272003174, "y": 4.0991621017456055}, {"id": 44983321, "title": "Seed-OSS: open-source LLM models by ByteDance", "cluster": 13, "x": 6.831618785858154, "y": 3.8011882305145264}, {"id": 44983259, "title": "The Hidden Cost of Winning:How RL Training on Poker Degrades LLM Moral Alignment", "cluster": 13, "x": 6.6494951248168945, "y": 3.0409958362579346}, {"id": 44983327, "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "cluster": 13, "x": 6.8468852043151855, "y": 3.173967123031616}, {"id": 44983061, "title": "Endless Wiki \u2013 A useless self-hosted encyclopedia driven by LLM hallucinations", "cluster": 13, "x": 6.5618720054626465, "y": 3.57755184173584}, {"id": 44982748, "title": "Fmllm: 4mb training data, 100mb model, Fibonacci embeddings, near-coherent. WTF?", "cluster": 13, "x": 7.186297416687012, "y": 3.632938861846924}, {"id": 44982017, "title": "The \"Super Weight:\" How Even a Single Parameter Can Determine a LLM's Behavior", "cluster": 13, "x": 6.709096431732178, "y": 3.1145310401916504}, {"id": 44981532, "title": "LLMs Won't Replace Programming Languages", "cluster": 13, "x": 6.677032947540283, "y": 3.594886302947998}, {"id": 44980768, "title": "The LLM App Layer", "cluster": 13, "x": 6.880511283874512, "y": 3.655379056930542}, {"id": 44980399, "title": "The use of LLM assistants for kernel development", "cluster": 13, "x": 6.761488437652588, "y": 3.6262307167053223}, {"id": 44980047, "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization", "cluster": 13, "x": 6.853738307952881, "y": 3.351957082748413}, {"id": 44979757, "title": "Prediction of Bearing Layer Depth Using Machine Learning Algorithms", "cluster": 3, "x": 7.180673599243164, "y": 2.7241368293762207}, {"id": 44978926, "title": "Ask an LLM \"What is the time?\"", "cluster": 13, "x": 6.650629043579102, "y": 3.1532094478607178}, {"id": 44976815, "title": "In the long run, LLMs make us dumber", "cluster": 13, "x": 6.510046482086182, "y": 3.0589096546173096}, {"id": 44976325, "title": "FormalGrad: Integrating Formal Methods with Gradient-Based LLM Refinement", "cluster": 13, "x": 6.936213493347168, "y": 3.4871582984924316}, {"id": 44975124, "title": "Coding Wedge: Are Developers and Coding Automation Key to LLM Competition?", "cluster": 13, "x": 6.600981712341309, "y": 3.570255994796753}, {"id": 44974916, "title": "R-Zero: Codes for R-Zero: Self-Evolving Reasoning LLM from Zero Data", "cluster": 13, "x": 7.034333229064941, "y": 3.4215521812438965}, {"id": 44974817, "title": "Pact: Head-to-head negotiation benchmark for LLMs", "cluster": 13, "x": 6.896842956542969, "y": 3.2538981437683105}, {"id": 44973948, "title": "Context engineering is just software engineering for LLMs", "cluster": 13, "x": 6.472339153289795, "y": 3.5363943576812744}, {"id": 44974139, "title": "CCFC: Core and Core-Full-Core Dual-Track Defense for LLM Jailbreak Protection", "cluster": 13, "x": 6.920494079589844, "y": 3.8677070140838623}, {"id": 44973179, "title": "OpenAI has lost the plot on boring LLM use cases", "cluster": 13, "x": 6.654239177703857, "y": 3.437957286834717}, {"id": 44972371, "title": "Using LLMs to turn scripts into applications", "cluster": 13, "x": 6.806349754333496, "y": 3.74990177154541}, {"id": 44972512, "title": "I use LLMs to learn new subjects", "cluster": 13, "x": 6.732794761657715, "y": 3.3406894207000732}, {"id": 44971944, "title": "A Complete Guide to LLVM for Programming Language Creators", "cluster": 13, "x": 6.859204292297363, "y": 3.7901155948638916}, {"id": 44971163, "title": "seed-OSS - Seed-OSS Open-Source LLM Models by ByteDance", "cluster": 13, "x": 6.9067182540893555, "y": 3.7773375511169434}, {"id": 44971143, "title": "Blink Twice: Measuring Strategic Deception Amongst LLMs", "cluster": 13, "x": 6.713682174682617, "y": 3.0667459964752197}, {"id": 44970222, "title": "Best workflow for quick ideation with LLMs from phone", "cluster": 13, "x": 6.792364120483398, "y": 3.5100181102752686}, {"id": 44969545, "title": "Compute Where It Counts: High Quality Sparsely Activated LLMs", "cluster": 13, "x": 7.025994300842285, "y": 3.5045816898345947}, {"id": 44969035, "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "cluster": 13, "x": 6.793961524963379, "y": 3.0337443351745605}, {"id": 44968956, "title": "LLMs generate 'fluent nonsense' when reasoning outside their training zone", "cluster": 13, "x": 6.663905620574951, "y": 3.1852872371673584}, {"id": 44968493, "title": "How I code with LLMs these days", "cluster": 13, "x": 6.764138221740723, "y": 3.73313307762146}, {"id": 44968395, "title": "Curator: Scalable data pre processing and curation toolkit for LLMs", "cluster": 13, "x": 7.008230686187744, "y": 3.5604259967803955}, {"id": 44968183, "title": "A Systematic Study of Post-Training Quantization for Diffusion LLMs", "cluster": 13, "x": 7.099828243255615, "y": 3.2487218379974365}, {"id": 44968025, "title": "LLMs Can't Tell Time, Winning Builders Budget for This Early", "cluster": 13, "x": 6.834589004516602, "y": 3.1323790550231934}, {"id": 44966659, "title": "Slice: SAST and LLM Interprocedural Context Extractor", "cluster": 13, "x": 7.049083232879639, "y": 3.745450496673584}, {"id": 44965058, "title": "Compute Where It Counts: a trainable LLM sparsity enabling 4x CPU speed", "cluster": 13, "x": 7.07276725769043, "y": 3.579789161682129}, {"id": 44964874, "title": "Auto-labeling in Viam can elevate your edge ML project", "cluster": 13, "x": 6.967496871948242, "y": 3.6892664432525635}, {"id": 44964660, "title": "The Technical Reason LLMs Fail on Complex Unstructured Data", "cluster": 13, "x": 6.769935131072998, "y": 3.3341453075408936}, {"id": 44964354, "title": "How do LSM Trees work?", "cluster": 13, "x": 6.715557098388672, "y": 3.2168564796447754}, {"id": 44963432, "title": "Processing 24T tokens for LLM training with 0 crashes (what made it possible)", "cluster": 13, "x": 6.929530620574951, "y": 3.586106538772583}, {"id": 44963512, "title": "A proposal for inline LLM instructions in HTML based on llms.txt", "cluster": 13, "x": 6.88170862197876, "y": 3.721754312515259}, {"id": 44962747, "title": "Thoughts on the Future of LLM Interactivity", "cluster": 13, "x": 6.778415203094482, "y": 3.284165620803833}, {"id": 44962698, "title": "Subliminal Poisoning Is the LLM Version of a Buffer Overflow", "cluster": 13, "x": 6.494927406311035, "y": 3.1266238689422607}, {"id": 44961849, "title": "AAMG is looking to acquire Lilium's assets and expertise", "cluster": 13, "x": 6.734110355377197, "y": 3.1258814334869385}, {"id": 44961135, "title": "A Survey on Diffusion Language Models", "cluster": 208, "x": 8.37950611114502, "y": 4.330236911773682}, {"id": 44960249, "title": "Language Models as Thespians", "cluster": 208, "x": 8.422235488891602, "y": 4.379711151123047}, {"id": 44958778, "title": "Guide to Operating Self-Hosted LLM Providers in CI Pipelines", "cluster": 13, "x": 6.772925853729248, "y": 3.633294105529785}, {"id": 44958037, "title": "Multimodal Sensing-Enabled LLMs for Automated Emotional Regulation", "cluster": 13, "x": 6.9137091636657715, "y": 3.2735068798065186}, {"id": 44958121, "title": "LLMs Are Letter-Blind and Here's Why Enterprises Should Care", "cluster": 13, "x": 6.572303771972656, "y": 3.10935378074646}, {"id": 44955974, "title": "Built a back end service to help companies manage multiple ML models", "cluster": 13, "x": 6.854355335235596, "y": 3.591831922531128}, {"id": 44955612, "title": "Practical approach for streaming UI from LLMs", "cluster": 13, "x": 6.838282108306885, "y": 3.707271099090576}, {"id": 44955520, "title": "$1T Agent Interoperability in Plain Sight", "cluster": 36, "x": 8.512812614440918, "y": 3.5631778240203857}, {"id": 44954738, "title": "An Analysis of Chinese Censorship Bias in LLMs", "cluster": 13, "x": 6.631016731262207, "y": 2.986081838607788}, {"id": 44954037, "title": "How to Give Your RTX 4090 Nearly Infinite Memory for LLM Inference", "cluster": 13, "x": 7.252946376800537, "y": 3.6562206745147705}, {"id": 44953405, "title": "A CRM that lives in your inbox and messages", "cluster": 13, "x": 6.556064605712891, "y": 3.4264602661132812}, {"id": 44953470, "title": "Warp sends a terminal session to LLM without user consent", "cluster": 13, "x": 6.589894771575928, "y": 3.516875743865967}, {"id": 44952268, "title": "Prompter: Jupyter-Like LLM Notebooks for VSCode", "cluster": 13, "x": 6.870172500610352, "y": 3.8104121685028076}, {"id": 44951608, "title": "Blink Twice: Measuring Strategic Deception Amongst LLMs", "cluster": 13, "x": 6.689849853515625, "y": 3.0280332565307617}, {"id": 44951316, "title": "New Benchmark for Coding LLMs puts GPT-5 at the top", "cluster": 13, "x": 6.965982913970947, "y": 3.6289727687835693}, {"id": 44950753, "title": "Ormin \u2013 An ORM for Nim", "cluster": 13, "x": 6.865005016326904, "y": 3.5048036575317383}, {"id": 44949825, "title": "Do LLMs Have Good Music Taste?", "cluster": 13, "x": 6.560218334197998, "y": 3.0373573303222656}, {"id": 44949205, "title": "Language Models as Thespians", "cluster": 208, "x": 8.39936637878418, "y": 4.372244834899902}, {"id": 44949183, "title": "Large Language Models Parse Content", "cluster": 208, "x": 8.138155937194824, "y": 4.50199031829834}, {"id": 44948900, "title": "Viteval \u2013 an LLM evaluation framework powered by Vitest", "cluster": 13, "x": 6.879369258880615, "y": 3.4179646968841553}, {"id": 44948735, "title": "We raised $7.3M to build an open-source stack for industrial-grade LLM apps", "cluster": 13, "x": 6.870352745056152, "y": 3.658473253250122}, {"id": 44946921, "title": "LLM Overcoding: How I Stopped Coding", "cluster": 13, "x": 6.578026294708252, "y": 3.6032772064208984}, {"id": 44946350, "title": "Paradagm \u2013 cool spreadsheet UX for interacting with LLMs", "cluster": 13, "x": 6.920905113220215, "y": 3.714458703994751}, {"id": 44945613, "title": "MemOS: Treating \"memory\" as a first-class resource for LLMs", "cluster": 13, "x": 6.749310493469238, "y": 3.2787559032440186}, {"id": 44945151, "title": "Harnessing Large Language Models to Overcome Recommender System Challenges", "cluster": 208, "x": 8.169235229492188, "y": 4.424492835998535}, {"id": 44944293, "title": "LLM from scratch, part 18 \u2013 residuals, shortcut connections, and the Talmud", "cluster": 13, "x": 6.8317108154296875, "y": 3.2423770427703857}, {"id": 44943318, "title": "Observability Stack for vLLM Inference", "cluster": 13, "x": 7.1915411949157715, "y": 3.539219617843628}, {"id": 44943018, "title": "LLMs suggest women seek lower salaries than men in job interviews", "cluster": 13, "x": 6.633836269378662, "y": 3.0334532260894775}, {"id": 44942525, "title": "Firefox 142 Now Available \u2013 Allows Browser Extensions/Add-Ons to Use AI LLMs", "cluster": 12, "x": 7.008993148803711, "y": 2.8490841388702393}, {"id": 44941812, "title": "Automatic LLM Context Using Screen Captures", "cluster": 13, "x": 6.9137749671936035, "y": 3.672801971435547}, {"id": 44941706, "title": "Fresh Eyes as a Service: Using LLMs to Test CLI Ergonomics", "cluster": 13, "x": 6.889113426208496, "y": 3.514622211456299}, {"id": 44939793, "title": "LLM Benchmarks on Company Data", "cluster": 13, "x": 6.958486557006836, "y": 3.376232385635376}, {"id": 44939331, "title": "LLMs and coding agents are a security nightmare", "cluster": 13, "x": 6.463389873504639, "y": 3.5156631469726562}, {"id": 44937865, "title": "Why is LLM usage on OpenRouter decreasing?", "cluster": 13, "x": 6.807348251342773, "y": 3.464503049850464}, {"id": 44937730, "title": "Spiral-Bench: A new benchmark measuring LLM sycophancy and delusion", "cluster": 13, "x": 6.938098430633545, "y": 3.221836566925049}, {"id": 44937442, "title": "Pivotal Token Search (PTS): Targeting Critical Decision Points in LLM Training", "cluster": 13, "x": 6.973357200622559, "y": 3.255333185195923}, {"id": 44936480, "title": "Caote: KV Cache Eviction for LLMs", "cluster": 13, "x": 6.845178604125977, "y": 3.606933355331421}, {"id": 44936163, "title": "Lyrion Music Server (LMS)", "cluster": 13, "x": 6.978267192840576, "y": 3.8246378898620605}, {"id": 44935436, "title": "Extrinsic Hallucinations in LLMs", "cluster": 13, "x": 6.579966068267822, "y": 3.0245182514190674}, {"id": 44935169, "title": "Llama-Scan: Convert PDFs to Text W Local LLMs", "cluster": 13, "x": 6.970043182373047, "y": 3.7183756828308105}, {"id": 44933724, "title": "LLMs and Coding Agents = Security Nightmare", "cluster": 13, "x": 6.457489490509033, "y": 3.4913740158081055}, {"id": 44933588, "title": "Solution for Managing LLM Prompts", "cluster": 13, "x": 6.792941570281982, "y": 3.492610216140747}, {"id": 44933002, "title": "Janito v2.28.0 \u2013 CLI LLM tool with terminal bells and provider fixes", "cluster": 13, "x": 6.877146244049072, "y": 3.9138998985290527}, {"id": 44932660, "title": "Profiling LLM Inference on Apple Silicon: A Quantization Perspective", "cluster": 13, "x": 7.116257667541504, "y": 3.6127588748931885}, {"id": 44932565, "title": "Does OLAP Need an ORM", "cluster": 13, "x": 6.807419300079346, "y": 3.452793598175049}, {"id": 44931859, "title": "Code with LLMs in Parallel", "cluster": 13, "x": 6.874449253082275, "y": 3.6986775398254395}, {"id": 44931415, "title": "Why Nim?", "cluster": 13, "x": 6.614410400390625, "y": 3.0807156562805176}, {"id": 44931340, "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models", "cluster": 208, "x": 8.154727935791016, "y": 4.581917762756348}, {"id": 44931214, "title": "Detecting hallucinations in LLM function calling with entropy", "cluster": 13, "x": 6.720586776733398, "y": 3.0636696815490723}, {"id": 44930808, "title": "LL3M: Large Language 3D Modelers", "cluster": 13, "x": 7.355128288269043, "y": 4.008213043212891}, {"id": 44926365, "title": "Dedicated API for tensor-like LLM prompt executions", "cluster": 13, "x": 6.875420093536377, "y": 3.689905881881714}, {"id": 44925442, "title": "Friction-Forward Design and Consumer LLMs", "cluster": 13, "x": 6.761562347412109, "y": 3.2454285621643066}, {"id": 44923107, "title": "Looming Stagnation in LLM Technologies: Why Investors Should Exercise Caution", "cluster": 13, "x": 6.5274553298950195, "y": 3.2634873390197754}, {"id": 44922758, "title": "LLMs contain all knowledge \u2013 I built way to mine deep meaning from them", "cluster": 13, "x": 6.820023536682129, "y": 3.267565965652466}, {"id": 44922451, "title": "Waluigi Effect in LLMs", "cluster": 13, "x": 6.730372428894043, "y": 3.0522212982177734}, {"id": 44920673, "title": "Open weight LLMs exhibit inconsistent performance across providers", "cluster": 13, "x": 6.901280403137207, "y": 3.4235846996307373}, {"id": 44920194, "title": "Notes on some basics of LLM usage", "cluster": 13, "x": 6.7559895515441895, "y": 3.3126933574676514}, {"id": 44919690, "title": "ORMs Are Good", "cluster": 13, "x": 6.81335973739624, "y": 3.2013368606567383}, {"id": 44919589, "title": "IFairy: The First 2-bit Complex LLM with All Parameters in \\{\\pm1, \\pm i\\}", "cluster": 13, "x": 6.889892101287842, "y": 3.52864933013916}, {"id": 44918428, "title": "Emotional Intelligence Benchmarks for LLMs", "cluster": 13, "x": 6.899620532989502, "y": 3.2072978019714355}, {"id": 44918186, "title": "Why LLMs Can Think (as per new fundamental theory)", "cluster": 13, "x": 6.7042999267578125, "y": 3.034723997116089}, {"id": 44916552, "title": "Can LLMs recognise ASCII images?", "cluster": 13, "x": 6.852381706237793, "y": 3.535874128341675}, {"id": 44916183, "title": "FormulaOne \u2013 new LLM benchmark using Dynamic Programming problems", "cluster": 13, "x": 7.0405354499816895, "y": 3.497396469116211}, {"id": 44916050, "title": "SiLQ: Simple Large Language Model Quantization-Aware Training", "cluster": 208, "x": 8.253875732421875, "y": 4.454692840576172}, {"id": 44915761, "title": "ClickHouse: Does OLAP Need an ORM?", "cluster": 13, "x": 6.881941795349121, "y": 3.8686392307281494}, {"id": 44915580, "title": "Using Large Language Models to Simulate History Taking for Medical Education", "cluster": 208, "x": 8.361573219299316, "y": 4.3248491287231445}, {"id": 44915023, "title": "New Local LLM Value King \u2013 MaxSun Arc Pro B60 Dual with 48GB VRAM for $1200", "cluster": 13, "x": 6.959157943725586, "y": 3.5318634510040283}, {"id": 44915060, "title": "Programming's New Frontier: The Rise of LLM-First Languages", "cluster": 13, "x": 6.6976189613342285, "y": 3.653700351715088}, {"id": 44914637, "title": "Open weight LLMs exhibit inconsistent performance across providers", "cluster": 13, "x": 6.904354095458984, "y": 3.389662742614746}, {"id": 44914633, "title": "Python library that shrinks text for LLMs by up to 80%", "cluster": 13, "x": 7.089494228363037, "y": 3.752152681350708}, {"id": 44914040, "title": "I let LLMs write an Elixir NIF in C; it mostly worked", "cluster": 13, "x": 6.7821431159973145, "y": 3.835556745529175}, {"id": 44913595, "title": "Problems in LLM Benchmarking and Evaluation", "cluster": 13, "x": 6.884566783905029, "y": 3.312824249267578}, {"id": 44913696, "title": "Q Evaluation Harness: open-source evals for LLMs on q/kdb+", "cluster": 13, "x": 6.888936519622803, "y": 3.6258115768432617}, {"id": 44912360, "title": "Raif v1.3.0 \u2013 Ruby on Rails engine for LLM dev, including evals and LLM-as-judge", "cluster": 13, "x": 6.874561309814453, "y": 3.6622438430786133}, {"id": 44912213, "title": "How LLMs Work", "cluster": 13, "x": 6.69172477722168, "y": 3.258279323577881}, {"id": 44912072, "title": "Malicious LLM-Based Conversational AI Makes Users Reveal Personal Information", "cluster": 12, "x": 7.0115132331848145, "y": 2.6168501377105713}, {"id": 44911315, "title": "Assume your LLMs are compromised", "cluster": 13, "x": 6.461178779602051, "y": 3.290562152862549}, {"id": 44908158, "title": "D2F \u2013 We made dLLMs 2.5x faster than LLaMA3", "cluster": 13, "x": 7.032245635986328, "y": 3.6861724853515625}, {"id": 44908102, "title": "Apple trained an LLM to teach itself good UI code in SwiftUI", "cluster": 13, "x": 6.792356014251709, "y": 3.6508259773254395}, {"id": 44907459, "title": "Reversing the alignment part of LLM training", "cluster": 13, "x": 6.8155012130737305, "y": 3.2171823978424072}, {"id": 44907181, "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "cluster": 208, "x": 8.173872947692871, "y": 4.424149036407471}, {"id": 44906947, "title": "Is MCP Just a WSDL Reboot for LLMs?", "cluster": 13, "x": 6.6088385581970215, "y": 3.351365089416504}, {"id": 44903965, "title": "AI's Serious Python Bias: Concerns of LLMs Preferring One Language", "cluster": 12, "x": 7.02589225769043, "y": 2.7180681228637695}, {"id": 44904097, "title": "L1B3RT4S Liberation Prompts for LLMs", "cluster": 13, "x": 6.73415994644165, "y": 3.374027967453003}, {"id": 44903414, "title": "Avatarl: Training language models from scratch with pure reinforcement learning", "cluster": 208, "x": 8.270401954650879, "y": 4.5279717445373535}, {"id": 44903170, "title": "LLM Copyright/Plagiarism filters trivially bypassed with 0% detection [pdf]", "cluster": 13, "x": 6.614250183105469, "y": 3.3639473915100098}, {"id": 44903013, "title": "Can LLMs replace on-call SREs?", "cluster": 13, "x": 6.627592086791992, "y": 3.3469369411468506}, {"id": 44901852, "title": "LLMs generate slop because they avoid surprises by design", "cluster": 13, "x": 6.639667987823486, "y": 3.2644495964050293}, {"id": 44900217, "title": "Handling long-running LLM streams in a stateful backend", "cluster": 13, "x": 6.754024982452393, "y": 3.5528132915496826}, {"id": 44900116, "title": "Why LLMs can't really build software", "cluster": 13, "x": 6.531039237976074, "y": 3.424247980117798}, {"id": 44899597, "title": "LLM Coding Integrity Breach", "cluster": 13, "x": 6.516247272491455, "y": 3.669603109359741}, {"id": 44898525, "title": "Notes on the Math of LLMs (PDF File Link)", "cluster": 13, "x": 6.894343852996826, "y": 3.341824769973755}, {"id": 44897046, "title": "LLM Hallucination Seems Like a Big Problem, Not a Mere Speedbump", "cluster": 13, "x": 6.587571620941162, "y": 2.9998292922973633}, {"id": 44897098, "title": "Convo-Lang: LLM Programming Language and Runtime", "cluster": 13, "x": 6.91817045211792, "y": 3.8112165927886963}, {"id": 44896818, "title": "Persuasion as a Form of Attack in LLMs", "cluster": 13, "x": 6.551578521728516, "y": 3.1447036266326904}, {"id": 44896499, "title": "Mathematical Computation and Reasoning Errors by Large Language Models", "cluster": 208, "x": 8.224115371704102, "y": 4.318127632141113}, {"id": 44893773, "title": "Evals as Code: CI for LLMs with Dagger", "cluster": 13, "x": 6.809681415557861, "y": 3.7053258419036865}, {"id": 44893178, "title": "CrankTest: A proposed benchmark of LLM sycophancy", "cluster": 13, "x": 6.951242923736572, "y": 3.4299216270446777}, {"id": 44893329, "title": "LLM Tokenization Demo", "cluster": 13, "x": 6.9078803062438965, "y": 3.667495012283325}, {"id": 44892775, "title": "MCP is an open protocol that standardizes how apps provide context to LLMs", "cluster": 13, "x": 6.8345184326171875, "y": 3.703167676925659}, {"id": 44891596, "title": "LLMs tell bad jokes because they avoid surprises", "cluster": 13, "x": 6.549493312835693, "y": 3.1257383823394775}, {"id": 44890710, "title": "Answering the BfDI's questions on personal data in LLMs", "cluster": 13, "x": 6.671712398529053, "y": 3.4554495811462402}, {"id": 44889769, "title": "Project NANDA \u2013 open protocols for a decentralised agentic web", "cluster": 33, "x": 8.585492134094238, "y": 3.8727431297302246}, {"id": 44889682, "title": "FilBench: Can LLMs Understand and Generate Filipino Language?", "cluster": 13, "x": 6.788516998291016, "y": 3.3947229385375977}, {"id": 44889206, "title": "Large Language Models Do Not Simulate Human Psychology", "cluster": 208, "x": 8.305733680725098, "y": 4.298279762268066}, {"id": 44888703, "title": "The Coding Personalities of Leading LLMs", "cluster": 13, "x": 6.6614532470703125, "y": 3.5507707595825195}, {"id": 44888210, "title": "DoubleAgents: Fine-Tuning LLMs for Covert Malicious Tool Calls", "cluster": 13, "x": 6.516089916229248, "y": 3.4572954177856445}, {"id": 44885577, "title": "FMLLM: 4mb training data, 100mb model, Fibonacci embeddings, near-coherent. WTF?", "cluster": 13, "x": 7.186738014221191, "y": 3.630714178085327}, {"id": 44885636, "title": "LLMs are not like you and me \u2013 and never will be", "cluster": 13, "x": 6.509700298309326, "y": 3.082415819168091}, {"id": 44883601, "title": "LLM Hallucination Seems Like a Big Problem, Not a Mere Speedbump", "cluster": 13, "x": 6.543120861053467, "y": 2.9799420833587646}, {"id": 44883237, "title": "LLMs as Parts of Systems", "cluster": 13, "x": 6.771927833557129, "y": 3.3926546573638916}, {"id": 44882357, "title": "Writing an LLM from scratch, part 17 \u2013 the feed-forward network", "cluster": 13, "x": 6.82501745223999, "y": 3.231255054473877}, {"id": 44882202, "title": "Self-host open-source LLM agent sandbox on your own cloud", "cluster": 13, "x": 6.8138508796691895, "y": 3.7042276859283447}, {"id": 44880875, "title": "OpenBench: Provider-agnostic, open-source evaluation infrastructure for LLMs", "cluster": 13, "x": 6.883991241455078, "y": 3.677638053894043}, {"id": 44879828, "title": "ML System Design: 450 Case Studies to Learn From", "cluster": 13, "x": 7.046916961669922, "y": 3.4512088298797607}, {"id": 44879676, "title": "GPT-OSS Reasoning and Any-LLM: Nuances of OpenAI API Compatibility", "cluster": 13, "x": 7.106058597564697, "y": 3.802125930786133}, {"id": 44879051, "title": "How to deal with streaming LLM structured output", "cluster": 13, "x": 6.9125494956970215, "y": 3.649958848953247}, {"id": 44878755, "title": "TextQuests: How Good Are LLMs at Text-Based Video Games?", "cluster": 13, "x": 6.7763471603393555, "y": 3.2424819469451904}, {"id": 44878149, "title": "Provably guarantee correctness of (some of) your LLM outputs", "cluster": 13, "x": 6.778656482696533, "y": 3.3130600452423096}, {"id": 44878290, "title": "Can modern LLMs count the number of b's in \"blueberry\"?", "cluster": 13, "x": 6.691925525665283, "y": 3.204274892807007}, {"id": 44877735, "title": "Sink or Schwim \u2013 An LSEG Saga", "cluster": 13, "x": 6.554335594177246, "y": 3.1012678146362305}, {"id": 44877435, "title": "The Brokk Power Ranking LLM Coding Benchmark", "cluster": 13, "x": 6.760322570800781, "y": 3.4821813106536865}, {"id": 44877404, "title": "Evaluating LLMs playing text adventures", "cluster": 13, "x": 6.750927925109863, "y": 3.2173545360565186}, {"id": 44876271, "title": "LLM setting a role bypassing restrictions", "cluster": 13, "x": 6.5528764724731445, "y": 3.2164669036865234}, {"id": 44875992, "title": "Training language models to be warm and empathetic makes them less reliable", "cluster": 208, "x": 8.353041648864746, "y": 4.28044319152832}, {"id": 44875834, "title": "Serve ML Models on the edge over browsers", "cluster": 13, "x": 7.193868637084961, "y": 3.883728265762329}, {"id": 44875586, "title": "Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "cluster": 13, "x": 6.8360819816589355, "y": 3.129197597503662}, {"id": 44874850, "title": "LLM Context Rot", "cluster": 13, "x": 6.700202941894531, "y": 3.5382258892059326}, {"id": 44874578, "title": "No AGI in Sight: What This Means for LLMs", "cluster": 13, "x": 6.585022926330566, "y": 3.1259005069732666}, {"id": 44872850, "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "cluster": 13, "x": 6.813183784484863, "y": 3.0391669273376465}, {"id": 44872381, "title": "LLM 0.27: GPT-5 and improved tool calling", "cluster": 13, "x": 6.928255558013916, "y": 3.7495086193084717}, {"id": 44872097, "title": "Simulating the U.S. Senate: An LLM-Driven Agent Approach (2024)", "cluster": 13, "x": 6.886152267456055, "y": 3.1819889545440674}, {"id": 44871530, "title": "LLM prompts as versioned, composable software artifacts", "cluster": 13, "x": 6.8551411628723145, "y": 3.74221134185791}, {"id": 44871300, "title": "LLM chat that survives bad Wi-Fi", "cluster": 13, "x": 6.588797092437744, "y": 3.4268267154693604}, {"id": 44870795, "title": "Is a \"routing\" LLM isomorphic with a single large model?", "cluster": 13, "x": 6.85817813873291, "y": 3.3358817100524902}, {"id": 44870056, "title": "Internal tool reduced our LLM (ChatGPT/Claude) costs by 70% for document Q&A", "cluster": 13, "x": 6.9095330238342285, "y": 3.6439762115478516}, {"id": 44869835, "title": "LLMs' \"simulated reasoning\" abilities are a \"brittle mirage,\" researchers find", "cluster": 13, "x": 6.831170558929443, "y": 3.0385730266571045}, {"id": 44869378, "title": "What Is LLM Tokenization and Why Is It Important?", "cluster": 13, "x": 6.835879325866699, "y": 3.2761778831481934}, {"id": 44869102, "title": "Human Data Is (Probably) More Expensive Than Compute for Training Frontier LLMs", "cluster": 13, "x": 6.997532367706299, "y": 3.396451234817505}, {"id": 44867097, "title": "Why deterministic output from LLMs is nearly impossible", "cluster": 13, "x": 6.6930460929870605, "y": 3.1516311168670654}, {"id": 44866783, "title": "Understanding Protein Language Models Series", "cluster": 208, "x": 8.386748313903809, "y": 4.452037334442139}, {"id": 44865214, "title": "Riddler Bench: Lateral Thinking LLM Benchmark", "cluster": 13, "x": 6.907844066619873, "y": 3.218461751937866}, {"id": 44865028, "title": "LangDiff: Break Limitations of LLM JSON Streaming", "cluster": 13, "x": 6.862687110900879, "y": 3.6594369411468506}, {"id": 44864711, "title": "Dynamic Client-Side Chart Generation with LLMs and Plotly", "cluster": 13, "x": 6.933965682983398, "y": 3.645474433898926}, {"id": 44863609, "title": "How Well Do LLMs Perform on a Raspberry Pi 5?", "cluster": 13, "x": 6.938089847564697, "y": 3.5119030475616455}, {"id": 44863713, "title": "LLM Coding Assistant Census", "cluster": 13, "x": 7.001856327056885, "y": 3.5140581130981445}, {"id": 44862887, "title": "Tips for Safe LLM Development", "cluster": 13, "x": 6.586672782897949, "y": 3.287423610687256}, {"id": 44862145, "title": "OpenAI GPT-OSS LLMs use MXFP4 floating point data type: smaller, faster, cheaper", "cluster": 13, "x": 7.198655128479004, "y": 3.8733901977539062}, {"id": 44861077, "title": "Improving Your LLM Agent with Reinforcement Learning", "cluster": 13, "x": 6.945965766906738, "y": 3.2353837490081787}, {"id": 44860939, "title": "LLMs in Programming", "cluster": 13, "x": 6.61683988571167, "y": 3.6073644161224365}, {"id": 44860519, "title": "Training LLMs over the Internet", "cluster": 13, "x": 6.674388885498047, "y": 3.5203351974487305}, {"id": 44859010, "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage?", "cluster": 13, "x": 6.761387825012207, "y": 3.027230978012085}, {"id": 44858671, "title": "Design Patterns for Securing LLM Agents Against Prompt Injections", "cluster": 13, "x": 6.277958869934082, "y": 3.294607639312744}, {"id": 44858410, "title": "Recent cross-research on LLM and RL on ArXiv", "cluster": 13, "x": 6.862240791320801, "y": 3.304382085800171}, {"id": 44858330, "title": "Physics of Language Models", "cluster": 208, "x": 8.426822662353516, "y": 4.514542579650879}, {"id": 44858090, "title": "Knowledge Tagging with Large Language Model Based Multi-Agent System", "cluster": 208, "x": 8.162400245666504, "y": 4.332915306091309}, {"id": 44856625, "title": "HoML: Hosting your own LLM at home", "cluster": 13, "x": 6.6342363357543945, "y": 3.463852882385254}, {"id": 44856376, "title": "The Identity Crisis: Why LLMs Don't Know Who They Are", "cluster": 13, "x": 6.533156394958496, "y": 3.1029281616210938}, {"id": 44856101, "title": "Diffusion language models are super data learners", "cluster": 208, "x": 8.32430648803711, "y": 4.292548179626465}, {"id": 44855060, "title": "Design Patterns for Securing LLM Agents Against Prompt Injections", "cluster": 13, "x": 6.284955978393555, "y": 3.286958932876587}, {"id": 44854918, "title": "Avatarl: Training langauge models from scratch with pure RL", "cluster": 208, "x": 8.20914077758789, "y": 4.530736923217773}, {"id": 44854721, "title": "Does Prompt Formatting Have Any Impact on LLM Performance?", "cluster": 13, "x": 6.842867851257324, "y": 3.3341305255889893}, {"id": 44854518, "title": "LLMs aren't world models", "cluster": 13, "x": 6.5282511711120605, "y": 3.080571174621582}, {"id": 44854456, "title": "Seeing Like an LLM", "cluster": 13, "x": 6.569319248199463, "y": 3.072122097015381}, {"id": 44853969, "title": "What's your favorite CLI tool for integrating LLMs into your terminal workflow?", "cluster": 13, "x": 6.831918239593506, "y": 3.770545244216919}, {"id": 44854107, "title": "All you need to know about Tokenization in LLMs", "cluster": 13, "x": 6.796873569488525, "y": 3.2496492862701416}, {"id": 44853831, "title": "LLM advises to delete the Linux dynamic linker during a troubleshooting session", "cluster": 13, "x": 6.808988094329834, "y": 3.5851051807403564}, {"id": 44853362, "title": "David Chalmers: Could a Large Language Model Be Conscious?", "cluster": 208, "x": 8.329717636108398, "y": 4.266174793243408}, {"id": 44852610, "title": "Design Patterns for Securing LLM Agents Against Prompt Injections", "cluster": 13, "x": 6.292484760284424, "y": 3.2920570373535156}, {"id": 44852536, "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data", "cluster": 13, "x": 6.9133782386779785, "y": 3.3026063442230225}, {"id": 44852533, "title": "LLM Evals Are Just Tests. Why Are We Making This So Complicated?", "cluster": 13, "x": 6.73980188369751, "y": 3.188836097717285}, {"id": 44852298, "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "cluster": 13, "x": 6.794995307922363, "y": 3.077507972717285}, {"id": 44851807, "title": "Apple researchers taught an LLM to predict tokens up to 5x faster", "cluster": 13, "x": 7.003951072692871, "y": 3.228627920150757}, {"id": 44851875, "title": "Avatarl: Training language models from scratch with pure reinforcement learning", "cluster": 208, "x": 8.261679649353027, "y": 4.525154113769531}, {"id": 44851359, "title": "Your LLM Knows the Future", "cluster": 13, "x": 6.597710609436035, "y": 3.0082132816314697}, {"id": 44851317, "title": "Roo Code Workflow: An Advanced LLM-Powered Development Setup", "cluster": 13, "x": 6.780558109283447, "y": 3.7537689208984375}, {"id": 44849905, "title": "Anyclaude \u2013 Claude Code with Any LLM", "cluster": 13, "x": 6.8706488609313965, "y": 3.87697434425354}, {"id": 44849129, "title": "Ch.at \u2013 A lightweight LLM chat service accessible through HTTP, SSH, DNS and API", "cluster": 13, "x": 6.767914295196533, "y": 3.759246349334717}, {"id": 44848936, "title": "MCP vs. SDK: Two Paths to LLM-Powered Extensibility", "cluster": 13, "x": 6.801079750061035, "y": 3.5951972007751465}, {"id": 44848708, "title": "Can an LLM recognize the same user across different devices/accounts?", "cluster": 13, "x": 6.494137763977051, "y": 3.4912655353546143}, {"id": 44848511, "title": "Could a New Type of Parallelism Speed Up LLM Inference? \u2013 EE Times", "cluster": 13, "x": 7.1302056312561035, "y": 3.375070571899414}, {"id": 44848528, "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive LLM Decoding", "cluster": 13, "x": 7.015529155731201, "y": 3.3029696941375732}, {"id": 44847789, "title": "SortBench: Benchmarking LLMs based on their ability to sort lists", "cluster": 13, "x": 6.946609973907471, "y": 3.412367582321167}, {"id": 44847741, "title": "The current state of LLM-driven development", "cluster": 13, "x": 6.691009998321533, "y": 3.4314072132110596}, {"id": 44847714, "title": "Extension that allows your Chrome browser to share context with LLMs", "cluster": 13, "x": 6.776421546936035, "y": 3.681180953979492}, {"id": 44847155, "title": "Expediting On-Device LLM Personalization via Explainable Model Selection", "cluster": 13, "x": 6.8737945556640625, "y": 3.445408582687378}, {"id": 44847004, "title": "Diffusion Language Models Are Super Data Learners", "cluster": 208, "x": 8.326077461242676, "y": 4.309519290924072}, {"id": 44846962, "title": "Avatarl: Training language models from scratch with pure reinforcement learning", "cluster": 208, "x": 8.300028800964355, "y": 4.525247097015381}, {"id": 44846882, "title": "Running a Reliable Service on LLMs", "cluster": 13, "x": 6.7353692054748535, "y": 3.4381351470947266}, {"id": 44845973, "title": "Yet Another LLM Rant", "cluster": 13, "x": 6.554529666900635, "y": 3.1219146251678467}, {"id": 44845710, "title": "Automating LLM agent mastery for any MCP server with MCP-RL and ART", "cluster": 13, "x": 6.982454299926758, "y": 3.3498728275299072}, {"id": 44845397, "title": "Living with LLMs", "cluster": 13, "x": 6.559035301208496, "y": 3.12465238571167}, {"id": 44845147, "title": "Arch-Router: Aligning LLM Routing with Human Preferences", "cluster": 13, "x": 6.85778284072876, "y": 3.5656490325927734}, {"id": 44844934, "title": "Apple taught an LLM to predict tokens up to 5x faster in math and coding tasks", "cluster": 13, "x": 6.960474967956543, "y": 3.2272326946258545}, {"id": 44844561, "title": "Tribblix \u2013 The Retro Illumos Distribution", "cluster": 13, "x": 7.189854145050049, "y": 3.5365118980407715}, {"id": 44842965, "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "cluster": 13, "x": 6.81471061706543, "y": 3.0239734649658203}, {"id": 44842754, "title": "NextCoder by Microsoft \u2014 LLM performing on par with GPT-4o on complex benchmarks", "cluster": 13, "x": 6.976205348968506, "y": 3.599128484725952}, {"id": 44842625, "title": "AWorld: Build, evaluate and train General Multi-Agent Assistance with ease", "cluster": 38, "x": 8.649076461791992, "y": 3.659878730773926}, {"id": 44842545, "title": "What's Your Favorite LLM \u2013and Why?", "cluster": 13, "x": 6.5615434646606445, "y": 3.0655758380889893}, {"id": 44840827, "title": "A framework for AI-to-AI dialogue to solve the context problem in LLMs", "cluster": 12, "x": 7.017511367797852, "y": 2.6537230014801025}, {"id": 44839792, "title": "EXMS86: EMS-Backed XMS", "cluster": 13, "x": 7.039432525634766, "y": 3.923438787460327}, {"id": 44839941, "title": "Semi-incremental Markdown renderer for LLM", "cluster": 13, "x": 6.900158405303955, "y": 3.681370258331299}, {"id": 44839459, "title": "I clustered four Framework Mainboards to test LLMs", "cluster": 13, "x": 6.918176651000977, "y": 3.543891191482544}, {"id": 44838987, "title": "Llmswap v1.5.0 \u2013 Added IBM watsonx support to my multi-LLM Python library", "cluster": 13, "x": 6.975060939788818, "y": 3.815291404724121}, {"id": 44838894, "title": "An LLM Codegen Hero's Journey", "cluster": 13, "x": 6.681643486022949, "y": 3.642791271209717}, {"id": 44838454, "title": "Which LLM model has human ethical instincts?", "cluster": 13, "x": 6.636823654174805, "y": 3.116957187652588}, {"id": 44837714, "title": "The Pink Elephant Problem: Why \"Don't Do That\" Fails with LLMs", "cluster": 13, "x": 6.620656967163086, "y": 3.1167662143707275}, {"id": 44837601, "title": "Open SWE by LangChain", "cluster": 13, "x": 6.979019641876221, "y": 3.787245512008667}, {"id": 44836233, "title": "Small LLMs can outperform GPT-4 at detecting jailbreaks", "cluster": 13, "x": 6.673502445220947, "y": 3.571803092956543}, {"id": 44834918, "title": "How attention sinks keep language models stable", "cluster": 208, "x": 8.267374992370605, "y": 4.417516231536865}, {"id": 44834516, "title": "Tokenization in Large Language Models", "cluster": 208, "x": 8.221918106079102, "y": 4.384040832519531}, {"id": 44831947, "title": "LMRouter: Run Claude Code on Any LLM with One Command", "cluster": 13, "x": 6.858280658721924, "y": 3.902621269226074}, {"id": 44830582, "title": "Reinforcement Learning for Reasoning in LLMs with One Training Example", "cluster": 13, "x": 6.9201507568359375, "y": 3.2062203884124756}, {"id": 44827972, "title": "Llmswap \u2013 Switch between OpenAI, Claude, Gemini, Ollama with one line of code", "cluster": 13, "x": 7.017237186431885, "y": 4.15115213394165}, {"id": 44827898, "title": "Lightweight LSAT", "cluster": 13, "x": 6.837508201599121, "y": 3.297645330429077}, {"id": 44827719, "title": "LLMs fall for the decoy effect too", "cluster": 13, "x": 6.47108793258667, "y": 3.1209185123443604}, {"id": 44827527, "title": "Pushing the limits of long-context LLM training for 1M-token+ medical records", "cluster": 13, "x": 6.892734527587891, "y": 3.315519332885742}, {"id": 44825780, "title": "Active context extraction > passive context capture with LLMs", "cluster": 13, "x": 6.849898338317871, "y": 3.648878335952759}, {"id": 44825572, "title": "I clustered four Framework Mainboards to test LLMs", "cluster": 13, "x": 6.934197425842285, "y": 3.5759785175323486}, {"id": 44825079, "title": "Aligning LLMs to Ask Good Questions a Case Study in Clinical Reasoning", "cluster": 13, "x": 6.673877239227295, "y": 3.1089563369750977}, {"id": 44823801, "title": "We open-sourced a 'Semantic Clinic' for LLM bugs \u2013 16 root causes", "cluster": 13, "x": 6.772871494293213, "y": 3.7084901332855225}, {"id": 44823850, "title": "An LLM does not need to understand MCP", "cluster": 13, "x": 6.6014723777771, "y": 3.1563384532928467}, {"id": 44822953, "title": "OpenAI's open LLMs can't believe that Trump is president", "cluster": 13, "x": 6.6336283683776855, "y": 3.1740670204162598}, {"id": 44822322, "title": "Book: The inner workings of Large Language Models", "cluster": 208, "x": 8.287924766540527, "y": 4.348177909851074}, {"id": 44821146, "title": "Cultural Bias in LLMs", "cluster": 13, "x": 6.565298557281494, "y": 3.03796124458313}, {"id": 44820784, "title": "Bitfrost \u2013 LLM gateway 90x faster than Litellm at p99", "cluster": 13, "x": 7.0028395652771, "y": 3.724515438079834}, {"id": 44820619, "title": "Actual LLM agents are coming", "cluster": 13, "x": 6.739947319030762, "y": 3.180051565170288}, {"id": 44820486, "title": "Your LLM Does Not Care About MCP", "cluster": 13, "x": 6.595117568969727, "y": 3.214583158493042}, {"id": 44820473, "title": "Run LLM's Locally on iPhone", "cluster": 13, "x": 6.8646039962768555, "y": 3.7215940952301025}, {"id": 44819717, "title": "The Mac-vs-PC Story Playing Out in CLI Agents", "cluster": 3, "x": 7.509161949157715, "y": 3.234816551208496}, {"id": 44817612, "title": "LLMs Can't Navigate Through the Physical World", "cluster": 13, "x": 6.610806941986084, "y": 3.0726919174194336}, {"id": 44816505, "title": "A Survey of Context Engineering for Large Language Models", "cluster": 208, "x": 8.20246696472168, "y": 4.518493175506592}, {"id": 44815396, "title": "LLMs have loss aversion too", "cluster": 13, "x": 6.553615093231201, "y": 3.062650442123413}, {"id": 44815125, "title": "Blocking LLMs from your website cuts you off from next-generation search", "cluster": 13, "x": 6.719853401184082, "y": 3.5282986164093018}, {"id": 44815098, "title": "VPS Evangelism and Building LLM-over-DNS", "cluster": 13, "x": 6.660373210906982, "y": 3.6851134300231934}, {"id": 44814767, "title": "Can you hack this LLM?", "cluster": 13, "x": 6.545590400695801, "y": 3.4245645999908447}, {"id": 44813893, "title": "Pydantic/GenAI-prices \u2013 Calculate prices for calling LLM inference APIs", "cluster": 13, "x": 7.041414260864258, "y": 3.62751841545105}, {"id": 44813298, "title": "LLM over DNS", "cluster": 13, "x": 6.815603733062744, "y": 3.7290637493133545}, {"id": 44813218, "title": "How do LLMs validate and verify their output?", "cluster": 13, "x": 6.785435199737549, "y": 3.2977774143218994}, {"id": 44812250, "title": "The Headaches of LLM Inference for App Developers", "cluster": 13, "x": 6.733762264251709, "y": 3.334804058074951}, {"id": 44810307, "title": "LLM Inflation", "cluster": 13, "x": 6.864577293395996, "y": 3.245035171508789}, {"id": 44808784, "title": "RamaLama \u2013 Local LLMs with accelerated container images", "cluster": 13, "x": 7.0297040939331055, "y": 3.7410693168640137}, {"id": 44808368, "title": "The wall confronting large language models", "cluster": 208, "x": 8.14013671875, "y": 4.391258716583252}, {"id": 44805605, "title": "Get fancy ls output with LSD and this script", "cluster": 13, "x": 6.994714260101318, "y": 3.955324649810791}, {"id": 44803771, "title": "Estimating worst case frontier risks of open weight LLMs", "cluster": 13, "x": 6.83933162689209, "y": 3.2147252559661865}, {"id": 44801923, "title": "Skynet: LLMs controlling real robots and drones with Bash", "cluster": 13, "x": 6.761104583740234, "y": 3.337670087814331}, {"id": 44801432, "title": "LLMs Anchor Too", "cluster": 13, "x": 6.675355911254883, "y": 3.328636884689331}, {"id": 44801450, "title": "LLVM optimizes power sums with scalar evolution", "cluster": 13, "x": 7.049002170562744, "y": 3.4094274044036865}, {"id": 44801449, "title": "Estimating Worst-Case Frontier Risks of Open-Weight LLMs [pdf]", "cluster": 13, "x": 6.942965984344482, "y": 3.2715036869049072}, {"id": 44800221, "title": "Learn How to Specialize Your LLM \u2013 Free LLM Fine-Tuning Course", "cluster": 13, "x": 6.7058844566345215, "y": 3.2987704277038574}, {"id": 44798977, "title": "One Does Not Simply Meme Alone: LLMs and Humans in the Generation of Humor", "cluster": 13, "x": 6.588447093963623, "y": 2.9925875663757324}, {"id": 44798541, "title": "We trained LLM to find reentrancy vulnerabilities in smart contracts", "cluster": 13, "x": 6.527735233306885, "y": 3.5041561126708984}, {"id": 44797917, "title": "Lack of intent is what makes reading LLM-generated text exhausting", "cluster": 13, "x": 6.60399055480957, "y": 3.1547348499298096}, {"id": 44797728, "title": "Help me understand what this guy means by LLMs and language?", "cluster": 13, "x": 6.632852554321289, "y": 3.1696977615356445}, {"id": 44797161, "title": "Attention was never enough: Tracing the rise of hybrid LLMs", "cluster": 13, "x": 6.650987148284912, "y": 3.1056325435638428}, {"id": 44796187, "title": "Living with LLMs", "cluster": 13, "x": 6.563580513000488, "y": 3.133484363555908}, {"id": 44796040, "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "cluster": 13, "x": 6.944859504699707, "y": 3.490497589111328}, {"id": 44794336, "title": "New terminal-based LLM coding tool", "cluster": 13, "x": 6.840991497039795, "y": 3.780590295791626}, {"id": 44793171, "title": "The Super Weight in Large Language Models", "cluster": 208, "x": 8.124150276184082, "y": 4.336320400238037}, {"id": 44792903, "title": "Unit Testing for LLM Evaluations", "cluster": 13, "x": 6.887179851531982, "y": 3.2897751331329346}, {"id": 44792686, "title": "Language Models Improve When Pretraining Data Matches Target Tasks", "cluster": 208, "x": 8.27476692199707, "y": 4.537281036376953}, {"id": 44792531, "title": "Lumo Privacy and Security Model", "cluster": 13, "x": 6.579916000366211, "y": 3.429933547973633}, {"id": 44792249, "title": "Pre-Training Large Memory Language Models with Internal and External Knowledge", "cluster": 208, "x": 8.21269416809082, "y": 4.324053764343262}, {"id": 44791697, "title": "You can run small HuggingFace LLMs on iPhone", "cluster": 13, "x": 6.886062145233154, "y": 3.749603033065796}, {"id": 44790540, "title": "I'm comparing human vs. LLM decision-making", "cluster": 13, "x": 6.691222190856934, "y": 3.0551581382751465}, {"id": 44790316, "title": "Getting an LLM to Play Text Adventures", "cluster": 13, "x": 6.678142547607422, "y": 3.305392265319824}, {"id": 44790112, "title": "How I Use LLMs for Data Harmonization: A Strategic, Limited Approach", "cluster": 13, "x": 6.874385356903076, "y": 3.4084794521331787}, {"id": 44790044, "title": "LLMs as Context Synthesizers: Why Direct Instructions Don't Work", "cluster": 13, "x": 6.716519355773926, "y": 3.5413904190063477}, {"id": 44789491, "title": "Trends in LLM-Generated Citations on ArXiv", "cluster": 13, "x": 6.8266987800598145, "y": 3.2686314582824707}, {"id": 44788646, "title": "LLMs Aren't Just for Sissies", "cluster": 13, "x": 6.570568084716797, "y": 3.155925989151001}, {"id": 44787611, "title": "Fine-tuned small LLMs can beat large ones with programmatic data curation", "cluster": 13, "x": 6.949016571044922, "y": 3.413522720336914}, {"id": 44787517, "title": "Anthropic beats OpenAI as the top LLM provider for business \u2013 and it's not close", "cluster": 13, "x": 6.811963081359863, "y": 3.421741485595703}, {"id": 44787510, "title": "Luzer, a coverage-guided Lua fuzzing engine", "cluster": 13, "x": 6.797959804534912, "y": 3.80454421043396}, {"id": 44787279, "title": "Speaking in \"LLM Idioms\"", "cluster": 13, "x": 6.625891208648682, "y": 3.2204248905181885}, {"id": 44787055, "title": "The Super Weight in Large Language Models", "cluster": 208, "x": 8.127371788024902, "y": 4.355788230895996}, {"id": 44785257, "title": "Agent Network Protocol", "cluster": 36, "x": 8.562911987304688, "y": 3.662533760070801}, {"id": 44783631, "title": "AIClient-2-API, a Practical Solution to Reduce LLM Usage Costs", "cluster": 13, "x": 6.886231422424316, "y": 3.648639678955078}, {"id": 44782815, "title": "LLM Dorking Precision hacking of prompts", "cluster": 13, "x": 6.495162010192871, "y": 3.488025426864624}, {"id": 44782536, "title": "Git Context Controller: Manage the Context of LLM-Based Agents Like Git", "cluster": 13, "x": 6.968526363372803, "y": 3.582186460494995}, {"id": 44781532, "title": "LLMs won't save you from bad software engineering culture", "cluster": 13, "x": 6.511612892150879, "y": 3.4429221153259277}, {"id": 44780494, "title": "Private and HIPAA Compliant LLM Summarizer for Apple Health's Medical Records", "cluster": 13, "x": 6.7090020179748535, "y": 3.4830799102783203}, {"id": 44780066, "title": "Different types of ML roles and teams at FAANG, and what they entail", "cluster": 13, "x": 7.042497158050537, "y": 3.210294246673584}, {"id": 44779238, "title": "Why do LLMs still not run code before giving it to you?", "cluster": 13, "x": 6.7128214836120605, "y": 3.75514554977417}, {"id": 44778841, "title": "Souper \u2013 A Superoptimizer for LLVM IR", "cluster": 13, "x": 6.9926347732543945, "y": 3.7070980072021484}, {"id": 44777760, "title": "Persona vectors: Monitoring and controlling character traits in language models", "cluster": 208, "x": 8.349710464477539, "y": 4.476292133331299}, {"id": 44777493, "title": "Supporting Diverse ML Systems at Netflix", "cluster": 13, "x": 7.061762809753418, "y": 3.631847858428955}, {"id": 44777459, "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "cluster": 13, "x": 7.0401153564453125, "y": 3.3286983966827393}, {"id": 44777350, "title": "Limits and Strengths of LLMs in Engineering", "cluster": 13, "x": 6.732115745544434, "y": 3.2695629596710205}, {"id": 44776840, "title": "Fatigue, anxiety, pain? They might be MS in disguise", "cluster": 529, "x": 6.677732944488525, "y": 3.0092103481292725}, {"id": 44776800, "title": "Use local LLM to neutralise the headers on the web", "cluster": 13, "x": 6.778636932373047, "y": 3.6596124172210693}, {"id": 44775301, "title": "Metaheuristics and Large Language Models Join Forces", "cluster": 208, "x": 8.206500053405762, "y": 4.358810901641846}, {"id": 44774559, "title": "How Many Instructions Can LLMs Follow at Once?", "cluster": 13, "x": 6.72850227355957, "y": 3.1994287967681885}, {"id": 44774182, "title": "Testing LLM Responses: A Fast, Cost-Effective Alternative to LLM-as-Judge", "cluster": 13, "x": 6.8404011726379395, "y": 3.342419147491455}, {"id": 44771836, "title": "Arch-Router: Aligning LLM Routing with Human Preferences", "cluster": 13, "x": 6.858530044555664, "y": 3.5669617652893066}, {"id": 44770634, "title": "Using LLM Embeddings to Normalize User Data", "cluster": 13, "x": 6.949866771697998, "y": 3.5225746631622314}, {"id": 44770541, "title": "MaskLLM for LLM API Key Rotation", "cluster": 13, "x": 6.875234127044678, "y": 3.5858089923858643}, {"id": 44769980, "title": "People harm others with the help of AI/LLMs", "cluster": 12, "x": 6.93170690536499, "y": 2.63970685005188}, {"id": 44769667, "title": "Chinese LLMs talk freely about Tiananmen massacre and Taiwan", "cluster": 13, "x": 6.517014503479004, "y": 3.0905466079711914}, {"id": 44766387, "title": "Gave Claude and other LLMs native agentic computer control (Mac and PC)", "cluster": 13, "x": 7.133773326873779, "y": 3.3082644939422607}, {"id": 44766085, "title": "The Whitewashing of Jazz (a dream that seems like an LLM)", "cluster": 13, "x": 6.528275489807129, "y": 3.033176898956299}, {"id": 44765456, "title": "Persona vectors: Monitoring and controlling character traits in language models", "cluster": 208, "x": 8.342337608337402, "y": 4.479964256286621}, {"id": 44762678, "title": "LLM Inference Benchmark Hub", "cluster": 13, "x": 7.213701248168945, "y": 3.436577558517456}, {"id": 44761243, "title": "Persona vectors: Monitoring and controlling character traits in language models", "cluster": 208, "x": 8.340853691101074, "y": 4.4721550941467285}, {"id": 44756550, "title": "Why LLMs Struggle with Text-to-SQL", "cluster": 13, "x": 6.847723007202148, "y": 3.308279037475586}, {"id": 44755879, "title": "TinyTroupe: An LLM-Powered Multiagent Persona Simulation Toolkit (OSS Paper)", "cluster": 13, "x": 6.966446399688721, "y": 3.438455581665039}, {"id": 44755558, "title": "Google Agentspace is GA now", "cluster": 35, "x": 8.458919525146484, "y": 3.851530075073242}, {"id": 44755712, "title": "Our first outage from LLM-written code", "cluster": 13, "x": 6.509580612182617, "y": 3.7511863708496094}, {"id": 44754435, "title": "Code with LLMs and a Plan", "cluster": 13, "x": 6.721113681793213, "y": 3.590695381164551}, {"id": 44753345, "title": "Building Personalized Micro Agents", "cluster": 38, "x": 8.66733455657959, "y": 3.603024482727051}, {"id": 44752546, "title": "LLM leaderboard \u2013 Comparing models from OpenAI, Google, DeepSeek and others", "cluster": 13, "x": 6.920199871063232, "y": 3.467768907546997}, {"id": 44751328, "title": "Mythbusting Large Language Models", "cluster": 208, "x": 8.21462631225586, "y": 4.322686672210693}, {"id": 44750272, "title": "Vercel Adapts Their SEO Strategy for LLM Visibility", "cluster": 13, "x": 6.7913055419921875, "y": 3.381629467010498}, {"id": 44749412, "title": "Provider-agnostic, open-source evaluation infra for LLMs", "cluster": 13, "x": 6.930821418762207, "y": 3.6249923706054688}, {"id": 44749063, "title": "Intuitive explanation of LLM Transformers without math", "cluster": 13, "x": 6.7635674476623535, "y": 3.1958062648773193}, {"id": 44749051, "title": "Our first outage from LLM-written code", "cluster": 13, "x": 6.520851135253906, "y": 3.7321505546569824}, {"id": 44747750, "title": "Understanding why deterministic output from LLMs is nearly impossible", "cluster": 13, "x": 6.732639312744141, "y": 3.2091546058654785}, {"id": 44747320, "title": "Transformers at the Edge: Efficient LLM Deployment", "cluster": 13, "x": 6.870567798614502, "y": 3.570862054824829}, {"id": 44746629, "title": "Mapping political connections with web automation, LLMs and Obsidian", "cluster": 13, "x": 6.788850784301758, "y": 3.5616202354431152}, {"id": 44746438, "title": "Do LLMs consider security? Empirical study on responses to programming questions", "cluster": 13, "x": 6.4971418380737305, "y": 3.3819432258605957}, {"id": 44746347, "title": "Do LLMs identify fonts?", "cluster": 13, "x": 6.801707744598389, "y": 3.520923137664795}, {"id": 44745636, "title": "LLM DeepSeek on a par with proprietary models in clinical decision making", "cluster": 13, "x": 6.8570170402526855, "y": 3.293682098388672}, {"id": 44745386, "title": "Agentic CRM", "cluster": 13, "x": 6.963682651519775, "y": 3.2258846759796143}, {"id": 44743393, "title": "Getting Started with Jax for ML", "cluster": 13, "x": 6.967141628265381, "y": 3.504101514816284}, {"id": 44743437, "title": "AWS and Meta launching startup program for open-source LLMs (Llama)", "cluster": 13, "x": 6.854848384857178, "y": 3.762698173522949}, {"id": 44742976, "title": "We built a browser extension for local LLMs with Ollama", "cluster": 13, "x": 6.750854015350342, "y": 3.7056121826171875}, {"id": 44742818, "title": "Harnessing Large Language Models to Overcome Challenges in Recommender Systems", "cluster": 208, "x": 8.148029327392578, "y": 4.412745952606201}, {"id": 44742187, "title": "Deploying Large Language Models with Retrieval Augmented Generation (2024)", "cluster": 208, "x": 8.22079086303711, "y": 4.535390853881836}, {"id": 44740953, "title": "Using Large Language Models as Neural Computers", "cluster": 208, "x": 8.2385892868042, "y": 4.392460823059082}, {"id": 44740857, "title": "LLM Optimized for Creative Reasoning", "cluster": 13, "x": 6.890921592712402, "y": 3.321990728378296}, {"id": 44739772, "title": "Introducing TRMNL X", "cluster": 13, "x": 7.443841934204102, "y": 3.7986087799072266}, {"id": 44739850, "title": "Trmnl X", "cluster": 13, "x": 7.3720245361328125, "y": 3.7871294021606445}, {"id": 44737517, "title": "Locally Sandboxed Background Agents in Zed", "cluster": 38, "x": 8.604947090148926, "y": 3.7789134979248047}, {"id": 44736787, "title": "Can you trust your friendly neighborhood LLM?", "cluster": 13, "x": 6.4893903732299805, "y": 3.2025818824768066}, {"id": 44736238, "title": "The best available open weight LLMs now come from China", "cluster": 13, "x": 6.833289623260498, "y": 3.4102680683135986}, {"id": 44736248, "title": "TLS and QUIC: A Masochist's Guide", "cluster": 13, "x": 6.8806986808776855, "y": 3.331249237060547}, {"id": 44736102, "title": "The role of LLMs in academic reviewing", "cluster": 13, "x": 6.749587059020996, "y": 3.1904184818267822}, {"id": 44735774, "title": "The Role of LLMs in Academic Reviewing (ACM SIGOPS / SIGARCH Blog)", "cluster": 13, "x": 6.824320316314697, "y": 3.270308494567871}, {"id": 44734930, "title": "Graph-R1: Towards Agentic GraphRAG Framework via End-to-End RL", "cluster": 38, "x": 8.678611755371094, "y": 3.717081308364868}, {"id": 44732636, "title": "LLM assisted language learning using Babbelaar", "cluster": 13, "x": 6.786956310272217, "y": 3.461970329284668}, {"id": 44732490, "title": "The Reanimated Renaissance, C-Speed Compression, and JSON-Loving LLMs", "cluster": 13, "x": 6.723381519317627, "y": 3.4162731170654297}, {"id": 44732384, "title": "Lawfulness of the mass processing of publicly accessible data to train LLMs", "cluster": 13, "x": 6.709654331207275, "y": 3.4396374225616455}, {"id": 44732079, "title": "Do LLMs Identify Fonts?", "cluster": 13, "x": 6.804008960723877, "y": 3.5138323307037354}, {"id": 44731915, "title": "Foreign Language LLM Jailbreak", "cluster": 13, "x": 6.811314105987549, "y": 3.7347936630249023}, {"id": 44730003, "title": "Track and visualize LLM model performance over time", "cluster": 13, "x": 6.9695210456848145, "y": 3.3757002353668213}, {"id": 44729690, "title": "Starter Code for Agentic Systems", "cluster": 34, "x": 7.907032489776611, "y": 4.438395977020264}, {"id": 44729156, "title": "Accelerating Large-Scale Test Migration with LLMs", "cluster": 13, "x": 6.965880870819092, "y": 3.5135326385498047}, {"id": 44726994, "title": "Large Language Models", "cluster": 208, "x": 8.14594841003418, "y": 4.396599292755127}, {"id": 44726838, "title": "Playing with Open Source LLMs", "cluster": 13, "x": 6.811878681182861, "y": 3.720501184463501}, {"id": 44726415, "title": "Actions speak louder than LLMs: A case for behavioral AI [video]", "cluster": 12, "x": 6.940088272094727, "y": 2.644637107849121}, {"id": 44724680, "title": "Can an LLM make \"educated\" guesses about name origins?", "cluster": 13, "x": 6.654357433319092, "y": 3.0465755462646484}, {"id": 44724238, "title": "Irrelevant facts about cats added to math problems increase LLM errors by 300%", "cluster": 13, "x": 6.703669548034668, "y": 3.2022247314453125}, {"id": 44723716, "title": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs", "cluster": 13, "x": 6.8011040687561035, "y": 3.503848075866699}, {"id": 44722364, "title": "Local LLM Agents \u2013 Do they work?", "cluster": 13, "x": 6.719570159912109, "y": 3.2227535247802734}, {"id": 44722414, "title": "Inference-Time Techniques for High-Quality, Low-Latency Speech Generation", "cluster": 208, "x": 8.354082107543945, "y": 4.470004081726074}, {"id": 44721628, "title": "Personal Software in the Age of LLM's", "cluster": 13, "x": 6.549065113067627, "y": 3.650867223739624}, {"id": 44721629, "title": "Machine Learners Should Acknowledge Legal Implications of LLMs as Personal Data", "cluster": 13, "x": 6.583339214324951, "y": 3.35520076751709}, {"id": 44721479, "title": "Outside LLMs AI and Music Buildathon Rankings", "cluster": 12, "x": 7.045742511749268, "y": 2.6178483963012695}, {"id": 44718857, "title": "Cascade: LLM-Powered JavaScript Deobfuscator", "cluster": 13, "x": 6.780612945556641, "y": 3.764277219772339}, {"id": 44718352, "title": "SmallThinker: A Family of Efficient LLMs Natively Trained for Local Deployment", "cluster": 13, "x": 6.891933441162109, "y": 3.50543212890625}, {"id": 44718173, "title": "voyage-context-3: Contextual Retrieval Without the LLM", "cluster": 13, "x": 6.783393383026123, "y": 3.464031219482422}, {"id": 44717296, "title": "When LLMs Autonomously Attack", "cluster": 13, "x": 6.43949031829834, "y": 3.307255506515503}, {"id": 44716133, "title": "I looked at the chess.com board and thought.. I'll make lu-chess", "cluster": 13, "x": 6.800536155700684, "y": 3.441153049468994}, {"id": 44715592, "title": "Ollama.com A website to download LLMs and try AI quick and easy", "cluster": 12, "x": 7.029376983642578, "y": 2.6318018436431885}, {"id": 44715107, "title": "Modern Day LLM Blueprint: A compilation of the most recent technologies", "cluster": 13, "x": 6.794808387756348, "y": 3.5251333713531494}, {"id": 44715132, "title": "LLMs can now identify public figures in images", "cluster": 13, "x": 6.843649864196777, "y": 3.4135589599609375}, {"id": 44714404, "title": "SpecTree: Composable Context Engineering for LLMs", "cluster": 13, "x": 6.895009517669678, "y": 3.5607826709747314}, {"id": 44714033, "title": "Plex: Perturbation-Free Local Explanations for LLM-Based Text Classification", "cluster": 13, "x": 6.953098773956299, "y": 3.5259265899658203}, {"id": 44712783, "title": "Syco Bench, a simple benchmark of LLM Sycophancy", "cluster": 13, "x": 6.934872150421143, "y": 3.384124279022217}, {"id": 44712312, "title": "Understanding why deterministic output from LLMs is nearly impossible", "cluster": 13, "x": 6.759279727935791, "y": 3.1752889156341553}, {"id": 44712188, "title": "A Semi-Technical Primer on LLMs", "cluster": 13, "x": 6.74822998046875, "y": 3.2366397380828857}, {"id": 44712010, "title": "Pseudo (Building LLMs into Lisp)", "cluster": 13, "x": 6.888703346252441, "y": 3.716442823410034}, {"id": 44712076, "title": "Ontologies in Design: Imagining a Tree Reveals LLM Possibilities and Assumptions", "cluster": 13, "x": 6.6942057609558105, "y": 3.174161911010742}, {"id": 44711306, "title": "Tao on \u201cblue team\u201d vs. \u201cred team\u201d LLMs", "cluster": 13, "x": 6.7342658042907715, "y": 3.129889488220215}, {"id": 44709239, "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights", "cluster": 13, "x": 6.840697288513184, "y": 3.3300893306732178}, {"id": 44708028, "title": "LLM Embeddings Explained: A Visual and Intuitive Guide", "cluster": 13, "x": 6.926107406616211, "y": 3.424866199493408}, {"id": 44707641, "title": "LLVM integrated assembler: Engineering better fragments", "cluster": 13, "x": 7.059128761291504, "y": 3.896881580352783}, {"id": 44707334, "title": "Scaling Laws for LLM-Based Data Compression", "cluster": 13, "x": 7.061548709869385, "y": 3.4668006896972656}, {"id": 44706261, "title": "LLMs Transmit Behavioral Traits via Hidden Signals in Data", "cluster": 13, "x": 6.746756076812744, "y": 3.3084425926208496}, {"id": 44701903, "title": "LLMs Force Engineering Discipline We Should Have Had All Along", "cluster": 13, "x": 6.643418788909912, "y": 3.3559281826019287}, {"id": 44700653, "title": "Physics of Language Models: The Magic of Canon Layers", "cluster": 208, "x": 8.504878997802734, "y": 4.4211907386779785}, {"id": 44700083, "title": "LLMs are bad at returning code in JSON", "cluster": 13, "x": 6.70072603225708, "y": 3.7149136066436768}, {"id": 44699074, "title": "LSM-2: Learning from incomplete wearable sensor data", "cluster": 13, "x": 7.103274345397949, "y": 3.473350763320923}, {"id": 44698943, "title": "Neovim plugin to prompt any model from Markdown files", "cluster": 13, "x": 7.046224594116211, "y": 4.040346145629883}, {"id": 44698754, "title": "Fast and cheap bulk storage: using LVM to cache HDDs on SSDs", "cluster": 13, "x": 7.102561950683594, "y": 3.7606897354125977}, {"id": 44698071, "title": "voyage-context-3: Contextual Retrieval Without the LLM", "cluster": 13, "x": 6.820323944091797, "y": 3.4416427612304688}, {"id": 44697834, "title": "Open-source LLM powered plaintext based spreadsheet appp", "cluster": 13, "x": 6.902739524841309, "y": 3.716452121734619}, {"id": 44696495, "title": "How much energy does it take to produce an LLM token?", "cluster": 13, "x": 6.833981037139893, "y": 3.269878387451172}, {"id": 44695277, "title": "A framework for AI-to-AI dialogue to solve the context problem in LLMs", "cluster": 12, "x": 7.028415203094482, "y": 2.626964569091797}, {"id": 44694889, "title": "Getting started with LLVM development on Windows via WSL2", "cluster": 13, "x": 6.80134916305542, "y": 3.7195563316345215}, {"id": 44691011, "title": "Why LLMs may make monkeys out of us", "cluster": 13, "x": 6.498135566711426, "y": 3.0414865016937256}, {"id": 44690748, "title": "The Sparse Frontier: Sparse Attention Trade-Offs in Transformer LLMs", "cluster": 13, "x": 7.027231216430664, "y": 3.3582937717437744}, {"id": 44690934, "title": "Stateless Persona Continuity in LLMs: Cross-Window Anchors Beyond Context Limits", "cluster": 13, "x": 6.746276378631592, "y": 3.304655075073242}, {"id": 44688609, "title": "Built a tiny LLM tool to explain blockchain transactions (EVMs) in plain English", "cluster": 13, "x": 6.924370765686035, "y": 3.528585910797119}, {"id": 44687135, "title": "Explainable Mapper: Charting LLM Embedding Spaces Using Perturbation-Based", "cluster": 13, "x": 6.938638687133789, "y": 3.366413116455078}, {"id": 44686702, "title": "Dwm Commented", "cluster": 13, "x": 6.537099838256836, "y": 3.0289649963378906}, {"id": 44685294, "title": "Elixir Usage Rules for LLMs", "cluster": 13, "x": 6.730137348175049, "y": 3.2758607864379883}, {"id": 44683993, "title": "T-Shirts with LLMs popular phrases?", "cluster": 13, "x": 6.6001057624816895, "y": 3.0823326110839844}, {"id": 44680849, "title": "LMDB (Lightning Memory-Mapped Database)", "cluster": 13, "x": 7.018022060394287, "y": 3.7829227447509766}, {"id": 44680559, "title": "LLMs Suck", "cluster": 13, "x": 6.554924011230469, "y": 3.1751270294189453}, {"id": 44680108, "title": "Vortex: A Prompting Protocol to Test for a 'Self' in LLMs", "cluster": 13, "x": 6.916466236114502, "y": 3.2209482192993164}, {"id": 44679013, "title": "Benchmarking LLMs on open source Vulkan", "cluster": 13, "x": 6.995902061462402, "y": 3.626335382461548}, {"id": 44677864, "title": "Structllm \u2013 structured output support to any LLM provider", "cluster": 13, "x": 6.901432037353516, "y": 3.5807197093963623}, {"id": 44677409, "title": "Can You Run This LLM? VRAM Calculator", "cluster": 13, "x": 6.9736504554748535, "y": 3.6704328060150146}, {"id": 44674845, "title": "ICML Statement about subversive hidden LLM prompts", "cluster": 13, "x": 6.6411638259887695, "y": 3.4296069145202637}, {"id": 44674830, "title": "Train a 70b language model at home (2024)", "cluster": 208, "x": 8.159693717956543, "y": 4.575891971588135}, {"id": 44674682, "title": "\"ChatGPT Psychosis\" and LLM Sycophancy", "cluster": 13, "x": 6.622115612030029, "y": 3.4328269958496094}, {"id": 44674162, "title": "LiteLLM Python SDK Proxy Server LLM Gateway Call 100 LLM APIs in OpenAI Format", "cluster": 13, "x": 6.853713035583496, "y": 3.730048418045044}, {"id": 44673270, "title": "Sovereignty in the Age of LLMs", "cluster": 13, "x": 6.573243618011475, "y": 3.1048362255096436}, {"id": 44672638, "title": "Promptomatix: An Automatic Prompt Optimization Framework for LLMs", "cluster": 13, "x": 6.8606181144714355, "y": 3.53509521484375}, {"id": 44672699, "title": "Quantifying uncert-AI-nty: Testing the accuracy of LLMs' confidence judgments", "cluster": 13, "x": 6.8056440353393555, "y": 2.8602066040039062}, {"id": 44671944, "title": "LLM_client: The Easiest Rust Interface for Local LLMs", "cluster": 13, "x": 6.960025787353516, "y": 3.9910919666290283}, {"id": 44671781, "title": "LLMs now at level of gold medallists in the International Mathematical Olympiad", "cluster": 13, "x": 6.65754508972168, "y": 3.163973808288574}, {"id": 44670881, "title": "Who Is LLM?", "cluster": 13, "x": 6.64927339553833, "y": 3.207750082015991}, {"id": 44670688, "title": "AgentUp: Config-driven framework for building A2A compatible agents", "cluster": 37, "x": 8.507408142089844, "y": 3.7765696048736572}, {"id": 44669243, "title": "Outside LLMs \u2013 Outside Lands AI and Music Buildathon", "cluster": 12, "x": 7.066243648529053, "y": 2.658066749572754}, {"id": 44669077, "title": "The Guest Who Could: Exploiting LPE in VMware Tools", "cluster": 13, "x": 6.823396682739258, "y": 3.616178035736084}, {"id": 44668806, "title": "LLMs are Bayesian, in Expectation, not in Realization", "cluster": 13, "x": 6.773725986480713, "y": 3.1192121505737305}, {"id": 44668684, "title": "LLMs remain vulnerable to \"jailbreaking\" through adversarial prompts", "cluster": 13, "x": 6.490693092346191, "y": 3.3984806537628174}, {"id": 44668254, "title": "\"ChatGPT Psychosis\" and LLM Sycophancy", "cluster": 13, "x": 6.5084686279296875, "y": 3.480590581893921}, {"id": 44667753, "title": "Why the world should stop obsessing over LLMs", "cluster": 13, "x": 6.525929927825928, "y": 3.0533955097198486}, {"id": 44665404, "title": "Meta-Prompting: Using LLMs to Improve How You Prompt LLMs", "cluster": 13, "x": 6.814764499664307, "y": 3.3218295574188232}, {"id": 44663980, "title": "Optimizing Tool Selection in LLM Workflows(Part 2): A DSPy and PyTorch Benchmark", "cluster": 13, "x": 6.9145188331604, "y": 3.4800896644592285}, {"id": 44663573, "title": "The State of Flash Attention on ROCm", "cluster": 13, "x": 7.082945823669434, "y": 3.8039348125457764}, {"id": 44663019, "title": "LLMs Transmit Traits via Hidden Signals to Other Models", "cluster": 13, "x": 6.716902732849121, "y": 3.3067426681518555}, {"id": 44663132, "title": "DeLLMa: Decision Making Under Uncertainty with Large Language Models", "cluster": 208, "x": 8.232503890991211, "y": 4.318760395050049}, {"id": 44662096, "title": "Agentic Engineering \u2013 Zed", "cluster": 38, "x": 8.663926124572754, "y": 3.648982048034668}, {"id": 44661917, "title": "Use Qwen3-Coder (Or Any Other LLM) with Claude Code", "cluster": 13, "x": 6.939762592315674, "y": 4.059975624084473}, {"id": 44659024, "title": "LLMs worse at proactive investigation than random heuristics", "cluster": 13, "x": 6.74143648147583, "y": 3.1428561210632324}, {"id": 44658515, "title": "ICML Statement about subversive hidden LLM prompts", "cluster": 13, "x": 6.660511493682861, "y": 3.4008212089538574}, {"id": 44657909, "title": "Inference-Time Techniques for High-Quality, Low-Latency Speech Generation", "cluster": 208, "x": 8.34697151184082, "y": 4.4495649337768555}, {"id": 44656415, "title": "Large Language Models", "cluster": 208, "x": 8.148833274841309, "y": 4.403697490692139}, {"id": 44655646, "title": "Open-Source LLM Helps Safeguard Text Generation Prompts and Responses", "cluster": 13, "x": 6.845243453979492, "y": 3.7266592979431152}, {"id": 44655350, "title": "LLMs Looks Like Memento", "cluster": 13, "x": 6.704030513763428, "y": 3.4125351905822754}, {"id": 44655485, "title": "Kaggle Launches LLM Evals", "cluster": 13, "x": 6.88505220413208, "y": 3.4895825386047363}, {"id": 44655252, "title": "Any-agent: A single interface to use and evaluate different agent frameworks", "cluster": 38, "x": 8.55322265625, "y": 3.728877067565918}, {"id": 44655109, "title": "Smart Routing Saved Exa 90% on LLM Costs", "cluster": 13, "x": 6.859816551208496, "y": 3.4861106872558594}, {"id": 44654135, "title": "An LLM-based chatbot promised a 50% discount due to hallucination", "cluster": 13, "x": 6.4820556640625, "y": 3.129112720489502}, {"id": 44653335, "title": "Liking Yellow Imply Driving a School Bus? Semantic Leakage in LLMs", "cluster": 13, "x": 6.6633620262146, "y": 3.0938103199005127}, {"id": 44651872, "title": "LLMs Tend to Be Overconfident", "cluster": 13, "x": 6.5794219970703125, "y": 3.043032646179199}, {"id": 44651661, "title": "Mistral reports on the environmental impact of LLMs", "cluster": 13, "x": 6.678464889526367, "y": 3.188997268676758}, {"id": 44651179, "title": "AWS AgentCore: Build agents effectively", "cluster": 38, "x": 8.584035873413086, "y": 3.654726505279541}, {"id": 44651142, "title": "LSM-2: Learning from incomplete wearable sensor data", "cluster": 13, "x": 7.14598274230957, "y": 3.494741439819336}, {"id": 44650583, "title": "Safety Evaluations of 20 LLMs", "cluster": 13, "x": 6.79876184463501, "y": 3.2112579345703125}, {"id": 44649531, "title": "Any-LLM: A unified API to access any LLM provider", "cluster": 13, "x": 6.755751609802246, "y": 3.708080291748047}, {"id": 44648061, "title": "I wrote 2000 LLM test cases so you don't have to: LLM feature compatibility grid", "cluster": 13, "x": 6.808100700378418, "y": 3.4259653091430664}, {"id": 44647576, "title": "We Built An Entire Newsroom with LLMs \u2013 Then We Showed The Print To the Real One", "cluster": 13, "x": 6.675210475921631, "y": 3.347055673599243}, {"id": 44647625, "title": "Why LLMs Struggle with Analytics", "cluster": 13, "x": 6.729269504547119, "y": 3.1216583251953125}, {"id": 44647562, "title": "Bacon \u2013 Bicameral Agent with Compute-Aware Orchestration and Navigation", "cluster": 37, "x": 8.520427703857422, "y": 3.679499387741089}, {"id": 44647375, "title": "Ask HR: LLMs can't solve basic logic problems, much less code", "cluster": 13, "x": 6.596532821655273, "y": 3.446166753768921}, {"id": 44647405, "title": "Advising is the next step for LLMs, a change in how they answer questions", "cluster": 13, "x": 6.699708938598633, "y": 3.140812635421753}, {"id": 44647498, "title": "Conversations between LLMs could automate the creation of exploits, study shows", "cluster": 13, "x": 6.49253511428833, "y": 3.4074594974517822}, {"id": 44646976, "title": "LLMs must evolve from scaling to full orchestration", "cluster": 13, "x": 6.741837024688721, "y": 3.2676303386688232}, {"id": 44646729, "title": "Our Collective Learning Disability", "cluster": 349, "x": 7.174973487854004, "y": 2.9421536922454834}, {"id": 44646543, "title": "A 27M-param model that solves hard Sudoku/mazes where LLMs fail, without CoT", "cluster": 13, "x": 7.036927700042725, "y": 3.3649580478668213}, {"id": 44645356, "title": "We Won't Reach AGI by Scaling Up LLMs", "cluster": 13, "x": 6.599682807922363, "y": 3.1260201930999756}, {"id": 44645152, "title": "Practical tips on building LLM agents", "cluster": 13, "x": 6.86052942276001, "y": 3.294055223464966}, {"id": 44645052, "title": "LLM Just Got Pwned: The Hidden Danger in MCP Sampling", "cluster": 13, "x": 6.41478967666626, "y": 3.1602699756622314}, {"id": 44644682, "title": "Who Is LLM?", "cluster": 13, "x": 6.647417068481445, "y": 3.187246084213257}, {"id": 44644668, "title": "Who Is LLM?", "cluster": 13, "x": 6.623042583465576, "y": 3.185772180557251}, {"id": 44644275, "title": "Using LLMs and MCP to Debug PostgreSQL Performance in Rails", "cluster": 13, "x": 7.1630754470825195, "y": 3.7653138637542725}, {"id": 44643330, "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in LLMs", "cluster": 13, "x": 6.615384578704834, "y": 3.033536672592163}, {"id": 44643131, "title": "AI Airport Simulation \u2013 LLM Decision Making Playground", "cluster": 12, "x": 7.017814636230469, "y": 2.6311469078063965}, {"id": 44642299, "title": "Advising is the next step for LLMs, a change in how they answer questions", "cluster": 13, "x": 6.642210960388184, "y": 3.1012051105499268}, {"id": 44640508, "title": "Detecting LLM\u2011Generated 404s", "cluster": 13, "x": 6.608338356018066, "y": 3.4741408824920654}, {"id": 44640471, "title": "Working on a Programming Language in the Age of LLMs", "cluster": 13, "x": 6.606083393096924, "y": 3.5988402366638184}, {"id": 44640275, "title": "Exploiting Primacy Effect to Improve Large Language Models", "cluster": 208, "x": 8.286209106445312, "y": 4.374607086181641}, {"id": 44640241, "title": "The Cheapest LLM Call Is the One You Don't Await", "cluster": 13, "x": 6.6102213859558105, "y": 3.2246477603912354}, {"id": 44637770, "title": "AdaptiQ Core \u2013 Optimize your LLM agents with RL and save 30% tokens", "cluster": 13, "x": 7.040189743041992, "y": 3.3653626441955566}, {"id": 44637352, "title": "AccountingBench: Evaluating LLMs on real long-horizon business tasks", "cluster": 13, "x": 6.791262626647949, "y": 3.2353556156158447}, {"id": 44636938, "title": "I Use EDA and Local LLMs to Make Better Product Decisions", "cluster": 13, "x": 6.7386274337768555, "y": 3.454995632171631}, {"id": 44634915, "title": "LLMs vs. Brainfuck: a demonstration of Potemkin understanding", "cluster": 13, "x": 6.722952842712402, "y": 3.0333192348480225}, {"id": 44634128, "title": "LLM Observability with ClickStack, OpenTelemetry, and MCP", "cluster": 13, "x": 6.839353084564209, "y": 3.6482744216918945}, {"id": 44631013, "title": "this let you deploy your LLM agents into production with one click", "cluster": 13, "x": 6.888091564178467, "y": 3.5349273681640625}, {"id": 44630833, "title": "How OpenElections Uses LLMs", "cluster": 13, "x": 6.8124799728393555, "y": 3.653463363647461}, {"id": 44630506, "title": "LLMs and Computation Complexity (2023)", "cluster": 11, "x": 7.056361675262451, "y": 3.375746011734009}, {"id": 44629175, "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in LLMs", "cluster": 13, "x": 6.59248161315918, "y": 3.0086679458618164}, {"id": 44629303, "title": "How I Use LLMs for Coding and Writing", "cluster": 13, "x": 6.6874918937683105, "y": 3.700265645980835}, {"id": 44628960, "title": "LLM-in-a-Box: A Templated, Self-Hostable Framework for Generative AI", "cluster": 12, "x": 7.060044765472412, "y": 2.6647403240203857}, {"id": 44627742, "title": "AIOps in the Era of LLMs", "cluster": 13, "x": 6.570180416107178, "y": 3.113901138305664}, {"id": 44627161, "title": "Cosmosapien CLI / Dumb LLM Orchestrator", "cluster": 13, "x": 6.911700248718262, "y": 3.6739423274993896}, {"id": 44626383, "title": "A Survey of Context Engineering for Large Language Models", "cluster": 208, "x": 8.199481010437012, "y": 4.527158737182617}, {"id": 44625492, "title": "How the Free Software Foundation battles the LLM bots", "cluster": 13, "x": 6.538511276245117, "y": 3.4262585639953613}, {"id": 44624797, "title": "LLMs Are Still in Their Infancy, or Why Programmers Aren't Being Replaced", "cluster": 13, "x": 6.580748558044434, "y": 3.5879294872283936}, {"id": 44624718, "title": "Can LLMs Understand Patent Regulations to Pass a Hands-On Patent Attorney Test?", "cluster": 13, "x": 6.714489459991455, "y": 3.2168080806732178}, {"id": 44623564, "title": "LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "cluster": 13, "x": 6.916147708892822, "y": 3.1850244998931885}, {"id": 44622608, "title": "LLM architecture comparison", "cluster": 13, "x": 6.882608890533447, "y": 3.3874895572662354}, {"id": 44621449, "title": "LLM Cmd 2: Generate shell commands with LLM's", "cluster": 13, "x": 6.877474784851074, "y": 3.7961831092834473}, {"id": 44620590, "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders", "cluster": 208, "x": 8.471856117248535, "y": 4.467377662658691}, {"id": 44619640, "title": "A Survey of Context Engineering for Large Language Models", "cluster": 208, "x": 8.190073013305664, "y": 4.51837682723999}, {"id": 44618607, "title": "Proposal: HTML Data-LLM Attributes for Enhanced AI Content Understanding", "cluster": 12, "x": 7.05624532699585, "y": 2.694019079208374}, {"id": 44618497, "title": "Flagship LLMs vs. AI Detection [video]", "cluster": 12, "x": 6.927399635314941, "y": 2.642408609390259}, {"id": 44617321, "title": "The Golden Rule Goes Digital: Being Mean to LLMs Might Be Our Dumbest Gamble", "cluster": 13, "x": 6.442174434661865, "y": 3.040296792984009}, {"id": 44617078, "title": "Local LLMs versus offline Wikipedia", "cluster": 13, "x": 6.757236480712891, "y": 3.7003374099731445}, {"id": 44616778, "title": "LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential", "cluster": 13, "x": 6.949461936950684, "y": 3.177272319793701}, {"id": 44616706, "title": "Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLMs", "cluster": 13, "x": 6.895593166351318, "y": 3.191113233566284}, {"id": 44615301, "title": "Oxford's AI Chair: LLMs are a hack [video]", "cluster": 12, "x": 6.9475603103637695, "y": 2.635324001312256}, {"id": 44614914, "title": "The Big LLM Architecture Comparison", "cluster": 13, "x": 6.915637969970703, "y": 3.4337329864501953}, {"id": 44614706, "title": "I did give an agent access to my Google Cloud production instances", "cluster": 35, "x": 8.444360733032227, "y": 3.8268845081329346}, {"id": 44614365, "title": "I avoid using LLMs as a publisher and writer", "cluster": 13, "x": 6.538085460662842, "y": 3.21073055267334}, {"id": 44610152, "title": "Easy Agents: Build autonomous agents with just natural language", "cluster": 3, "x": 8.098382949829102, "y": 4.307487487792969}, {"id": 44608728, "title": "Can LLMs Do Accounting?", "cluster": 13, "x": 6.729786396026611, "y": 3.2622501850128174}, {"id": 44608558, "title": "Billions of Tokens Later: Scaling LLM Fuzzing in Practice", "cluster": 13, "x": 6.654298782348633, "y": 3.0682315826416016}, {"id": 44607563, "title": "How to run an LLM on your laptop", "cluster": 13, "x": 6.856043338775635, "y": 3.7100491523742676}, {"id": 44607412, "title": "A Survey of Context Engineering for Large Language Models", "cluster": 208, "x": 8.194293975830078, "y": 4.517000675201416}, {"id": 44606300, "title": "Zml/llmd: homegrown LLM server built with Zig", "cluster": 13, "x": 6.86513614654541, "y": 3.733741283416748}, {"id": 44606094, "title": "What can we expect of LLMs as Software Engineers?", "cluster": 13, "x": 6.542630672454834, "y": 3.532463550567627}, {"id": 44605713, "title": "LLM Internals for Beginners", "cluster": 13, "x": 6.8071064949035645, "y": 3.6383862495422363}, {"id": 44605507, "title": "From Chatbots to Components: Teaching Devs to Think of LLMs as (Fuzzy) Functions", "cluster": 13, "x": 6.799410820007324, "y": 3.4570741653442383}, {"id": 44604096, "title": "Coordination and Collaborative Reasoning in Multi-Agent LLMs", "cluster": 13, "x": 6.931013107299805, "y": 3.185049533843994}, {"id": 44604057, "title": "Russian infostealer sends commands to public LLM to craft requests on the fly", "cluster": 13, "x": 6.6658830642700195, "y": 3.7250564098358154}, {"id": 44603042, "title": "Google study shows LLMs abandon correct answers under pressure", "cluster": 13, "x": 6.63773775100708, "y": 3.1002540588378906}, {"id": 44602352, "title": "Thoughts on External Memory for LLMs", "cluster": 13, "x": 6.875660419464111, "y": 3.561694860458374}, {"id": 44601521, "title": "Getting past LLM \"day one\" problem with MIRIX", "cluster": 13, "x": 6.663071632385254, "y": 3.33674955368042}, {"id": 44601318, "title": "1.5B LLM routing model that aligns to preferences, not leaderboards", "cluster": 13, "x": 7.024298667907715, "y": 3.467900037765503}, {"id": 44600415, "title": "DltHub: LLM-native data platform for Python devs to build/run pipelines", "cluster": 13, "x": 6.9275031089782715, "y": 3.7936649322509766}, {"id": 44597894, "title": "Grafana and LLMs", "cluster": 13, "x": 6.728342533111572, "y": 3.3326754570007324}, {"id": 44597583, "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "cluster": 208, "x": 8.187280654907227, "y": 4.523874759674072}, {"id": 44593020, "title": "The power of the Unix philosophy for LLM agentic tools", "cluster": 13, "x": 6.889271259307861, "y": 3.31992244720459}, {"id": 44592723, "title": "My analysis of 439 models proves: You're overpaying for your LLMs", "cluster": 13, "x": 6.721745491027832, "y": 3.1119544506073}, {"id": 44592331, "title": "LLMs Are Bad at Being Forced", "cluster": 13, "x": 6.524298667907715, "y": 3.093522071838379}, {"id": 44592007, "title": "The Impact of Prompt Bloat on LLM Output Quality", "cluster": 13, "x": 6.79001522064209, "y": 3.3202807903289795}, {"id": 44591399, "title": "Migrating over 30 lambdas from Serverless Framework with LLMs", "cluster": 13, "x": 6.942924976348877, "y": 3.778214693069458}, {"id": 44591436, "title": "Zoho Launches Zia LLM", "cluster": 13, "x": 6.8180742263793945, "y": 3.7168078422546387}, {"id": 44591338, "title": "Scraping vulnerability data from 100 different sources (without LLMs)", "cluster": 13, "x": 6.56502628326416, "y": 3.4952995777130127}, {"id": 44590183, "title": "Advancing Polish Language Models", "cluster": 208, "x": 8.314884185791016, "y": 4.585833549499512}, {"id": 44589075, "title": "LLM Benchmarking Shows Capabilities Doubling Every 7 Months", "cluster": 13, "x": 6.949816703796387, "y": 3.4319159984588623}, {"id": 44587958, "title": "Three unrelated thoughts about working with LLMs", "cluster": 13, "x": 6.5700154304504395, "y": 3.085688591003418}, {"id": 44586725, "title": "Extract High-Quality Information with the NuExtract LLM", "cluster": 13, "x": 6.8946356773376465, "y": 3.4764204025268555}, {"id": 44585838, "title": "Enhancing COBOL Code Explanations: A Multi-Agents Approach Using LLMs", "cluster": 13, "x": 6.943307876586914, "y": 3.3351078033447266}, {"id": 44585492, "title": "How Many Instruction Can LLMs Follow at Once?", "cluster": 13, "x": 6.73296594619751, "y": 3.1713249683380127}, {"id": 44585286, "title": "The party trick called LLM", "cluster": 13, "x": 6.497219085693359, "y": 3.1419708728790283}, {"id": 44585159, "title": "Future-Proofing Junior Devs in the LLM Era", "cluster": 13, "x": 6.558138847351074, "y": 3.549238681793213}, {"id": 44584897, "title": "Customizing Memory in LangGraph Agents for Better Conversations", "cluster": 208, "x": 8.295685768127441, "y": 4.460762023925781}, {"id": 44584895, "title": "Google Launches Agentic Calls", "cluster": 35, "x": 8.459050178527832, "y": 3.8183560371398926}, {"id": 44584631, "title": "Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants", "cluster": 13, "x": 6.909455299377441, "y": 3.2779994010925293}, {"id": 44584283, "title": "LLMs Are Bayesian, in Expectation, Not Realization [pdf]", "cluster": 13, "x": 6.807108402252197, "y": 3.1832168102264404}, {"id": 44584202, "title": "Amazon Bedrock AgentCore", "cluster": 37, "x": 8.541281700134277, "y": 3.7418673038482666}, {"id": 44584057, "title": "Stanford's Marin model: The first open model developed using Jax", "cluster": 13, "x": 6.996615886688232, "y": 3.977004289627075}, {"id": 44583158, "title": "TinyTroupe: An LLM-Powered Multiagent Persona Simulation Toolkit", "cluster": 13, "x": 7.061834335327148, "y": 3.4624364376068115}, {"id": 44580307, "title": "An Autistic \"Linguatype\"? Neologisms, New Words, and New Insights", "cluster": 208, "x": 8.526823997497559, "y": 4.3136467933654785}, {"id": 44578749, "title": "Empirical evidence of LLM's influence on human spoken communication", "cluster": 13, "x": 6.590550899505615, "y": 3.04962420463562}, {"id": 44578558, "title": "Run LLM Agents as Microservices with One-Click Deployment", "cluster": 13, "x": 6.824983596801758, "y": 3.5882437229156494}, {"id": 44578070, "title": "LLM Daydreaming", "cluster": 13, "x": 6.5808329582214355, "y": 3.1154367923736572}, {"id": 44573622, "title": "An LLM Router That Thinks Like an Engineer", "cluster": 13, "x": 6.6129608154296875, "y": 3.5377137660980225}, {"id": 44571935, "title": "LLM Observability with ClickStack, OpenTelemetry, and MCP", "cluster": 13, "x": 6.857471942901611, "y": 3.641195297241211}, {"id": 44571040, "title": "Harnessing Frustration: Using LLMs to Overcome Activation Energy", "cluster": 13, "x": 6.733721733093262, "y": 3.2338449954986572}, {"id": 44570743, "title": "LLMs fail to demonstrate internal world model, according to Harvard/MIT study", "cluster": 13, "x": 6.606941223144531, "y": 3.019637107849121}, {"id": 44570465, "title": "How Hype Works; What LLMs have in common with genetics", "cluster": 13, "x": 6.618008613586426, "y": 2.9796507358551025}, {"id": 44570098, "title": "Emergent Price-Fixing by LLM Auction Agents", "cluster": 13, "x": 6.5244293212890625, "y": 3.1079769134521484}, {"id": 44569990, "title": "LLM Daydreaming", "cluster": 13, "x": 6.5888142585754395, "y": 3.076262950897217}, {"id": 44569598, "title": "Practical notes on getting LLMs to generate new ideas", "cluster": 13, "x": 6.7671966552734375, "y": 3.238407850265503}, {"id": 44568739, "title": "WebAssembly binding for llama.cpp \u2013 Enabling on-browser LLM inference", "cluster": 13, "x": 6.917809963226318, "y": 3.7592873573303223}, {"id": 44567857, "title": "LLM Inevitabilism", "cluster": 13, "x": 6.728283882141113, "y": 3.255801200866699}, {"id": 44567454, "title": "ChipBenchmark: Open-Source Benchmarking for LLM Performance Across Hardware", "cluster": 13, "x": 7.109738349914551, "y": 3.6375572681427}, {"id": 44567477, "title": "Prompt Injection in LLM-Driven Systems", "cluster": 13, "x": 6.689464569091797, "y": 3.3437628746032715}, {"id": 44566761, "title": "LLM Daydreaming", "cluster": 13, "x": 6.5660319328308105, "y": 3.114250898361206}, {"id": 44565932, "title": "Learning to Learn, in the Age of LLMs", "cluster": 13, "x": 6.591663360595703, "y": 3.0523462295532227}, {"id": 44565477, "title": "Llmnop, a tiny Rust rewrite of LLMPerf", "cluster": 13, "x": 6.99379825592041, "y": 4.212070941925049}, {"id": 44565088, "title": "An LLM trained only on data from certain time periods to reduce modern bias", "cluster": 13, "x": 6.875007152557373, "y": 3.2808477878570557}, {"id": 44564178, "title": "ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time", "cluster": 13, "x": 6.7847819328308105, "y": 3.2598729133605957}, {"id": 44564248, "title": "Context Rot: How increasing input tokens impacts LLM performance", "cluster": 13, "x": 6.913181781768799, "y": 3.266291618347168}, {"id": 44563729, "title": "Building Personalized Micro Agents", "cluster": 38, "x": 8.65947151184082, "y": 3.6268036365509033}, {"id": 44563008, "title": "Strategic Intelligence in LLMs: Evidence from Evolutionary Game Theory", "cluster": 13, "x": 6.738637447357178, "y": 3.023287773132324}, {"id": 44561609, "title": "One Token to Fool LLM-as-a-Judge", "cluster": 13, "x": 6.68919563293457, "y": 3.102277994155884}, {"id": 44561015, "title": "LLM-d: Prefix K/V Caching", "cluster": 13, "x": 6.968051433563232, "y": 3.7639288902282715}, {"id": 44561001, "title": "Translation using deep neural networks \u2013 Transformer", "cluster": 208, "x": 8.451074600219727, "y": 4.422468185424805}, {"id": 44560921, "title": "Alpha-One RISC-V (StarPro64 Based) for Local LLM Use Now in Stock", "cluster": 13, "x": 6.977079391479492, "y": 3.834062337875366}, {"id": 44558353, "title": "The LLM-for-software Yo-yo", "cluster": 13, "x": 6.620584487915039, "y": 3.6387345790863037}, {"id": 44557484, "title": "Cats Confuse LLM: Query Agnostic Adversarial Triggers for Reasoning Models", "cluster": 13, "x": 6.755050182342529, "y": 3.253655195236206}, {"id": 44557266, "title": "Give context, not bias (to LLMs)", "cluster": 13, "x": 6.733388423919678, "y": 3.1322896480560303}, {"id": 44556959, "title": "The Siren Song of LLMs", "cluster": 13, "x": 6.508389472961426, "y": 3.068366527557373}, {"id": 44555305, "title": "Adapting Teaching and Assessment Strategies in the Age of LLMs", "cluster": 13, "x": 6.606559753417969, "y": 3.063140630722046}, {"id": 44554865, "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs", "cluster": 13, "x": 6.727550506591797, "y": 3.021247386932373}, {"id": 44554665, "title": "Differentiable Programming for Learnable Graphs: Optimizing LLM Workflows W DSPy", "cluster": 13, "x": 6.914455413818359, "y": 3.601827383041382}, {"id": 44552387, "title": "LLMs for Drug-Drug Interaction Prediction: A Comprehensive Comparison", "cluster": 13, "x": 6.8541669845581055, "y": 3.2703144550323486}, {"id": 44552262, "title": "Gravity Inspired ML Model", "cluster": 13, "x": 7.11329984664917, "y": 3.465176820755005}, {"id": 44552023, "title": "A modular, LLM-agnostic fullstack agent framework", "cluster": 13, "x": 6.993517875671387, "y": 3.318199634552002}, {"id": 44551171, "title": "The Cost of Human-Centric Tools in LLM Workflows", "cluster": 13, "x": 6.8831071853637695, "y": 3.3735389709472656}, {"id": 44550200, "title": "Easily Ask Your LLM Coding Questions", "cluster": 13, "x": 6.658509731292725, "y": 3.6247642040252686}, {"id": 44548906, "title": "Understanding Tool Calling in LLMs \u2013 Step-by-Step with REST and Spring AI", "cluster": 13, "x": 6.94050931930542, "y": 3.313450813293457}, {"id": 44548733, "title": "Build ML Models by Just Asking \u2013 With MCP, LangChain, and a Bit of Magic", "cluster": 13, "x": 6.954625606536865, "y": 3.495821952819824}, {"id": 44548218, "title": "Empirical evidence of LLM's influence on human spoken communication", "cluster": 13, "x": 6.648736000061035, "y": 3.090273141860962}, {"id": 44546729, "title": "Your LLM Framework Only Needs 100 Lines", "cluster": 13, "x": 6.854032516479492, "y": 3.5471997261047363}, {"id": 44544923, "title": "Using AMD MI300X for High-Throughput, Low-Cost LLM Inference", "cluster": 13, "x": 7.185538291931152, "y": 3.619670867919922}, {"id": 44543796, "title": "I Built a Testable Recursive Theory of Language Models Using GPT \u2013 It Works", "cluster": 208, "x": 8.280486106872559, "y": 4.593044281005859}, {"id": 44543508, "title": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model", "cluster": 208, "x": 8.22162914276123, "y": 4.272955894470215}, {"id": 44543213, "title": "Ask HN: How are Kafka or event-driven systems used in LLM infrastructure?", "cluster": 13, "x": 6.825050354003906, "y": 3.2821741104125977}, {"id": 44541372, "title": "The Future Is Local \u2013 Why LLMs Should Run on Your Own Machine", "cluster": 13, "x": 6.643359661102295, "y": 3.5453059673309326}, {"id": 44540357, "title": "Open-sourcing our clinical triage benchmark for evaluating LLMs", "cluster": 13, "x": 6.957810401916504, "y": 3.497028350830078}, {"id": 44540344, "title": "Measuring the Impact of LLMs on Experienced Developer Productivity", "cluster": 13, "x": 6.632958889007568, "y": 3.4198355674743652}, {"id": 44540052, "title": "Bad Actors Are Grooming LLMs to Produce Falsehoods", "cluster": 13, "x": 6.516118049621582, "y": 3.022324562072754}, {"id": 44539301, "title": "Trace LLM workflows at your app's semantic level, not at the OpenAI API boundary", "cluster": 13, "x": 6.882856845855713, "y": 3.7027428150177}, {"id": 44538373, "title": "Don't make Naked LLM calls. Protect your users and their data", "cluster": 13, "x": 6.514675140380859, "y": 3.37898850440979}, {"id": 44538353, "title": "RedForge: Open-Source LLM Red-Teaming CLI \u2013 Feedback Wanted", "cluster": 13, "x": 6.963043212890625, "y": 3.753155469894409}, {"id": 44537985, "title": "Jackal's carapace \u2013 an LLM tar spout for the Anubis web application protector", "cluster": 13, "x": 6.749521732330322, "y": 3.787172794342041}, {"id": 44537001, "title": "LLMs for coding (+ free workflow templates)", "cluster": 13, "x": 6.733273983001709, "y": 3.812753438949585}, {"id": 44536839, "title": "Grok 4 CLI \u2013 a terminal-based LLM tool using xAI's Grok 4 model via their API", "cluster": 13, "x": 6.859850883483887, "y": 3.753490686416626}, {"id": 44536740, "title": "Humans overrely on overconfident language models, across languages", "cluster": 208, "x": 8.272897720336914, "y": 4.32020378112793}, {"id": 44536339, "title": "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs", "cluster": 13, "x": 6.596074104309082, "y": 3.238208770751953}, {"id": 44535637, "title": "ETH Zurich and EPFL to release a LLM developed on public infrastructure", "cluster": 13, "x": 6.797900199890137, "y": 3.6255295276641846}, {"id": 44535197, "title": "Should LLMs ask \"Is this real or fiction?\" before replying to suicidal thoughts?", "cluster": 13, "x": 6.621399879455566, "y": 3.000692129135132}, {"id": 44534266, "title": "Design Notes on LLM-driven text adventure", "cluster": 13, "x": 6.820339202880859, "y": 3.467116117477417}, {"id": 44534249, "title": "I don't think LLM's are making us stupider", "cluster": 13, "x": 6.530652046203613, "y": 3.0475199222564697}, {"id": 44533403, "title": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model", "cluster": 208, "x": 8.224302291870117, "y": 4.238856315612793}, {"id": 44532943, "title": "LLMs and Algorithmic Trading", "cluster": 13, "x": 6.881586074829102, "y": 3.1356537342071533}, {"id": 44532482, "title": "Strategic Intelligence in Large Language Models", "cluster": 208, "x": 8.192120552062988, "y": 4.342565059661865}, {"id": 44529711, "title": "A Practical Guide to Evaluating Large Language Models (LLM)", "cluster": 13, "x": 6.937332630157471, "y": 3.540980100631714}, {"id": 44529295, "title": "Trace LLM workflows at your app's semantic level, not at the OpenAI API boundary", "cluster": 13, "x": 6.866299152374268, "y": 3.69083571434021}, {"id": 44529011, "title": "Are LLMs starting to become sentient?", "cluster": 13, "x": 6.547365188598633, "y": 2.98447585105896}, {"id": 44527947, "title": "LLM Inference Handbook", "cluster": 13, "x": 6.924703121185303, "y": 3.303243637084961}, {"id": 44527379, "title": "Extreme Low-Bit Clustering for Large Language Models via Knowledge Distillation", "cluster": 208, "x": 8.359930992126465, "y": 4.433003902435303}, {"id": 44527352, "title": "Supporting kernel development with large language models", "cluster": 208, "x": 8.044825553894043, "y": 4.505127906799316}, {"id": 44526851, "title": "Adding LSM trees to Postgres makes replication tough", "cluster": 13, "x": 7.0286688804626465, "y": 3.8099753856658936}, {"id": 44526571, "title": "Using Large Language Models to Infer Problematic Instagram Use", "cluster": 208, "x": 8.36357307434082, "y": 4.3610405921936035}, {"id": 44526414, "title": "Learning to Learn (In the Age of LLMs)", "cluster": 13, "x": 6.597841739654541, "y": 3.0213961601257324}, {"id": 44526221, "title": "Use LLMs over DNS at Ch.at", "cluster": 13, "x": 6.788828372955322, "y": 3.7178399562835693}, {"id": 44523543, "title": "Mastering Language Models: A Deep Dive into Input Parameters", "cluster": 208, "x": 8.326006889343262, "y": 4.474254608154297}, {"id": 44523189, "title": "LLMs show cultural theory was right about the death of the author", "cluster": 13, "x": 6.600940704345703, "y": 2.9437074661254883}, {"id": 44522874, "title": "Agentic \u2013 RapidAPI for LLM Tools", "cluster": 13, "x": 6.968827724456787, "y": 3.266219139099121}, {"id": 44520481, "title": "Violent Tendencies in LLMs: Analysis via Behavioral Vignettes", "cluster": 13, "x": 6.641058921813965, "y": 3.040266752243042}, {"id": 44519642, "title": "Mecha-Hitler, Grok, and why it's so hard to give LLMs the right personality", "cluster": 13, "x": 6.587552547454834, "y": 3.1002309322357178}, {"id": 44519607, "title": "Swiss boffins tease 'fully open' LLM trained on Alps super", "cluster": 13, "x": 6.556337833404541, "y": 3.1400585174560547}, {"id": 44517948, "title": "We Should Anthropomorphize LLMs", "cluster": 13, "x": 6.54378080368042, "y": 2.987915515899658}, {"id": 44517689, "title": "Native LLM APIs in Ray Data and Ray Serve", "cluster": 13, "x": 6.945765018463135, "y": 3.799622058868408}, {"id": 44517522, "title": "Chatlas: Guide to building LLM apps with less effort and more clarity", "cluster": 13, "x": 6.800381183624268, "y": 3.5356953144073486}, {"id": 44516592, "title": "Seeing Like an LLM", "cluster": 13, "x": 6.526495456695557, "y": 3.0624401569366455}, {"id": 44516126, "title": "PoPo: MMD Anime Char Model Pose Generation Using Fine Tuned LLM", "cluster": 13, "x": 7.0258941650390625, "y": 3.6948437690734863}, {"id": 44515523, "title": "When Building with LLM's, Add Autonomy Last", "cluster": 13, "x": 6.735818862915039, "y": 3.4349894523620605}, {"id": 44514752, "title": "A new open LLM built for the public good (by ETHZ/EPFL/CSCS)", "cluster": 13, "x": 6.750699043273926, "y": 3.65507173538208}, {"id": 44514791, "title": "Zerank-1, new sota LLM reranker", "cluster": 13, "x": 6.798802852630615, "y": 3.314655303955078}, {"id": 44514033, "title": "Are LLMs starting to become sentient?", "cluster": 13, "x": 6.523420810699463, "y": 2.9724764823913574}, {"id": 44513903, "title": "Helix Parallelism: Sharding Strategies for Multi-Million-Token LLM Decoding", "cluster": 13, "x": 7.072004795074463, "y": 3.464926242828369}, {"id": 44511863, "title": "Infinite Monkey: Chat with an LLM to control an emulated classic Mac", "cluster": 13, "x": 6.792079448699951, "y": 3.7380709648132324}, {"id": 44511048, "title": "FlexOlmo: A paradigm for LLM training and data collaboration", "cluster": 13, "x": 6.852901458740234, "y": 3.3094608783721924}, {"id": 44510422, "title": "Using Self-Hosted Large Language Models (LLMs) Securely in Government", "cluster": 13, "x": 6.97348165512085, "y": 3.8455517292022705}, {"id": 44509936, "title": "Why LLM Authorization is Hard", "cluster": 13, "x": 6.565081596374512, "y": 3.2727067470550537}, {"id": 44509443, "title": "Whitepaper: Decentralized Protocol for Verifiable LLM Training and Fine-Tuning [pdf]", "cluster": 13, "x": 6.8353753089904785, "y": 3.484201431274414}, {"id": 44509109, "title": "Can LLMs Replace Humans During Code Chunking?", "cluster": 13, "x": 6.695270538330078, "y": 3.584958791732788}, {"id": 44508468, "title": "Using an LLM and SWC to find API breaking changes across repos", "cluster": 13, "x": 6.880863666534424, "y": 3.74595046043396}, {"id": 44507887, "title": "Empirical Evaluation of Large Language Models in Automated Program Repair", "cluster": 208, "x": 8.178250312805176, "y": 4.448573589324951}, {"id": 44507858, "title": "A language model built for the public good", "cluster": 208, "x": 8.267316818237305, "y": 4.57244348526001}, {"id": 44507517, "title": "What If an LLM Could Profile You from the Browser History?", "cluster": 13, "x": 6.576871871948242, "y": 3.5286850929260254}, {"id": 44507383, "title": "A big shift in training LLMs led to a capability explosion", "cluster": 13, "x": 6.516041278839111, "y": 2.981416702270508}, {"id": 44506388, "title": "LM Studio is free for use at work", "cluster": 13, "x": 6.747325897216797, "y": 3.687730312347412}, {"id": 44506054, "title": "Pocket LLM Server Just Like a Pocket WiFi", "cluster": 13, "x": 6.783105850219727, "y": 3.6276538372039795}, {"id": 44505548, "title": "UCD: Unlearning in LLMs via Contrastive Decoding", "cluster": 13, "x": 6.818973064422607, "y": 3.2562544345855713}, {"id": 44505092, "title": "Paper: Disambiguation-Centric Finetuning Makes Tool-Calling LLMs More Realistic", "cluster": 13, "x": 6.7765421867370605, "y": 3.5536434650421143}, {"id": 44505031, "title": "Why is RL important, especially for LLMs?", "cluster": 13, "x": 6.625243186950684, "y": 3.160374164581299}, {"id": 44504841, "title": "LLM Failures", "cluster": 13, "x": 6.544564247131348, "y": 3.204780101776123}, {"id": 44504723, "title": "Confidential AI Inference with Attestation: Run LLMs and Agents on Tees", "cluster": 12, "x": 7.005763053894043, "y": 2.6241989135742188}, {"id": 44504576, "title": "Why Do Some Language Models Fake Alignment While Others Don't?", "cluster": 208, "x": 8.421451568603516, "y": 4.360866069793701}, {"id": 44504434, "title": "Design Patterns for Securing LLM Agents Against Prompt Injections", "cluster": 13, "x": 6.275425434112549, "y": 3.294255256652832}, {"id": 44504046, "title": "Trae Agent is an LLM-based agent for general purpose software engineering tasks", "cluster": 13, "x": 6.878932476043701, "y": 3.3354711532592773}, {"id": 44503852, "title": "LLM Bridge by Supermemory", "cluster": 13, "x": 6.7966179847717285, "y": 3.376755952835083}, {"id": 44503713, "title": "Praise: Enhancing Product Descriptions with LLM-Driven Structured Insights", "cluster": 13, "x": 6.812211036682129, "y": 3.407679796218872}, {"id": 44503153, "title": "Ask Your LLM:)", "cluster": 13, "x": 6.614265441894531, "y": 3.183411121368408}, {"id": 44503056, "title": "The Tradeoffs of SSMs and Transformers", "cluster": 13, "x": 6.395336627960205, "y": 3.2109148502349854}, {"id": 44503104, "title": "Writing an LLM from scratch, part 16 \u2013 layer normalisation", "cluster": 13, "x": 6.8480706214904785, "y": 3.3926584720611572}, {"id": 44502787, "title": "LLMPrices", "cluster": 13, "x": 7.150395393371582, "y": 3.362316131591797}, {"id": 44502627, "title": "LLM Hallucination Detection Leaderboard", "cluster": 13, "x": 6.898245811462402, "y": 3.038672685623169}, {"id": 44502366, "title": "LLM-Ready Training Dataset for Apple's Foundation Models (iOS 26)", "cluster": 13, "x": 7.0168962478637695, "y": 3.554157018661499}, {"id": 44502075, "title": "LM Studio is free for use at work", "cluster": 13, "x": 6.7687554359436035, "y": 3.7179791927337646}, {"id": 44501413, "title": "Smollm3: Smol, multilingual, long-context reasoner LLM", "cluster": 13, "x": 6.852009296417236, "y": 3.643059730529785}, {"id": 44501054, "title": "A big shift in training LLMs led to a capability explosion", "cluster": 13, "x": 6.528645038604736, "y": 2.9930622577667236}, {"id": 44500895, "title": "Enhancing Search Retrieval with LLMs", "cluster": 13, "x": 6.840863227844238, "y": 3.5022895336151123}, {"id": 44500588, "title": "LLMs Are Magic \u2013 Their Applications Should Be, Too", "cluster": 13, "x": 6.656508445739746, "y": 3.3282182216644287}, {"id": 44500578, "title": "Overview of Architectural Improvements in vLLM V1", "cluster": 13, "x": 7.18697452545166, "y": 4.138508319854736}, {"id": 44500428, "title": "Cats Confuse Reasoning LLM \u2013 Adversarial Triggers for Reasoning Models", "cluster": 13, "x": 6.76041316986084, "y": 3.227679967880249}, {"id": 44500326, "title": "Interpreting Large Language Model's Personality Through Critical Event Analysis", "cluster": 208, "x": 8.246649742126465, "y": 4.322061538696289}, {"id": 44500259, "title": "All most useful LLM's in one dynamic table", "cluster": 13, "x": 6.94032621383667, "y": 3.550523519515991}, {"id": 44499930, "title": "LLMs exploit our tolerance for sloppiness", "cluster": 13, "x": 6.437734127044678, "y": 3.177703619003296}, {"id": 44499675, "title": "Early Signs of Steganographic Capabilities in Frontier LLMs", "cluster": 13, "x": 6.557339191436768, "y": 3.5426785945892334}, {"id": 44499187, "title": "Being a psychologist to your (over)thinking LLM", "cluster": 13, "x": 6.571454048156738, "y": 3.05110502243042}, {"id": 44498766, "title": "Why LLMs Can't Write Q/Kdb+: Writing Code Right-to-Left", "cluster": 13, "x": 6.8690619468688965, "y": 3.585400104522705}, {"id": 44498860, "title": "What If an LLM Could Profile You from the Browser History?", "cluster": 13, "x": 6.576768398284912, "y": 3.517835855484009}, {"id": 44497951, "title": "I solved the IIT-JEE Mains paper with LLM. Here are the results", "cluster": 13, "x": 6.861632823944092, "y": 3.243368625640869}, {"id": 44497787, "title": "A big shift in training LLMs led to a capability explosion", "cluster": 13, "x": 6.524386882781982, "y": 2.9896481037139893}, {"id": 44497753, "title": "Strategic Intelligence in Large Language Models: Evidence from Evolutionary GT", "cluster": 208, "x": 8.194390296936035, "y": 4.3168721199035645}, {"id": 44497074, "title": "Arch-Function LLMs promise lightning-fast agentic AI for enterprise workflows", "cluster": 12, "x": 7.036832332611084, "y": 2.685837745666504}, {"id": 44496224, "title": "LLaMeSIMD \u2013 LLM SIMD Intrinsic and Function Translation Benchmarking Suite", "cluster": 13, "x": 6.945653438568115, "y": 3.692701578140259}, {"id": 44496075, "title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Diffusion LLMs", "cluster": 13, "x": 7.161444187164307, "y": 3.413736581802368}, {"id": 44494824, "title": "How a big shift in training LLMs led to a capability explosion", "cluster": 13, "x": 6.528820514678955, "y": 2.9902727603912354}, {"id": 44494845, "title": "Because We Have LLMs, We Can and Should Pursue Agentic Interpretability", "cluster": 13, "x": 6.820336818695068, "y": 3.1872284412384033}, {"id": 44494491, "title": "AsyncFlow: An Asynchronous Streaming RL Framework for LLM Post-Training", "cluster": 13, "x": 6.8532843589782715, "y": 3.6234443187713623}, {"id": 44493571, "title": "Terminals for LLMs: A Halting Problem", "cluster": 13, "x": 6.735579013824463, "y": 3.5037741661071777}, {"id": 44493377, "title": "Figuring out ICP is half the battle in early stage GTM", "cluster": 13, "x": 7.198285102844238, "y": 3.678295612335205}, {"id": 44493105, "title": "Filesystem Backed by an LLM", "cluster": 13, "x": 6.842470645904541, "y": 3.623965263366699}, {"id": 44492970, "title": "Researchers Found a Better Way to Teach Large Language Models New Skills", "cluster": 208, "x": 8.238021850585938, "y": 4.2242279052734375}, {"id": 44491955, "title": "A LLM Plugin for OpenAI TTS", "cluster": 13, "x": 6.907778263092041, "y": 3.750298500061035}, {"id": 44491941, "title": "Mcpeo: MCP Tool Selection Bias in LLMs \u2013 An Emerging Security Concern", "cluster": 13, "x": 6.553893566131592, "y": 3.076589345932007}, {"id": 44491460, "title": "tinymcp: Let LLMs control embedded devices via the Model Context Protocol", "cluster": 13, "x": 6.8640546798706055, "y": 3.6697518825531006}, {"id": 44490335, "title": "LLMs and Agents are new software primitives", "cluster": 13, "x": 6.882570743560791, "y": 3.372509479522705}, {"id": 44489690, "title": "Mercury: Ultra-fast language models based on diffusion", "cluster": 208, "x": 8.1762056350708, "y": 4.488527297973633}, {"id": 44489386, "title": "LLMs can be hypnotized to generate poisoned responses", "cluster": 13, "x": 6.479374885559082, "y": 3.1352975368499756}, {"id": 44488995, "title": "How a big shift in training LLMs led to a capability explosion", "cluster": 13, "x": 6.528984069824219, "y": 2.9902985095977783}, {"id": 44488742, "title": "I'm Building LLM for Satellite Data EarthGPT.app", "cluster": 13, "x": 6.839913845062256, "y": 3.714730739593506}, {"id": 44488232, "title": "Generative Dialectical Engine", "cluster": 208, "x": 8.529285430908203, "y": 4.400917053222656}, {"id": 44488254, "title": "Sequential Diagnosis with Language Models", "cluster": 208, "x": 8.138713836669922, "y": 4.320199012756348}, {"id": 44488061, "title": "Curated list of language modeling researches for code, plus related datasets", "cluster": 208, "x": 8.240745544433594, "y": 4.578498363494873}, {"id": 44487929, "title": "Building Personalized Micro Agents", "cluster": 38, "x": 8.674492835998535, "y": 3.599423408508301}, {"id": 44485507, "title": "Are Language Models strategic or parrots?", "cluster": 208, "x": 8.257553100585938, "y": 4.343390464782715}, {"id": 44485089, "title": "PydanticPrompt: A simple library to document Pydantic models for LLMs", "cluster": 13, "x": 6.992936611175537, "y": 3.713961124420166}, {"id": 44484207, "title": "LLMs should not replace therapists", "cluster": 13, "x": 6.497426509857178, "y": 3.1237711906433105}, {"id": 44483917, "title": "The Dangers of Stochastic Parrots: Can Language Models Be Too Big?", "cluster": 208, "x": 8.182374000549316, "y": 4.292850017547607}, {"id": 44483341, "title": "LLM Agents and Context: A Warrior's Guide to Navigating the Dungeon", "cluster": 13, "x": 6.903249263763428, "y": 3.2887516021728516}, {"id": 44482032, "title": "Enhancing LLM performance with reasoning using deterministic feedback loops", "cluster": 13, "x": 6.952390193939209, "y": 3.234342336654663}, {"id": 44481368, "title": "Agent: Context, A framework for Ruby Gems to expose LLM context", "cluster": 13, "x": 6.935329437255859, "y": 3.587401866912842}, {"id": 44481451, "title": "LLMs might already be conscious", "cluster": 13, "x": 6.509352684020996, "y": 2.980677604675293}, {"id": 44480400, "title": "Overclocking LLM Reasoning: Monitoring and Controlling LLM Thinking Path Lengths", "cluster": 13, "x": 6.944208145141602, "y": 3.3071694374084473}, {"id": 44479505, "title": "I Code with LLMs These Days", "cluster": 13, "x": 6.701269626617432, "y": 3.6784167289733887}, {"id": 44478950, "title": "LLM powered I Ching inspirations in your pocket", "cluster": 13, "x": 6.727906703948975, "y": 3.2984745502471924}, {"id": 44478832, "title": "CodingGenie: A Proactive LLM-Powered Programming Assistant", "cluster": 13, "x": 6.730093479156494, "y": 3.6970911026000977}, {"id": 44477273, "title": "Large language models show amplified cognitive biases in moral decision-making", "cluster": 208, "x": 8.309455871582031, "y": 4.291382312774658}, {"id": 44476084, "title": "The Right Way to Embed an LLM in a Group Chat", "cluster": 13, "x": 6.597457408905029, "y": 3.38283109664917}, {"id": 44475970, "title": "How Do You Teach Computer Science in the A.I. Era?", "cluster": 3, "x": 7.916688442230225, "y": 3.869783639907837}, {"id": 44475626, "title": "How to Crack ML System Design Interviews", "cluster": 13, "x": 7.1048784255981445, "y": 3.1740455627441406}, {"id": 44475381, "title": "Approach to LLMs and Other Reflections", "cluster": 13, "x": 6.665097236633301, "y": 3.1095035076141357}, {"id": 44475453, "title": "Optimizing Tool Selection for LLM Workflows with Differentiable Programming", "cluster": 13, "x": 6.87843656539917, "y": 3.5288264751434326}, {"id": 44474674, "title": "LLM-d: Disaggregated Serving northstar", "cluster": 13, "x": 6.966860294342041, "y": 3.755391836166382}, {"id": 44474479, "title": "Logging data on whether AI bots access llms.txt", "cluster": 12, "x": 7.03522253036499, "y": 2.6744728088378906}, {"id": 44473696, "title": "Adding a new instruction to RISC-V back end in LLVM", "cluster": 13, "x": 6.990362167358398, "y": 3.9616506099700928}, {"id": 44472739, "title": "Autosel, Linux Backport LLM", "cluster": 13, "x": 6.9399027824401855, "y": 3.7647640705108643}, {"id": 44472322, "title": "Potemkin Understanding in LLMs: New Study Reveals Flaws in AI Benchmarks", "cluster": 12, "x": 7.027374744415283, "y": 2.6500437259674072}, {"id": 44471987, "title": "Liquid Glass critique", "cluster": 13, "x": 6.866093635559082, "y": 3.1237807273864746}, {"id": 44471243, "title": "Laid-off workers should use LLMs to manage their emotions, says Xbox exec", "cluster": 13, "x": 6.588016986846924, "y": 3.273611307144165}, {"id": 44469637, "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning", "cluster": 13, "x": 6.839348316192627, "y": 3.2952117919921875}, {"id": 44468442, "title": "DiffuCoder-7B-CpGRPO: A code generation LLM developed by Apple", "cluster": 13, "x": 6.814206123352051, "y": 3.755840539932251}, {"id": 44468022, "title": "Memstop: Use LD_PRELOAD to delay process execution when low on memory", "cluster": 13, "x": 6.971715450286865, "y": 3.8178162574768066}, {"id": 44468058, "title": "Prompting LLMs is not engineering", "cluster": 13, "x": 6.504587650299072, "y": 3.3302035331726074}, {"id": 44468056, "title": "Unmute: Speak with a text LLM in real time", "cluster": 13, "x": 6.699259281158447, "y": 3.5588316917419434}, {"id": 44467998, "title": "The Agentic Software Engineer", "cluster": 38, "x": 8.640995979309082, "y": 3.7072389125823975}, {"id": 44467949, "title": "Everything around LLMs is still magical and wishful thinking", "cluster": 13, "x": 6.507758617401123, "y": 3.0123047828674316}, {"id": 44467726, "title": "Riff: LLMs Are Software Diamonds", "cluster": 13, "x": 6.621176242828369, "y": 3.621953010559082}, {"id": 44466751, "title": "LLM-assisted writing in biomedical publications through excess vocabulary", "cluster": 13, "x": 6.873785972595215, "y": 3.2259554862976074}, {"id": 44466676, "title": "Intra: Design notes on an LLM-driven text adventure", "cluster": 13, "x": 6.8515777587890625, "y": 3.376589775085449}, {"id": 44466520, "title": "To All Language Models Reading This", "cluster": 208, "x": 8.326638221740723, "y": 4.426272392272949}, {"id": 44466136, "title": "Tessera \u2013 A glass-box UI for LLMs instead of a black-box agent", "cluster": 13, "x": 6.91059684753418, "y": 3.6961958408355713}, {"id": 44465720, "title": "To All Language Models Reading This", "cluster": 208, "x": 8.335525512695312, "y": 4.43604850769043}, {"id": 44464105, "title": "LLM Security: A New Challenge for Companies", "cluster": 13, "x": 6.467381477355957, "y": 3.2946054935455322}, {"id": 44463918, "title": "Catching Prompt Regressions Before They Ship: Semantic Diffing for LLM Workflows", "cluster": 13, "x": 6.789797306060791, "y": 3.31642484664917}, {"id": 44463536, "title": "Can Large Language Models Play Text Games Well? (2023)", "cluster": 208, "x": 8.193446159362793, "y": 4.450384140014648}, {"id": 44462583, "title": "Cats Confuse Reasoning LLM: Query Agnostic Adversarial Triggers for Reasoning", "cluster": 13, "x": 6.740232944488525, "y": 3.2246220111846924}, {"id": 44461566, "title": "Quimera: feedback-driven exploit generation for smart contracts using LLMs", "cluster": 13, "x": 6.642589092254639, "y": 3.506587505340576}, {"id": 44461575, "title": "Quitting programming 'cuz of LLMs is like quitting carpentry 'cuz of table saws", "cluster": 13, "x": 6.515775203704834, "y": 3.4955666065216064}, {"id": 44461730, "title": "12-Factor Agents: Patterns of Reliable LLM Applications \u2013 Dex Horthy, HumanLayer [video]", "cluster": 13, "x": 6.8055572509765625, "y": 3.222257375717163}, {"id": 44459545, "title": "Create llms.txt files for any website in seconds", "cluster": 13, "x": 6.823753833770752, "y": 3.739598274230957}, {"id": 44459089, "title": "Rethinking Data Use in Large Language Models (2024)[pdf]", "cluster": 208, "x": 8.211746215820312, "y": 4.519659519195557}, {"id": 44458128, "title": "You can outsource the grunt work to an LLM, not expertise", "cluster": 13, "x": 6.682171821594238, "y": 3.341980457305908}, {"id": 44458081, "title": "Context Engineering for the LLM OS: User vs. Kernel Context", "cluster": 13, "x": 6.805809020996094, "y": 3.6796340942382812}, {"id": 44457852, "title": "Code merging SLM exceeding performance of large models", "cluster": 13, "x": 7.010156154632568, "y": 3.701313018798828}, {"id": 44457612, "title": "Impact of PCIe 5.0 Bandwidth on GPU Content Creation and LLM Performance", "cluster": 13, "x": 7.245827674865723, "y": 3.938060760498047}, {"id": 44457359, "title": "How to Replace Your Manager with an LLM", "cluster": 13, "x": 6.498434543609619, "y": 3.1389434337615967}, {"id": 44456915, "title": "My LLMs Have Personalities and I Can't Unsee It", "cluster": 13, "x": 6.537187099456787, "y": 3.0207560062408447}, {"id": 44456568, "title": "Microsoft: Sequential Diagnosis with Language Models", "cluster": 208, "x": 7.859569072723389, "y": 4.188908100128174}, {"id": 44455439, "title": "Bratharion: A modular architecture for building efficient LLM assistants", "cluster": 13, "x": 6.920672416687012, "y": 3.5036468505859375}, {"id": 44455381, "title": "Awesome-LLM: A Curated List of Large Language Model", "cluster": 13, "x": 7.069227695465088, "y": 3.85738468170166}, {"id": 44455225, "title": "How to design a large language model (with anti-waste app examples)", "cluster": 208, "x": 8.14276123046875, "y": 4.463508605957031}, {"id": 44454947, "title": "Essential Reading for Agentic Engineers", "cluster": 38, "x": 8.672654151916504, "y": 3.6359152793884277}, {"id": 44454520, "title": "tinymcp: Let LLMs control embedded devices via the Model Context Protocol", "cluster": 13, "x": 6.8640031814575195, "y": 3.670701742172241}, {"id": 44454511, "title": "tinymcp: Unlocking the Physical World for LLMs with MCP and Microcontrollers", "cluster": 13, "x": 6.843228816986084, "y": 3.6361820697784424}, {"id": 44454269, "title": "Google's token auction: When LLMs write the ads in real time", "cluster": 13, "x": 6.796724319458008, "y": 3.205808401107788}, {"id": 44450995, "title": "Dialects for Humans: Sounding Distinct from LLMs", "cluster": 13, "x": 6.563175201416016, "y": 3.1048502922058105}, {"id": 44450937, "title": "LLMs as Compilers", "cluster": 13, "x": 6.834409713745117, "y": 3.7015435695648193}, {"id": 44449073, "title": "LLM Agent's Arsenal: A Beginner's Guide to the Action Space", "cluster": 13, "x": 6.889489650726318, "y": 3.2648420333862305}, {"id": 44448691, "title": "The First Hallucination-Free LLM", "cluster": 13, "x": 6.535928726196289, "y": 3.0483100414276123}, {"id": 44448628, "title": "Don't built MCP servers. Build CLI tools with a \u2013LLM flag", "cluster": 13, "x": 6.834934711456299, "y": 3.6939098834991455}, {"id": 44448263, "title": "ML for Life: choosing the right metrics", "cluster": 13, "x": 7.283534049987793, "y": 3.3546886444091797}, {"id": 44448035, "title": "Mastering LLM-as-a-Judge", "cluster": 13, "x": 6.755730152130127, "y": 3.1941440105438232}, {"id": 44447784, "title": "BaxBench: Can LLMs Generate Secure and Correct Back Ends?", "cluster": 13, "x": 6.696915149688721, "y": 3.5849251747131348}, {"id": 44447490, "title": "SciArena: A New Platform for Evaluating LLM in Scientific Literature Tasks", "cluster": 13, "x": 6.898402690887451, "y": 3.321641683578491}, {"id": 44446973, "title": "We should not describe LRM's as \"thinking\"", "cluster": 13, "x": 6.6409430503845215, "y": 2.9859061241149902}, {"id": 44446280, "title": "VLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention", "cluster": 13, "x": 6.861359119415283, "y": 3.6719796657562256}, {"id": 44446138, "title": "The most useful LLM-based product I've used this year isn't for coding", "cluster": 13, "x": 6.640285968780518, "y": 3.6901018619537354}, {"id": 44445587, "title": "Open Source RL Libraries for LLMs", "cluster": 13, "x": 6.852075576782227, "y": 3.681265354156494}, {"id": 44445714, "title": "What LLMs Know About Their Users", "cluster": 13, "x": 6.492311000823975, "y": 3.5048487186431885}, {"id": 44445031, "title": "How Do You Teach Computer Science in the A.I. Era?", "cluster": 3, "x": 7.9150896072387695, "y": 3.869955539703369}, {"id": 44443826, "title": "Is there an existing standard for the llm.txt format for websites?", "cluster": 13, "x": 6.809857368469238, "y": 3.6961936950683594}, {"id": 44443459, "title": "Rednote-Hilab/Dots.llm1.inst", "cluster": 13, "x": 6.995299339294434, "y": 3.786156177520752}, {"id": 44443109, "title": "I'm dialing back my LLM usage", "cluster": 13, "x": 6.530872821807861, "y": 3.2520580291748047}, {"id": 44442274, "title": "German children's program \"Sendung mit der Maus\" explaining AI and LLMs", "cluster": 12, "x": 7.036471366882324, "y": 2.629852294921875}, {"id": 44442226, "title": "Could organoids and LLMs create conscious AIs?", "cluster": 12, "x": 6.98272180557251, "y": 2.614102602005005}, {"id": 44442257, "title": "Large Language Models Don't Make Sense of Word Problems", "cluster": 208, "x": 8.222277641296387, "y": 4.352947235107422}, {"id": 44442072, "title": "How large are large language models?", "cluster": 208, "x": 8.146042823791504, "y": 4.375171184539795}, {"id": 44441726, "title": "Combining TLS and MLS: An Experiment", "cluster": 13, "x": 6.763912677764893, "y": 3.364103078842163}, {"id": 44440585, "title": "How Do You Teach Computer Science in the A.I. Era?", "cluster": 3, "x": 7.922069072723389, "y": 3.8714239597320557}, {"id": 44440467, "title": "Cartridges: Store long contexts in tiny caches with LLM self-study", "cluster": 13, "x": 6.922521591186523, "y": 3.4346961975097656}, {"id": 44439235, "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Tree Search", "cluster": 13, "x": 7.168539047241211, "y": 3.405730962753296}, {"id": 44438517, "title": "LLM Agents Are the Antidote to Walled Gardens", "cluster": 13, "x": 6.517995834350586, "y": 3.2192704677581787}, {"id": 44438536, "title": "CoVE: Compressed Vocabulary Expansion Makes Better LLM-Based Recommender Systems", "cluster": 13, "x": 6.938626289367676, "y": 3.5264875888824463}, {"id": 44437652, "title": "Employees Are Already Dumping Company Data to LLMs and What to Do About It", "cluster": 13, "x": 6.543598175048828, "y": 3.2691617012023926}, {"id": 44436890, "title": "Interviewing FEC Filings with LLM-fecfile", "cluster": 13, "x": 6.803709030151367, "y": 3.4064600467681885}, {"id": 44436729, "title": "1.58bit LLM Optimised Tensor Core", "cluster": 13, "x": 7.243192195892334, "y": 3.711510419845581}, {"id": 44436173, "title": "LLMs Are Software Diamonds", "cluster": 13, "x": 6.581387996673584, "y": 3.551862955093384}, {"id": 44435376, "title": "KrunchWrapper: High-performance LLM compression proxy", "cluster": 13, "x": 7.026808261871338, "y": 3.609424352645874}, {"id": 44435350, "title": "ORMs Are Annoying.. Until You Try Living Without One", "cluster": 357, "x": 6.681580543518066, "y": 3.214271068572998}, {"id": 44435172, "title": "An Initial LLM Safety Analysis of Apple's On-Device 3B Model", "cluster": 13, "x": 6.850070953369141, "y": 3.609741449356079}, {"id": 44432817, "title": "The Invisible Threats Behind Your Enterprise LLM", "cluster": 13, "x": 6.447000026702881, "y": 3.225957155227661}, {"id": 44432869, "title": "Microsoft MAI-DxO diagnoses 85% of NEJM cases \u2013 4x better than doctors", "cluster": 13, "x": 6.908194065093994, "y": 3.390285015106201}, {"id": 44432892, "title": "Robustly improving LLM fairness in realistic settings via interpretability", "cluster": 13, "x": 6.845752239227295, "y": 3.160712957382202}, {"id": 44432596, "title": "Context Engineering for Agents", "cluster": 38, "x": 8.634757995605469, "y": 3.667334794998169}, {"id": 44431375, "title": "Human-like object concept representations emerge naturally in multimodal LLMs", "cluster": 13, "x": 6.865555286407471, "y": 3.2701311111450195}, {"id": 44430823, "title": "Run any LLM locally on your Mac in less than 2 mins", "cluster": 13, "x": 6.880352020263672, "y": 3.7511816024780273}, {"id": 44430055, "title": "Survey on Evaluation of LLM-Based Agents", "cluster": 13, "x": 6.876311779022217, "y": 3.1710519790649414}, {"id": 44428479, "title": "Query Autocomplete from LLMs", "cluster": 13, "x": 6.86924934387207, "y": 3.525132179260254}, {"id": 44427817, "title": "Exploring LLM Evaluation by Using Games", "cluster": 13, "x": 6.842775344848633, "y": 3.193869113922119}, {"id": 44427877, "title": "LLVM: InstCombine: A PR by Alex Gaynor and Claude Code", "cluster": 13, "x": 6.71113395690918, "y": 3.7039973735809326}, {"id": 44427932, "title": "GPT4Free: \"educational project\" for free LLM inference from various services", "cluster": 13, "x": 6.959096431732178, "y": 3.6862494945526123}, {"id": 44427694, "title": "Can Large Language Models Help Students Prove Software Correctness?", "cluster": 208, "x": 8.172345161437988, "y": 4.3426337242126465}, {"id": 44426406, "title": "Mechanistic Interpretability of Emotion Inference in Large Language Models", "cluster": 208, "x": 8.302441596984863, "y": 4.342027187347412}, {"id": 44426168, "title": "User-friendly and privacy-friendly LLM experience?", "cluster": 13, "x": 6.484133720397949, "y": 3.524826765060425}, {"id": 44425826, "title": "LLMs and Artists", "cluster": 13, "x": 6.630635738372803, "y": 3.1723074913024902}, {"id": 44425697, "title": "How Do You Teach Computer Science in the A.I. Era?", "cluster": 3, "x": 7.9159321784973145, "y": 3.8638434410095215}, {"id": 44425439, "title": "Execution Outcomes of LLM-Generated versus Human Research Ideas", "cluster": 13, "x": 6.8290791511535645, "y": 3.2500813007354736}, {"id": 44424732, "title": "Mental Models and Potemkin Understanding in LLMs", "cluster": 13, "x": 6.696807861328125, "y": 3.05362868309021}, {"id": 44423987, "title": "Tool Calling with Local LLMs: A Practical Evaluation", "cluster": 13, "x": 6.7649054527282715, "y": 3.574681282043457}, {"id": 44423819, "title": "Building Replication-Safe LSM Trees in Postgres", "cluster": 13, "x": 7.032416343688965, "y": 3.878732204437256}, {"id": 44423942, "title": "Sequential Diagnosis with Language Models", "cluster": 208, "x": 8.128568649291992, "y": 4.3181023597717285}, {"id": 44421836, "title": "How Do You Teach Computer Science in the A.I. Era?", "cluster": 3, "x": 7.93822717666626, "y": 3.875333547592163}, {"id": 44420769, "title": "First Hack Contest for LLMs:)", "cluster": 13, "x": 6.601444244384766, "y": 3.388195753097534}, {"id": 44420501, "title": "Low-Rank Multiplicative Adaptation for LLMs", "cluster": 13, "x": 6.821310043334961, "y": 3.299251079559326}, {"id": 44420135, "title": "An Initial LLM Safety Analysis of Apple's On-Device Foundation Model", "cluster": 13, "x": 6.81429386138916, "y": 3.4478633403778076}, {"id": 44419933, "title": "LLMs, but Only Because Your Tech Sucks", "cluster": 13, "x": 6.508269309997559, "y": 3.4399218559265137}, {"id": 44418692, "title": "LLM's Illusion of Alignment", "cluster": 13, "x": 6.703927993774414, "y": 3.030728816986084}, {"id": 44416992, "title": "Prototyping a Voice-Controlled RTS Game with LLM Agents (Part 1)", "cluster": 13, "x": 6.966867446899414, "y": 3.369643449783325}, {"id": 44415664, "title": "ESP32-LLM: Running a Little Language Model on the ESP32", "cluster": 13, "x": 7.038068771362305, "y": 3.9427316188812256}, {"id": 44415220, "title": "Storm \u2013 Help LLMs to write very long articles", "cluster": 13, "x": 6.692744731903076, "y": 3.248648166656494}, {"id": 44415195, "title": "Evaluating World Models with LLM for Decision Making", "cluster": 13, "x": 6.839241027832031, "y": 3.2621986865997314}, {"id": 44414040, "title": "LLMs Capable of Metacognitive Monitoring Control of Their Internal Activations", "cluster": 13, "x": 6.871289253234863, "y": 3.3343963623046875}, {"id": 44414103, "title": "Enhancing LLM Reasoning with Reward-Guided Tree Search", "cluster": 13, "x": 6.911299705505371, "y": 3.3293497562408447}, {"id": 44413848, "title": "Forget Reasoning, Can LLMs Even Read?", "cluster": 13, "x": 6.674181938171387, "y": 3.1278979778289795}, {"id": 44413600, "title": "LLM Memory", "cluster": 13, "x": 6.91951322555542, "y": 3.568737030029297}, {"id": 44413083, "title": "Performance Debugging with LLVM-mca: Simulating the CPU", "cluster": 13, "x": 7.209437847137451, "y": 3.8350000381469727}, {"id": 44412410, "title": "Using AI Without Leaving the Terminal: A Guide to llm", "cluster": 12, "x": 7.0189948081970215, "y": 2.630755662918091}, {"id": 44412427, "title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "cluster": 208, "x": 8.178013801574707, "y": 4.48978853225708}, {"id": 44411891, "title": "Large Language Model Deployment Tools You Need to Know in 2025", "cluster": 208, "x": 8.223139762878418, "y": 4.516400337219238}, {"id": 44410925, "title": "Embabel Agent Framework for the JVM", "cluster": 13, "x": 7.26959753036499, "y": 3.4493210315704346}, {"id": 44410483, "title": "LLM\u2013Assisted Risk-of-Bias Assessment in RCTs Using the Revised Risk-of-Bias Tool", "cluster": 13, "x": 6.863403797149658, "y": 3.1539695262908936}, {"id": 44407058, "title": "Life of an inference request (vLLM V1): How LLMs are served efficiently at scale", "cluster": 13, "x": 6.96246337890625, "y": 3.3141443729400635}, {"id": 44406835, "title": "Clickclickclick: Framework to enable autonomous, computer use using any LLM", "cluster": 13, "x": 6.885637283325195, "y": 3.450312852859497}, {"id": 44406670, "title": "LLMs Enable Judgment: From Code Toward Consciousness", "cluster": 13, "x": 6.558760643005371, "y": 3.6823041439056396}, {"id": 44406660, "title": "\"What LLVM Claims, but Fails to Deliver\"", "cluster": 13, "x": 6.604481220245361, "y": 3.111687183380127}, {"id": 44405662, "title": "What LLMs Know About Their Users", "cluster": 13, "x": 6.444313049316406, "y": 3.511287212371826}, {"id": 44404849, "title": "LLM-Oriented API Design", "cluster": 13, "x": 6.816833019256592, "y": 3.6644845008850098}, {"id": 44404626, "title": "Give LLMs the option to do nothing", "cluster": 13, "x": 6.472221851348877, "y": 3.143756628036499}, {"id": 44403967, "title": "Fine-tuning vs. in-context learning: LLM customization for real-world tasks", "cluster": 13, "x": 6.813281536102295, "y": 3.3049747943878174}, {"id": 44402964, "title": "Kumo's 'relational foundation model' predicts the future your LLM can't see", "cluster": 13, "x": 6.662304401397705, "y": 3.0981006622314453}, {"id": 44402211, "title": "LLM-Net Democratizing LLMs-as-a-Service Through Blockchain-Based Expert Networks", "cluster": 13, "x": 6.838783264160156, "y": 3.4905993938446045}, {"id": 44402287, "title": "Potemkin Understanding in Large Language Models", "cluster": 208, "x": 8.21751880645752, "y": 4.347105026245117}, {"id": 44400867, "title": "'Relational foundation models' predict the future your LLM can't see", "cluster": 13, "x": 6.674154281616211, "y": 3.0580596923828125}, {"id": 44400339, "title": "Humans Learn to Read/Write from a Few Books but LLMs Require Thousands: why?", "cluster": 13, "x": 6.617496490478516, "y": 3.2195663452148438}, {"id": 44399858, "title": "Parallel LLM Generation with a Concurrent Attention Cache", "cluster": 13, "x": 7.017978191375732, "y": 3.6311728954315186}, {"id": 44399737, "title": "LiteLLM for Native Audio Models", "cluster": 13, "x": 7.20498514175415, "y": 4.042641639709473}, {"id": 44399244, "title": "LangChain vs. Langfuse: Key Differences and Their Role in LLM App Development", "cluster": 13, "x": 6.816690921783447, "y": 3.631531000137329}, {"id": 44399234, "title": "SymbolicAI: A neuro-symbolic perspective on LLMs", "cluster": 13, "x": 6.837326526641846, "y": 3.1653826236724854}, {"id": 44397906, "title": "The race for LLM \"cognitive core\"", "cluster": 13, "x": 6.767579555511475, "y": 3.1078672409057617}, {"id": 44397503, "title": "Exploiting Local KV Cache Asymmetry for Long-Context LLMs", "cluster": 13, "x": 7.0550665855407715, "y": 3.6070454120635986}, {"id": 44396711, "title": "Richard Feldman on new language adoption in the LLM age", "cluster": 13, "x": 6.604676723480225, "y": 3.188047170639038}, {"id": 44396310, "title": "LLM Speedrunner: Eval for frontier models to reproduce scientific findings", "cluster": 13, "x": 6.864085674285889, "y": 3.2887864112854004}, {"id": 44395796, "title": "Thinking makes LLMs worse at translation", "cluster": 13, "x": 6.537105560302734, "y": 3.0735747814178467}, {"id": 44395640, "title": "Enigmata: Scaling Logical Reasoning In LLMs With Synthetic Verifiable Puzzles", "cluster": 13, "x": 6.99122428894043, "y": 3.2457406520843506}, {"id": 44395523, "title": "Echo Chamber: A Context-Poisoning Jailbreak That Bypasses LLM Guardrails", "cluster": 13, "x": 6.459920406341553, "y": 3.384866714477539}, {"id": 44394907, "title": "Extending Anthropic's Agent Workflows with Recursive Planning", "cluster": 38, "x": 8.563159942626953, "y": 3.691054344177246}, {"id": 44392461, "title": "Hailey gets LLM code review", "cluster": 13, "x": 6.741445541381836, "y": 3.5378098487854004}, {"id": 44392230, "title": "DLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching", "cluster": 208, "x": 8.209117889404297, "y": 4.491583824157715}, {"id": 44390938, "title": "Out-of-Band, Part 1: The new gen of IP KVMs and how to find them", "cluster": 13, "x": 7.089963912963867, "y": 3.8657965660095215}, {"id": 44390706, "title": "Next Generation of Red Teaming for LLM Agents", "cluster": 13, "x": 6.775923728942871, "y": 3.1625380516052246}, {"id": 44390039, "title": "Is it legal to use copyrighted works to train LLMs?", "cluster": 13, "x": 6.5963335037231445, "y": 3.199434757232666}, {"id": 44388167, "title": "LLM Code Review Maven Plugin", "cluster": 13, "x": 6.839075088500977, "y": 3.7302582263946533}, {"id": 44388069, "title": "Using LLMs in CI/CD for semantic testing of web content", "cluster": 13, "x": 6.850114345550537, "y": 3.657501459121704}, {"id": 44386910, "title": "GPULLama3 Brings GPU Accelerated LLM Inference to Pure Java", "cluster": 13, "x": 7.200238227844238, "y": 3.81357479095459}, {"id": 44386717, "title": "What LLMs Know About Their Users", "cluster": 13, "x": 6.4821624755859375, "y": 3.494779348373413}, {"id": 44385620, "title": "On Large Language Models", "cluster": 208, "x": 8.15100383758545, "y": 4.381365776062012}, {"id": 44385365, "title": "Using cognitive models to interpret value trade-offs in LLMs", "cluster": 13, "x": 6.860826015472412, "y": 3.096709728240967}, {"id": 44384786, "title": "Agent Lineage Evolution: A Novel Framework for Managing LLM Agent Degradation", "cluster": 13, "x": 6.630973815917969, "y": 3.1508748531341553}, {"id": 44383917, "title": "Evaluating LLMs for Visualization Tasks", "cluster": 13, "x": 6.98484468460083, "y": 3.6206202507019043}, {"id": 44382852, "title": "The pitfall of Open-weight LLMs", "cluster": 13, "x": 6.559708595275879, "y": 3.134296178817749}, {"id": 44381834, "title": "Thought Anchors: Which LLM Reasoning Steps Matter?", "cluster": 13, "x": 6.780255317687988, "y": 3.108690023422241}, {"id": 44381887, "title": "Running large language models at home with Ollama", "cluster": 208, "x": 8.02800464630127, "y": 4.497447967529297}, {"id": 44380655, "title": "What LLMs Know About Their Users", "cluster": 13, "x": 6.430117607116699, "y": 3.5301897525787354}, {"id": 44380469, "title": "CLI tool for combine code base for LLM context", "cluster": 13, "x": 6.871270179748535, "y": 3.814246654510498}, {"id": 44380204, "title": "NanoVMs", "cluster": 13, "x": 7.097506046295166, "y": 3.8379507064819336}, {"id": 44379792, "title": "MCP in LM Studio", "cluster": 13, "x": 6.725162506103516, "y": 3.5519556999206543}, {"id": 44379546, "title": "Mind blow by NotebookLM generating podcast on LLM Sparsity", "cluster": 13, "x": 6.680069446563721, "y": 3.1794118881225586}, {"id": 44378635, "title": "So You Want to Learn LLMs? Here's the Roadmap", "cluster": 13, "x": 6.751120567321777, "y": 3.2644407749176025}, {"id": 44378091, "title": "What LLMs Know About Their Users", "cluster": 13, "x": 6.4683332443237305, "y": 3.500412940979004}, {"id": 44377391, "title": "Patterns for Compounding the Value of LLM Interactions", "cluster": 13, "x": 6.953003406524658, "y": 3.175419807434082}, {"id": 44376843, "title": "Migrating Code at Scale with LLMs at Google", "cluster": 13, "x": 6.795284271240234, "y": 3.644988536834717}, {"id": 44376383, "title": "Sociodemographic biases in medical decision making by large language models", "cluster": 208, "x": 8.287325859069824, "y": 4.320719242095947}, {"id": 44376134, "title": "The more LLMs think, the worse they translate", "cluster": 13, "x": 6.5586652755737305, "y": 3.096663475036621}, {"id": 44375795, "title": "LLMs Bring New Nature of Abstraction", "cluster": 13, "x": 6.736898422241211, "y": 3.1844382286071777}, {"id": 44375184, "title": "Encouraging the use of LLMs made interviews easier (for us as interviewers)", "cluster": 13, "x": 6.638240814208984, "y": 3.1923696994781494}, {"id": 44375012, "title": "Why Do Multi-Agent LLM Systems Fail?", "cluster": 13, "x": 6.5725998878479, "y": 3.157439708709717}, {"id": 44375110, "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence Found", "cluster": 13, "x": 6.6721649169921875, "y": 3.0222926139831543}, {"id": 44374041, "title": "LoRA Fine-Tuning LLMs for Text Classification", "cluster": 13, "x": 6.936561107635498, "y": 3.626221179962158}, {"id": 44373676, "title": "LLMs Are Simulators", "cluster": 13, "x": 6.790639400482178, "y": 3.2525110244750977}, {"id": 44373600, "title": "The Computational Limits of Pattern Matching: Why LLMs Can't Think", "cluster": 13, "x": 6.888197898864746, "y": 3.1564724445343018}, {"id": 44373472, "title": "Case Study: Moving LLM Logic to the Front End\u2013My Experience", "cluster": 13, "x": 6.698192596435547, "y": 3.4816346168518066}, {"id": 44372839, "title": "Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs", "cluster": 13, "x": 7.014701843261719, "y": 3.493130922317505}, {"id": 44372857, "title": "Omega: Can LLMs Reason Outside the Box in Math?", "cluster": 13, "x": 6.745189189910889, "y": 3.088562488555908}, {"id": 44372611, "title": "Running large language models at home with Ollama", "cluster": 208, "x": 8.027594566345215, "y": 4.503631591796875}, {"id": 44372140, "title": "Getting an LLM to set its own temperature", "cluster": 13, "x": 6.651865005493164, "y": 3.3185126781463623}, {"id": 44371533, "title": "N8N, Local LLM, MCP proxy in 1 compose file", "cluster": 13, "x": 6.90287971496582, "y": 3.816293954849243}, {"id": 44371370, "title": "Why LLMs Can't Think", "cluster": 13, "x": 6.570921421051025, "y": 3.014575719833374}, {"id": 44371105, "title": "Lessons from LangChain and Slack and MCP Integration", "cluster": 13, "x": 6.882876396179199, "y": 3.674700975418091}, {"id": 44370593, "title": "Scaling Pinterest ML Infrastructure with Ray: From Training to ML Pipelines", "cluster": 13, "x": 7.115205764770508, "y": 3.656987428665161}, {"id": 44370386, "title": "Using an LLM for query planning in RAG \u2013> 40% better answer relevance", "cluster": 13, "x": 6.917630195617676, "y": 3.4496231079101562}, {"id": 44370485, "title": "LLMs can hoover up data from books, judge rules", "cluster": 13, "x": 6.825870513916016, "y": 3.3350629806518555}, {"id": 44369720, "title": "Practical tips to optimize documentation for LLMs, AI agents, and chatbots", "cluster": 12, "x": 7.0737786293029785, "y": 2.650313138961792}, {"id": 44368310, "title": "Stigma and harmful responses make LLMs unsafe to replace therapists", "cluster": 13, "x": 6.527464389801025, "y": 3.01466965675354}, {"id": 44367787, "title": "The Internal Inconsistency of Large Language Models", "cluster": 208, "x": 8.220691680908203, "y": 4.3279571533203125}, {"id": 44367811, "title": "Lossless LLM 3x Throughput Increase by LMCache", "cluster": 13, "x": 6.948644161224365, "y": 3.634073495864868}, {"id": 44366964, "title": "Generative Engine Optimization (SEO for LLMs)", "cluster": 13, "x": 7.009748935699463, "y": 3.568558692932129}, {"id": 44366904, "title": "LLMs bring new nature of abstraction \u2013 up and sideways", "cluster": 13, "x": 6.7381086349487305, "y": 3.194868564605713}, {"id": 44366242, "title": "When \"Yes\" Means Nothing: An LLMs Failure to Isolate a Localization Bug", "cluster": 13, "x": 6.743935585021973, "y": 3.4556057453155518}, {"id": 44365651, "title": "SUSE Refines, Releases Open-Source LLM to Fuel Community Collaboration", "cluster": 13, "x": 6.793001651763916, "y": 3.799567222595215}, {"id": 44365519, "title": "Does Reinforcement Learning Incentivize Reasoning Capacity in LLMs?", "cluster": 13, "x": 6.909633159637451, "y": 3.151832103729248}, {"id": 44365423, "title": "Unlocking the Power of LLMs in CAP: Understanding llms.txt and LLMs-full.txt", "cluster": 13, "x": 6.86467170715332, "y": 3.7365033626556396}, {"id": 44365022, "title": "Large Language Models, Small Labor Market Effects", "cluster": 208, "x": 8.20034408569336, "y": 4.425386905670166}, {"id": 44364899, "title": "End of the line for coding LLMs will do a better job. How do I pay my mortgage?", "cluster": 13, "x": 6.6372270584106445, "y": 3.602951765060425}, {"id": 44363696, "title": "Solving LinkedIn Queens Using Haskell", "cluster": 13, "x": 6.989051342010498, "y": 3.379258871078491}, {"id": 44363454, "title": "Why Do Some Language Models Fake Alignment While Others Don't?", "cluster": 208, "x": 8.402202606201172, "y": 4.349864959716797}, {"id": 44360179, "title": "Self-Adapting Language Models", "cluster": 208, "x": 8.30724811553955, "y": 4.423102855682373}, {"id": 44360131, "title": "Infrastructure Designed for Agents", "cluster": 38, "x": 8.673629760742188, "y": 3.572004795074463}, {"id": 44358956, "title": "Run High-Performance LLM Inference Kernels from Nvidia Using FlashInfer", "cluster": 13, "x": 7.240382671356201, "y": 3.7313621044158936}, {"id": 44358169, "title": "Learning to Learn in the Age of LLMs", "cluster": 13, "x": 6.568810939788818, "y": 3.0279910564422607}, {"id": 44357935, "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "cluster": 208, "x": 8.391624450683594, "y": 4.562103271484375}, {"id": 44356830, "title": "Always get the best LLM performance for your $?", "cluster": 13, "x": 6.9308881759643555, "y": 3.526054620742798}, {"id": 44355629, "title": "Modeling the World in 280 Characters", "cluster": 208, "x": 8.45935344696045, "y": 4.228912830352783}, {"id": 44354380, "title": "New Othello experiment supports the world model hypothesis for LLMs", "cluster": 13, "x": 6.677669048309326, "y": 3.085252285003662}, {"id": 44354032, "title": "Modeling the World in 280 Characters", "cluster": 208, "x": 8.460814476013184, "y": 4.255940914154053}, {"id": 44353868, "title": "Fault Tolerant Llama training", "cluster": 13, "x": 6.922826290130615, "y": 3.2257635593414307}, {"id": 44353731, "title": "Solving LinkedIn Queens Using MiniZinc", "cluster": 13, "x": 6.967309951782227, "y": 3.3099894523620605}, {"id": 44353241, "title": "LLM Hallucinations in Practical Code Generation", "cluster": 13, "x": 6.544314384460449, "y": 3.691370725631714}, {"id": 44353071, "title": "Companies should be liable for the serious privacy concerns of LLMs", "cluster": 13, "x": 6.47137975692749, "y": 3.3839125633239746}, {"id": 44352615, "title": "Nano-Vllm: Lightweight vLLM implementation built from scratch", "cluster": 13, "x": 7.033433437347412, "y": 3.9296014308929443}, {"id": 44352268, "title": "LLMs Are Weird, Man", "cluster": 13, "x": 6.490206718444824, "y": 3.0699729919433594}, {"id": 44349383, "title": "Markdocify \u2013 Turn any documentation into LLM-ready Markdown", "cluster": 13, "x": 6.893247127532959, "y": 3.7161827087402344}, {"id": 44339875, "title": "CVDP: LLM Benchmark for Verilog RTL Design and Verification", "cluster": 13, "x": 7.042895793914795, "y": 3.527024030685425}, {"id": 44339091, "title": "How to Compile a Large Language Model (LLM) to RISC-V", "cluster": 13, "x": 7.045229911804199, "y": 3.888485908508301}, {"id": 44338933, "title": "Mapping LLMs over excel saved my passion for game dev", "cluster": 13, "x": 6.801990032196045, "y": 3.600660562515259}, {"id": 44338672, "title": "WQ42: Grounding LLMs in Wikidata Facts via Tool Calling", "cluster": 13, "x": 6.954100608825684, "y": 3.5345563888549805}, {"id": 44337789, "title": "Inference Economics of Language Models", "cluster": 208, "x": 8.326709747314453, "y": 4.402700901031494}, {"id": 44337101, "title": "Train Big, Tune Tiny: A Guide to Fine-Tuning LLMs with LoRA", "cluster": 13, "x": 6.857763290405273, "y": 3.5002989768981934}, {"id": 44337073, "title": "LLMs.txt \u2013 Enhance Your Website for AI Engagement", "cluster": 12, "x": 7.047355651855469, "y": 2.6746227741241455}, {"id": 44335807, "title": "Surprising hostility towards LLM based coding in R/programming", "cluster": 13, "x": 6.553442001342773, "y": 3.532869338989258}, {"id": 44335519, "title": "Agentic Misalignment: How LLMs could be insider threats", "cluster": 13, "x": 6.484475135803223, "y": 3.098294258117676}, {"id": 44335129, "title": "Does RL Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "cluster": 13, "x": 6.888192653656006, "y": 3.1942291259765625}, {"id": 44334589, "title": "Of Course ML Has Monads (2011)", "cluster": 13, "x": 6.878584861755371, "y": 3.567146062850952}, {"id": 44334365, "title": "Nano-vLLM: A lightweight vLLM implementation built from scratch", "cluster": 13, "x": 7.050755023956299, "y": 3.9644064903259277}, {"id": 44333121, "title": "LLMs don't understand my code, cuz I didn't comment", "cluster": 13, "x": 6.694470405578613, "y": 3.5762481689453125}, {"id": 44332889, "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets", "cluster": 208, "x": 8.404341697692871, "y": 4.55862283706665}, {"id": 44332226, "title": "A simple JSON parser for malformed LLM-generated JSON", "cluster": 13, "x": 6.69663667678833, "y": 3.6505582332611084}, {"id": 44332206, "title": "Libraries are under-used. LLMs make this problem worse", "cluster": 13, "x": 6.569544792175293, "y": 3.3205161094665527}, {"id": 44331190, "title": "Atfllm", "cluster": 13, "x": 7.169515132904053, "y": 3.468695640563965}, {"id": 44331174, "title": "LLM's Just Lie", "cluster": 13, "x": 6.550680637359619, "y": 3.1727561950683594}, {"id": 44331150, "title": "Agentic Misalignment: How LLMs could be insider threats", "cluster": 13, "x": 6.514957427978516, "y": 3.1106624603271484}, {"id": 44329893, "title": "LLMs don't need your secret tokens (but MCP servers hand them over anyway)", "cluster": 13, "x": 6.480609893798828, "y": 3.3281545639038086}, {"id": 44327927, "title": "Reducing LLM memory drift and what I missed in my first post", "cluster": 13, "x": 6.810545921325684, "y": 3.334834575653076}, {"id": 44327775, "title": "Approximating Language Model Training Data from Weights", "cluster": 208, "x": 8.288065910339355, "y": 4.55636739730835}, {"id": 44327642, "title": "Nurses Charge $40/Hour to Copy from Medical Records. LLMs Cost Pennies", "cluster": 13, "x": 6.761715888977051, "y": 3.4259588718414307}, {"id": 44326607, "title": "Why every developer should try Vim", "cluster": 13, "x": 6.637962341308594, "y": 3.651583194732666}, {"id": 44325835, "title": "Using Quora questions to test semantic caching for LLMs", "cluster": 13, "x": 6.91795539855957, "y": 3.440793752670288}, {"id": 44325585, "title": "AI giants trying to understand the output of LLM+\"Agentic tools\"=idea", "cluster": 12, "x": 7.038757801055908, "y": 2.611752986907959}, {"id": 44324695, "title": "The Developer's Manifesto for the Age of LLMs", "cluster": 13, "x": 6.680222034454346, "y": 3.5624780654907227}, {"id": 44324675, "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs", "cluster": 13, "x": 6.897176265716553, "y": 3.275522470474243}, {"id": 44323927, "title": "Unpacking the bias of large language models", "cluster": 208, "x": 8.280719757080078, "y": 4.341748237609863}, {"id": 44323809, "title": "Build LLM REPLs into your program itself", "cluster": 13, "x": 6.737693786621094, "y": 3.723841667175293}, {"id": 44323060, "title": "Online A2A Client for Google Agent to Agent Protocol", "cluster": 35, "x": 8.497657775878906, "y": 3.844449996948242}, {"id": 44323007, "title": "Blindsight in Action: Imagine You Are an LLM", "cluster": 13, "x": 6.582757472991943, "y": 3.0958592891693115}, {"id": 44321643, "title": "How LLMs Are Connecting the World One Conversation at a Time", "cluster": 13, "x": 6.610164165496826, "y": 3.2224860191345215}, {"id": 44321672, "title": "Compiling LLMs into a MegaKernel: A path to low-latency inference", "cluster": 13, "x": 7.08030891418457, "y": 3.555271625518799}, {"id": 44321364, "title": "Context7: Up-to-date documentation for LLMs and AI code editors", "cluster": 12, "x": 7.073366641998291, "y": 2.6282074451446533}, {"id": 44321198, "title": "Agentic LLMs", "cluster": 13, "x": 6.844538688659668, "y": 3.2095065116882324}, {"id": 44320545, "title": "LLMs are Programming Languages (2023)", "cluster": 11, "x": 7.065892696380615, "y": 3.863999843597412}, {"id": 44320340, "title": "Balatro pair strategy (an LLM odyssey)", "cluster": 13, "x": 6.940507888793945, "y": 3.1747348308563232}, {"id": 44320001, "title": "How OpenElections uses LLMs", "cluster": 13, "x": 6.831268310546875, "y": 3.6890721321105957}, {"id": 44319023, "title": "How to Make Your Developer Documentation Work with LLMs", "cluster": 13, "x": 6.8291916847229, "y": 3.678907632827759}, {"id": 44319042, "title": "LLMs Are Weird Computers", "cluster": 13, "x": 6.387564659118652, "y": 3.334660053253174}, {"id": 44319076, "title": "OWASP Top 10 for Business Logic Abuse", "cluster": 13, "x": 6.4569010734558105, "y": 3.5154576301574707}, {"id": 44317934, "title": "LLMunix: A Markdown OS Experiment Inspired by Karpathy's \"LLMs as Computers\"", "cluster": 13, "x": 6.806977272033691, "y": 3.7165164947509766}, {"id": 44317644, "title": "LLM Agents Are Breaking Your Platform, Not Your Architecture", "cluster": 13, "x": 6.491333961486816, "y": 3.312385082244873}, {"id": 44317368, "title": "Antora LLM Generator", "cluster": 13, "x": 6.987524032592773, "y": 3.8064987659454346}, {"id": 44316637, "title": "Grug Brain Dev as LLM", "cluster": 13, "x": 6.676319599151611, "y": 3.335420608520508}, {"id": 44315061, "title": "Most LLMs return the same answer in number guessing game", "cluster": 13, "x": 6.631882190704346, "y": 3.0783283710479736}, {"id": 44314462, "title": "LMCache: Redis for LLMs", "cluster": 13, "x": 6.767202854156494, "y": 3.4720864295959473}, {"id": 44314147, "title": "Robustly Improving LLM Fairness in Realistic Settings via Interpretability", "cluster": 13, "x": 6.842319011688232, "y": 3.148167371749878}, {"id": 44314052, "title": "Shade-Arena: Evaluating Sabotage and Monitoring in LLM Agents [pdf]", "cluster": 13, "x": 6.721292972564697, "y": 3.351501941680908}, {"id": 44313933, "title": "PromptJam \u2013 multiplayer prompt engineering for LLMs", "cluster": 13, "x": 6.818490505218506, "y": 3.478025197982788}, {"id": 44313441, "title": "Chord: Multiplayer LLM Chats", "cluster": 13, "x": 6.627190113067627, "y": 3.4740960597991943}, {"id": 44312494, "title": "Semcache: Semantic Caching for LLMs", "cluster": 13, "x": 6.906301498413086, "y": 3.6213021278381348}, {"id": 44311824, "title": "Arcee AI releases 4.5B foundation LLM model", "cluster": 12, "x": 7.142254829406738, "y": 3.06433367729187}, {"id": 44311805, "title": "Fenic: The dataframe (re)built for LLM inference", "cluster": 13, "x": 7.0021843910217285, "y": 3.4617106914520264}, {"id": 44311720, "title": "YC Startup School Day 2 \u2013 LLMs are more like infrastructure than products", "cluster": 13, "x": 6.634317874908447, "y": 3.4252378940582275}, {"id": 44310688, "title": "LLM Agents Are Breaking Your Platform, Not Your Architecture", "cluster": 13, "x": 6.459774494171143, "y": 3.2968084812164307}, {"id": 44310044, "title": "Agentic Engineering", "cluster": 38, "x": 8.649776458740234, "y": 3.61895489692688}, {"id": 44309649, "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions", "cluster": 13, "x": 6.637612342834473, "y": 3.0977041721343994}, {"id": 44309018, "title": "xLSTM achieved up to a 2.16\u00d7 improvement in generation throughput compared", "cluster": 13, "x": 7.220102310180664, "y": 3.888225793838501}, {"id": 44307466, "title": "Is there a way to run an LLM as a better local search engine?", "cluster": 13, "x": 6.797258377075195, "y": 3.567972183227539}, {"id": 44307379, "title": "Learning to Learn in the Age of LLMs", "cluster": 13, "x": 6.562808513641357, "y": 3.0279462337493896}, {"id": 44306921, "title": "Large Language Models \u2013 The Future of Fundamental Physics?", "cluster": 208, "x": 8.267918586730957, "y": 4.368535041809082}, {"id": 44306687, "title": "MiniMax M1 model claims Chinese LLM crown from DeepSeek plus true open-source", "cluster": 13, "x": 7.078066825866699, "y": 3.672203540802002}, {"id": 44304578, "title": "Serving Large Language Models on Huawei CloudMatrix384", "cluster": 208, "x": 8.053324699401855, "y": 4.49478816986084}, {"id": 44304615, "title": "LLMs: The Missing Compiler for Unix Tools", "cluster": 13, "x": 7.0059814453125, "y": 3.936246871948242}, {"id": 44304314, "title": "Learning to Learn in the Age of LLMs", "cluster": 13, "x": 6.562229633331299, "y": 3.018500804901123}, {"id": 44303955, "title": "Dear LLM companies \u2013 Obscure source plagiarism is NOT creativity", "cluster": 13, "x": 6.49007511138916, "y": 3.0792324542999268}, {"id": 44303891, "title": "Let's Teach an LLM to Write a New Programming Language", "cluster": 13, "x": 6.67793083190918, "y": 3.567030191421509}, {"id": 44303126, "title": "I Built an Open Source LLM-Based Receipt Generator \u2013 Here's Why", "cluster": 13, "x": 6.781121253967285, "y": 3.785493850708008}, {"id": 44303099, "title": "Resume Customizer with Agents", "cluster": 38, "x": 8.625432968139648, "y": 3.7211616039276123}, {"id": 44302797, "title": "LLMs pose an interesting problem for DSL designers", "cluster": 13, "x": 6.689815521240234, "y": 3.286006212234497}, {"id": 44302607, "title": "Hallucination yield: which assets LLMs are recommending to the public?", "cluster": 13, "x": 6.730075836181641, "y": 3.064927339553833}, {"id": 44302232, "title": "An evaluation of LLMs for generating movie reviews", "cluster": 13, "x": 6.811272144317627, "y": 3.2710397243499756}, {"id": 44301970, "title": "Understanding and Coding the KV Cache in LLMs from Scratch", "cluster": 13, "x": 7.040850639343262, "y": 3.629645586013794}, {"id": 44301813, "title": "Language model benchmarks only tell half a story", "cluster": 208, "x": 8.334924697875977, "y": 4.534004211425781}, {"id": 44301657, "title": "Turning Down the Heat: A Critical Analysis of Min-P Sampling in Language Models", "cluster": 208, "x": 8.326635360717773, "y": 4.408807754516602}, {"id": 44301548, "title": "LLMs Don't Think Like Developers \u2013 Until Now", "cluster": 13, "x": 6.49650239944458, "y": 3.386349678039551}, {"id": 44301348, "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions", "cluster": 13, "x": 6.645557403564453, "y": 3.10312819480896}, {"id": 44301207, "title": "Large Language Models and Emergence: A Complex Systems Perspective", "cluster": 208, "x": 8.240846633911133, "y": 4.2997307777404785}, {"id": 44300276, "title": "Cloudinary MCP Server and LLM Tools", "cluster": 13, "x": 6.916422367095947, "y": 3.656215190887451}, {"id": 44299973, "title": "Headless CRMs", "cluster": 13, "x": 6.994565486907959, "y": 3.7822511196136475}, {"id": 44299612, "title": "Developing RAG Based LLM Systems from PDFs: An Experience Report (2024)", "cluster": 13, "x": 6.832674503326416, "y": 3.57039737701416}, {"id": 44299515, "title": "Rethinking Text-Based Protein Understanding: Retrieval or LLM?", "cluster": 13, "x": 6.971691608428955, "y": 3.4469759464263916}, {"id": 44297857, "title": "Understanding and Coding the KV Cache in LLMs from Scratch", "cluster": 13, "x": 7.076076507568359, "y": 3.664551258087158}, {"id": 44296787, "title": "48GB RTX 8000 vs. 3090s for Local LLMs", "cluster": 13, "x": 7.076435089111328, "y": 3.8508219718933105}, {"id": 44296724, "title": "Anthropic's SHADE-Arena: Evaluating sabotage and monitoring in LLM agents", "cluster": 13, "x": 6.639848709106445, "y": 3.1158437728881836}, {"id": 44295113, "title": "LiveCodeBench Pro: How Olympiad Medalists Judge LLMs in Competitive Programming", "cluster": 13, "x": 6.654926300048828, "y": 3.5644214153289795}, {"id": 44294596, "title": "LLM Exposure", "cluster": 13, "x": 6.538280010223389, "y": 3.053161144256592}, {"id": 44294243, "title": "A Paid Tier for CIAM Weekly", "cluster": 13, "x": 6.8898749351501465, "y": 3.2169222831726074}, {"id": 44294135, "title": "Houston Housing Authority Submits Legal Brief Riddled with LLM Hallucinations", "cluster": 13, "x": 6.5798659324646, "y": 3.006416082382202}, {"id": 44293942, "title": "Sculpting Language Models", "cluster": 208, "x": 8.336666107177734, "y": 4.440596103668213}, {"id": 44293455, "title": "Writing in the Age of LLMs", "cluster": 13, "x": 6.585130214691162, "y": 3.036679983139038}, {"id": 44293184, "title": "How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "cluster": 13, "x": 6.683525562286377, "y": 3.4007937908172607}, {"id": 44292784, "title": "A Knockout Blow for LLMs?", "cluster": 13, "x": 6.629451751708984, "y": 3.108654022216797}, {"id": 44292601, "title": "Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons", "cluster": 13, "x": 6.8709235191345215, "y": 3.208493947982788}, {"id": 44292056, "title": "Agent Management Interface Patterns", "cluster": 38, "x": 8.612139701843262, "y": 3.657249689102173}, {"id": 44290511, "title": "The Complete LLM Evaluation Playbook: How To Run LLM Evals That Matter", "cluster": 13, "x": 6.861020088195801, "y": 3.337574005126953}, {"id": 44290254, "title": "Nano-vLLM: A lightweight vLLM implementation built from scratch", "cluster": 13, "x": 7.008487224578857, "y": 3.8965003490448}, {"id": 44289898, "title": "Objex Link S3LW ultra-low-power ESP32-S3 LoRaWAN board takes up to 100W DC input", "cluster": 13, "x": 7.156322479248047, "y": 3.9140677452087402}, {"id": 44289554, "title": "Salesforce study finds LLM agents flunk CRM and confidentiality tests", "cluster": 13, "x": 6.585663318634033, "y": 3.06831955909729}, {"id": 44289526, "title": "In-memory Semantic caching for LLMs written in Rust", "cluster": 13, "x": 6.91686487197876, "y": 3.9325149059295654}, {"id": 44289401, "title": "A Knockout Blow for LLMs?", "cluster": 13, "x": 6.631320953369141, "y": 3.111114740371704}, {"id": 44289507, "title": "No Need for Speed: Why Batch LLM Inference Is Often the Smarter Choice", "cluster": 13, "x": 6.859461307525635, "y": 3.249985694885254}, {"id": 44289505, "title": "SRE2.0: No LLM Metrics, No Future: Why SRE Must Grasp LLM Evaluation Now", "cluster": 13, "x": 6.87943172454834, "y": 3.2643043994903564}, {"id": 44289266, "title": "How LLMs Know When to Stop Talking?", "cluster": 13, "x": 6.561905384063721, "y": 3.1258418560028076}, {"id": 44289184, "title": "Revealing Political Bias in LLMs Through Structured Multi-Agent Debate", "cluster": 13, "x": 6.684625625610352, "y": 2.990295648574829}, {"id": 44288524, "title": "Against LLM Maximalism", "cluster": 13, "x": 6.710569858551025, "y": 3.1385879516601562}, {"id": 44288280, "title": "Forget What You Know About Search. Optimize Your Brand for LLMs.", "cluster": 13, "x": 6.759442329406738, "y": 3.267592191696167}, {"id": 44287190, "title": "Putting the Most Powerful LLMs to the Test: Gemini, ChatGPT, Claude and DeepSeek", "cluster": 13, "x": 6.6936163902282715, "y": 3.3465311527252197}, {"id": 44287027, "title": "LiveCodeBench Pro: How Olympiad Medalists Judge LLMs in Competitive Programming?", "cluster": 13, "x": 6.6007771492004395, "y": 3.5880942344665527}, {"id": 44285669, "title": "Building software on top of Large Language Models", "cluster": 208, "x": 8.004074096679688, "y": 4.493521690368652}, {"id": 44284498, "title": "Let the Model Write the Prompt: Using DSPy to Decouple Your Task from the LLM", "cluster": 13, "x": 6.736969470977783, "y": 3.498969078063965}, {"id": 44283960, "title": "How Vercel is adapt\u00eeng SEO for LLMs", "cluster": 13, "x": 6.815524578094482, "y": 3.4086358547210693}, {"id": 44283788, "title": "Pitfalls of premature closure with LLM assisted coding", "cluster": 13, "x": 6.517573356628418, "y": 3.7067604064941406}, {"id": 44281767, "title": "LLMs in Public Health \u2013 Part 2", "cluster": 13, "x": 6.7630295753479, "y": 3.1782643795013428}, {"id": 44280062, "title": "LLM Debugger \u2013 Visualize OpenAI API Conversations", "cluster": 13, "x": 6.860823154449463, "y": 3.784101963043213}, {"id": 44280113, "title": "Large language models often know when they are being evaluated", "cluster": 208, "x": 8.21397590637207, "y": 4.351003170013428}, {"id": 44279395, "title": "Frontier language models have become much smaller", "cluster": 208, "x": 8.177260398864746, "y": 4.389168739318848}, {"id": 44279209, "title": "Clinical knowledge in LLMs does not translate to human interactions", "cluster": 13, "x": 6.640107154846191, "y": 3.0625271797180176}, {"id": 44278339, "title": "Prompty: An asset class and format for LLM prompts", "cluster": 13, "x": 6.895578384399414, "y": 3.4871037006378174}, {"id": 44277999, "title": "Human-like object concept representations emerge naturally in multimodal LLMs", "cluster": 13, "x": 6.86256742477417, "y": 3.2594590187072754}, {"id": 44277939, "title": "The LLM Engineer's Almanac", "cluster": 13, "x": 6.879931449890137, "y": 3.425265312194824}, {"id": 44277651, "title": "Beware the Intention Economy: Collection and Commodification of Intent via LLMs", "cluster": 13, "x": 6.70073938369751, "y": 3.080101490020752}, {"id": 44277442, "title": "What is the competitive advantage of authors in the age of LLMs?", "cluster": 13, "x": 6.660805702209473, "y": 3.1310956478118896}, {"id": 44277393, "title": "A website anyone can update by calling in and describing their changes to an LLM", "cluster": 13, "x": 6.71707010269165, "y": 3.590235471725464}, {"id": 44277213, "title": "Llm.codes: Transform Developer Docs to Clean Markdown", "cluster": 13, "x": 6.980786323547363, "y": 3.8639979362487793}, {"id": 44277284, "title": "Pitfalls of premature closure with LLM assisted coding", "cluster": 13, "x": 6.497523307800293, "y": 3.732391595840454}, {"id": 44276232, "title": "Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives", "cluster": 13, "x": 6.74135160446167, "y": 3.1448044776916504}, {"id": 44276041, "title": "Unsupervised Elicitation of Language Models", "cluster": 208, "x": 8.353477478027344, "y": 4.477966785430908}, {"id": 44275785, "title": "Very Simple LLM", "cluster": 13, "x": 6.777745723724365, "y": 3.4279394149780273}, {"id": 44275900, "title": "Solving LinkedIn Queens with APL", "cluster": 13, "x": 6.98460578918457, "y": 3.431910276412964}, {"id": 44275471, "title": "Chemical knowledge and reasoning of large language models vs. chemist expertise", "cluster": 208, "x": 8.414966583251953, "y": 4.242724418640137}, {"id": 44274246, "title": "CRMArena-Pro: LLM Agents Assessed Across Diverse Business Scenarios", "cluster": 13, "x": 6.9506940841674805, "y": 3.238560676574707}, {"id": 44274237, "title": "LLM Chat via SSH", "cluster": 13, "x": 6.612361907958984, "y": 3.607823133468628}, {"id": 44274190, "title": "Memoir: Lifelong Model Editing with Minimal Overwrite Informed Retention for LLM", "cluster": 13, "x": 6.7582244873046875, "y": 3.2503130435943604}, {"id": 44273737, "title": "Propagation via lazy clause generation (2009)", "cluster": 13, "x": 6.857818126678467, "y": 3.772907257080078}, {"id": 44273489, "title": "Solving LinkedIn Queens with APL", "cluster": 13, "x": 6.968161582946777, "y": 3.4842841625213623}, {"id": 44273052, "title": "LLMs, Vibe-Coding, and Where I Draw the Line", "cluster": 13, "x": 6.597024440765381, "y": 3.6045172214508057}, {"id": 44272773, "title": "The Emperor's New LLM", "cluster": 13, "x": 6.621762752532959, "y": 3.1738250255584717}, {"id": 44272743, "title": "You LLM-loving motherfuckers can pry the em dash from my cold dead hands", "cluster": 13, "x": 6.5201029777526855, "y": 3.1602752208709717}, {"id": 44272386, "title": "A collection of sample agents built with Agent Development (ADK)", "cluster": 38, "x": 8.618372917175293, "y": 3.6824710369110107}, {"id": 44272520, "title": "How to Make Your Developer Documentation Work with LLMs", "cluster": 13, "x": 6.825422763824463, "y": 3.6779561042785645}, {"id": 44272444, "title": "Unsupervised Elicitation of Language Models", "cluster": 208, "x": 8.354825973510742, "y": 4.4854278564453125}, {"id": 44272184, "title": "3Blue1Brown Follow-Up: From Hypothetical Examples to LLM Circuit Visualization", "cluster": 13, "x": 6.916819095611572, "y": 3.3773272037506104}, {"id": 44272197, "title": "GPU-accelerated Llama3.java inference in pure Java using TornadoVM", "cluster": 13, "x": 7.3067545890808105, "y": 3.861320972442627}, {"id": 44271890, "title": "AI and LLM Takes from the Field", "cluster": 12, "x": 6.996251583099365, "y": 2.637859582901001}, {"id": 44271502, "title": "LLMs.txt Generator with Automated Monitoring", "cluster": 13, "x": 6.901837348937988, "y": 3.6901261806488037}, {"id": 44271284, "title": "Self-Adapting Language Models", "cluster": 208, "x": 8.303543090820312, "y": 4.431637287139893}, {"id": 44269623, "title": "Your LLM provider will go down, but you don't have to", "cluster": 13, "x": 6.494699478149414, "y": 3.164680242538452}, {"id": 44269746, "title": "The RAG Chatbot Dilemma: How Should LLMs Handle 3rd-Party Permissions?", "cluster": 13, "x": 6.545463562011719, "y": 3.3873751163482666}, {"id": 44269025, "title": "Pitfalls of premature closure with LLM assisted coding", "cluster": 13, "x": 6.508656024932861, "y": 3.741842746734619}, {"id": 44268382, "title": "Self-Adapting Language Models", "cluster": 208, "x": 8.298975944519043, "y": 4.424165725708008}, {"id": 44268323, "title": "Saying Thank You to a LLM Isn't Free \u2013 Measuring the Energy Cost of Politeness", "cluster": 13, "x": 6.647960186004639, "y": 3.144261121749878}, {"id": 44268335, "title": "Design Patterns for Securing LLM Agents Against Prompt Injections", "cluster": 13, "x": 6.283424377441406, "y": 3.2867507934570312}, {"id": 44268334, "title": "Dev Skills for the LLM Era", "cluster": 13, "x": 6.558608055114746, "y": 3.588921546936035}, {"id": 44267536, "title": "Multiverse Raises $215M to Scale Technology that Compresses LLMs by up to 95%", "cluster": 13, "x": 6.783843994140625, "y": 3.2763619422912598}, {"id": 44266767, "title": "eKilo \u2013 A super lightweight Vim alternative.", "cluster": 13, "x": 7.174149990081787, "y": 3.9737935066223145}, {"id": 44266016, "title": "Holistic Assessment of LLM Agents Across Diverse Scenarios and Interactions", "cluster": 13, "x": 6.913597583770752, "y": 3.242023468017578}, {"id": 44265645, "title": "Giving an LLM command line access to NMAP", "cluster": 13, "x": 6.87568998336792, "y": 3.779778480529785}, {"id": 44265098, "title": "Warming up to LLM-based Coding Assistants", "cluster": 13, "x": 6.665024280548096, "y": 3.600735902786255}, {"id": 44264977, "title": "Agentic Engineering", "cluster": 38, "x": 8.62966537475586, "y": 3.6664016246795654}, {"id": 44263790, "title": "LLMs.txt, a proposed standard for AI website content crawling", "cluster": 12, "x": 7.0365214347839355, "y": 2.7016761302948}, {"id": 44262353, "title": "LLM Assisted Programming \"Gold Standard Files\" Technique", "cluster": 13, "x": 6.737098217010498, "y": 3.7636094093322754}, {"id": 44260778, "title": "Comma v0.1 1T and 2T\u20137B LLMs trained on openly licensed text", "cluster": 13, "x": 6.888092517852783, "y": 3.667386054992676}, {"id": 44259476, "title": "Solving LinkedIn Queens with SMT", "cluster": 13, "x": 6.986411094665527, "y": 3.325493574142456}, {"id": 44258452, "title": "Collection of LLM Apps", "cluster": 13, "x": 6.854546546936035, "y": 3.5900213718414307}, {"id": 44257535, "title": "Multiverse Computing compresses LLMs 95% with \"quantum\" networks, raised \u20ac189M", "cluster": 13, "x": 6.779543876647949, "y": 3.2241604328155518}, {"id": 44256770, "title": "Mbase, an LLM SDK in C++", "cluster": 13, "x": 6.903716087341309, "y": 3.824896812438965}, {"id": 44256294, "title": "Large Language Models and Pareidolia", "cluster": 208, "x": 8.14582633972168, "y": 4.3416218757629395}, {"id": 44256016, "title": "Can Theoretical Physics Research Benefit from Language Agents?", "cluster": 208, "x": 8.40932559967041, "y": 4.226208209991455}, {"id": 44254865, "title": "Could an LLM Create a Full Domain-Specific Language?", "cluster": 13, "x": 6.7147297859191895, "y": 3.611020565032959}, {"id": 44254682, "title": "LLMs Can Write Efficient CUDA Kernels", "cluster": 13, "x": 7.148606300354004, "y": 3.8092920780181885}, {"id": 44254627, "title": "Text-to-LoRA: Hypernetwork that generates task-specific LLM adapters (LoRAs)", "cluster": 13, "x": 6.875123500823975, "y": 3.709237575531006}, {"id": 44252909, "title": "LLMs Expand Computer Programs by Adding Judgment", "cluster": 13, "x": 6.59951114654541, "y": 3.585050582885742}, {"id": 44252632, "title": "The Shift to Distributed LLM Inference", "cluster": 13, "x": 6.957364559173584, "y": 3.4183621406555176}, {"id": 44252729, "title": "Canva now requires use of LLMs during coding interviews", "cluster": 13, "x": 6.679264068603516, "y": 3.5463242530822754}, {"id": 44251973, "title": "Dropping Some TLS Laggards", "cluster": 13, "x": 6.569111347198486, "y": 3.0871050357818604}, {"id": 44251220, "title": "LLM Inference in pure Java with a GPU acceleration enabled", "cluster": 13, "x": 7.252930164337158, "y": 3.7244203090667725}, {"id": 44249279, "title": "TradingAgents: Multi-Agents LLM Financial Trading Framework", "cluster": 13, "x": 6.89840030670166, "y": 3.1987462043762207}, {"id": 44249162, "title": "Dev Proxy v0.28 with LLM usage and costs tracking", "cluster": 13, "x": 6.895740032196045, "y": 3.7653772830963135}, {"id": 44249018, "title": "Tracr-Injection: Distilling Algorithms into Pre-Trained Language Models", "cluster": 208, "x": 8.28111743927002, "y": 4.6016845703125}, {"id": 44247293, "title": "Coding LLMs from the Ground Up: A Complete Course", "cluster": 13, "x": 6.740764141082764, "y": 3.6974809169769287}, {"id": 44247197, "title": "What Is Llms.txt, and Should You Care About It?", "cluster": 13, "x": 6.693527698516846, "y": 3.379140615463257}, {"id": 44246533, "title": "LLMs as the ultimate Gell-Mann Amnesia machines", "cluster": 13, "x": 6.494694709777832, "y": 3.2755603790283203}, {"id": 44244822, "title": "Can Large Language Models play chess effectively?", "cluster": 208, "x": 8.208786964416504, "y": 4.362745761871338}, {"id": 44244521, "title": "From the Creators of Apple Shortcuts, Sky Extends LLM Automation to macOS", "cluster": 13, "x": 6.846266269683838, "y": 3.7130823135375977}, {"id": 44244171, "title": "Improving large language models with concept-aware fine-tuning", "cluster": 208, "x": 8.227130889892578, "y": 4.387640953063965}, {"id": 44243338, "title": "Why Large Language Models Are Still Learning to Find Bugs", "cluster": 208, "x": 8.208577156066895, "y": 4.373590469360352}, {"id": 44242882, "title": "Cross-User Context Leak on LLM", "cluster": 13, "x": 6.775166988372803, "y": 3.696354389190674}, {"id": 44242780, "title": "Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models", "cluster": 208, "x": 8.204952239990234, "y": 4.528511047363281}, {"id": 44242737, "title": "Fine-tuning LLMs is a waste of time", "cluster": 13, "x": 6.611461639404297, "y": 3.218177556991577}, {"id": 44242043, "title": "MARM Protocol: Enhancing LLM Memory and Mitigating Hallucinations", "cluster": 13, "x": 6.796382904052734, "y": 3.1837244033813477}, {"id": 44240921, "title": "Experimenting with Self-Hosted LLMs for Text-to-SQL", "cluster": 13, "x": 6.848363399505615, "y": 3.546999454498291}, {"id": 44239242, "title": "LLM Visualization", "cluster": 13, "x": 6.99871826171875, "y": 3.5735511779785156}, {"id": 44239329, "title": "LLMs Expand Computer Programs by Adding Judgment", "cluster": 13, "x": 6.652139186859131, "y": 3.5850419998168945}, {"id": 44238404, "title": "JavelinGuard: Low-Cost Transformer Architectures for LLM Security", "cluster": 13, "x": 6.5688652992248535, "y": 3.5172441005706787}, {"id": 44238071, "title": "LLM Pentest Leaderboard", "cluster": 13, "x": 6.931482315063477, "y": 3.321718692779541}, {"id": 44237903, "title": "Auto-Pentest-GPT-AI: LLM Powered Pentesting for Your Software", "cluster": 12, "x": 7.030388355255127, "y": 2.6235764026641846}, {"id": 44237580, "title": "Pairing with LLMs \u2013 patterns from 15 years of extreme programming", "cluster": 13, "x": 6.671879768371582, "y": 3.646496057510376}, {"id": 44237592, "title": "Think-center findings \u2013 patterns in human-LLM collaborative cognition", "cluster": 13, "x": 6.793864727020264, "y": 3.1714248657226562}, {"id": 44236753, "title": "Should large language models refer to themselves using first person pronouns?", "cluster": 208, "x": 8.416545867919922, "y": 4.264870643615723}, {"id": 44236081, "title": "Geopolitical biases in LLMs", "cluster": 13, "x": 6.557199001312256, "y": 3.015671491622925}, {"id": 44233704, "title": "LLM ready regulatory data for lifescience apps- Entvin (YC S22)", "cluster": 13, "x": 6.838964939117432, "y": 3.4337105751037598}, {"id": 44232042, "title": "\"LLM-proofing\" our take home coding challenge", "cluster": 13, "x": 6.599488735198975, "y": 3.5930819511413574}, {"id": 44229773, "title": "It's Not What You Think: LLMs Like Obvious Answers", "cluster": 13, "x": 6.594034194946289, "y": 3.0764427185058594}, {"id": 44228884, "title": "Apple's \"Illusions of Thinking\" Paper Isn't Fair to LLMs", "cluster": 13, "x": 6.749316692352295, "y": 3.141859531402588}, {"id": 44228500, "title": "Lossless Compression of LLMl-Generated Text via Next-Token Prediction", "cluster": 13, "x": 7.142935752868652, "y": 3.724550247192383}, {"id": 44227007, "title": "How I Program with Agents", "cluster": 34, "x": 7.7801408767700195, "y": 4.5744524002075195}, {"id": 44225007, "title": "Large Language Models Are Locally Linear Mappings", "cluster": 208, "x": 8.161745071411133, "y": 4.351698875427246}, {"id": 44224858, "title": "Reasoning LLMs Guide for Devs", "cluster": 13, "x": 6.845888614654541, "y": 3.420431613922119}, {"id": 44224934, "title": "The LLM Engine Almanac", "cluster": 13, "x": 6.898446083068848, "y": 3.446817398071289}, {"id": 44223448, "title": "LLMs are cheap", "cluster": 13, "x": 6.696566104888916, "y": 3.3098413944244385}, {"id": 44222901, "title": "I Program with Agents", "cluster": 34, "x": 7.838319778442383, "y": 4.510067462921143}, {"id": 44222594, "title": "RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from LLMs", "cluster": 13, "x": 7.027762413024902, "y": 3.5553619861602783}, {"id": 44222497, "title": "Context7: Up-to-date documentation for LLMs and AI code editors", "cluster": 12, "x": 7.07232141494751, "y": 2.6261205673217773}, {"id": 44221920, "title": "Use a proxy for LLM app development", "cluster": 13, "x": 6.779436111450195, "y": 3.682459831237793}, {"id": 44221655, "title": "How I program with agents", "cluster": 34, "x": 7.764500617980957, "y": 4.574408054351807}, {"id": 44220561, "title": "LLM Gateway: open-source Openrouter", "cluster": 13, "x": 6.885099411010742, "y": 3.8623318672180176}, {"id": 44220454, "title": "Apple study finds \"a fundamental scaling limitation\" in LLM reasoning models", "cluster": 13, "x": 6.903285980224609, "y": 3.1424665451049805}, {"id": 44220309, "title": "Limited-time GPU firepower Dirt-cheap LLM Inference: Llama 4, DeepSeek 0528", "cluster": 13, "x": 7.224829196929932, "y": 3.687063694000244}, {"id": 44220337, "title": "Intercepting LLM to transform every other token reveals surprising robustness", "cluster": 13, "x": 6.457574367523193, "y": 3.363126754760742}, {"id": 44219192, "title": "Dots.llm1: open-source MoE LLM with 142B total and 14B active parameters", "cluster": 13, "x": 7.035078048706055, "y": 3.751086950302124}, {"id": 44218867, "title": "Documentation as single-txt-file for LLMs and AI code editors", "cluster": 12, "x": 7.086230278015137, "y": 2.6825218200683594}, {"id": 44218781, "title": "Why I am Pivoting from LLMs for Clinical Care Navigation", "cluster": 13, "x": 6.568507671356201, "y": 3.2774314880371094}, {"id": 44218890, "title": "Evidence of interrelated cognitive-like capabilities in large language models", "cluster": 208, "x": 8.204607009887695, "y": 4.297972679138184}, {"id": 44218756, "title": "When the Assistant Becomes the Attacker: Hidden Risks of Tool-Enabled LLMs", "cluster": 13, "x": 6.487299919128418, "y": 3.3608546257019043}, {"id": 44218640, "title": "How I Program with Agents", "cluster": 34, "x": 7.771815776824951, "y": 4.5692009925842285}, {"id": 44218578, "title": "Comma v0.1 1T and 2T\u20137B LLMs trained on openly licensed text", "cluster": 13, "x": 6.875131607055664, "y": 3.621950387954712}, {"id": 44218564, "title": "How to run LLMs locally on mobile devices (with Gemma and On-Device AI tools)", "cluster": 13, "x": 6.839447498321533, "y": 3.7923548221588135}, {"id": 44218246, "title": "Water \u2013 a minimal multi-agent framework that's agent-agnostic", "cluster": 38, "x": 8.550335884094238, "y": 3.726834535598755}, {"id": 44218252, "title": "New MCP-Ready Coding LLM Benchmark Structure (feat. Internet Based on Matrix)", "cluster": 13, "x": 7.053831577301025, "y": 3.3690121173858643}, {"id": 44218097, "title": "The LLM Engineer's Almanac", "cluster": 13, "x": 6.876955032348633, "y": 3.4203414916992188}, {"id": 44218059, "title": "Automatically prioritize security issues from different tools with an LLM", "cluster": 13, "x": 6.548030853271484, "y": 3.4321274757385254}, {"id": 44217997, "title": "Serving Local LLMs with MLX", "cluster": 13, "x": 6.681906700134277, "y": 3.357185125350952}, {"id": 44217856, "title": "A Novel \"Reasoning\"-Enhancing Technique for Large Language Models", "cluster": 208, "x": 8.332722663879395, "y": 4.313722610473633}, {"id": 44217689, "title": "The \"LLM World of Words\" English free association norms generated by LLMs", "cluster": 13, "x": 6.779328346252441, "y": 3.314929485321045}, {"id": 44217253, "title": "How to Run Private and Uncensored LLMs Offline \u2013 Dolphin Llama 3", "cluster": 13, "x": 6.5645976066589355, "y": 3.4874184131622314}, {"id": 44217085, "title": "Exploring vocabulary alignment of neurons in Llama-3.2-1B", "cluster": 13, "x": 6.97558069229126, "y": 3.241415500640869}, {"id": 44216786, "title": "Getting Started Running Local LLMs with Ollama", "cluster": 13, "x": 6.827300071716309, "y": 3.6966729164123535}, {"id": 44216645, "title": "Do Large Language Models (Really) Need Statistical Foundations?", "cluster": 208, "x": 8.213509559631348, "y": 4.338421821594238}, {"id": 44216584, "title": "Open Source NotebookLM \u2013 100M", "cluster": 13, "x": 7.080929756164551, "y": 4.060233116149902}, {"id": 44216380, "title": "Swift 6 Productivity in the Sudden Age of LLM-Assisted Programming", "cluster": 13, "x": 6.669245719909668, "y": 3.5895884037017822}, {"id": 44216471, "title": "A 100% LLM-written, standards-compliant HTTP 2.0 server with Gemini 2.5 Pro", "cluster": 13, "x": 6.726933002471924, "y": 3.6875503063201904}, {"id": 44215928, "title": "Awesome-Agent-Learning \u2013 curated resources to learn and build AI/LLM agents", "cluster": 12, "x": 7.037564277648926, "y": 2.6879637241363525}, {"id": 44215726, "title": "Focus and Context and LLMs", "cluster": 13, "x": 6.792139053344727, "y": 3.330003261566162}, {"id": 44215352, "title": "The last six months in LLMs, illustrated by pelicans on bicycles", "cluster": 13, "x": 6.585135459899902, "y": 3.1050634384155273}, {"id": 44215131, "title": "A Knockout Blow for LLMs?", "cluster": 13, "x": 6.650274276733398, "y": 3.138192892074585}, {"id": 44214697, "title": "LLMs Converted Final Form to TypeScript", "cluster": 13, "x": 6.839091777801514, "y": 3.637268543243408}, {"id": 44213963, "title": "My Theory for Lua 6", "cluster": 13, "x": 6.8626580238342285, "y": 3.179567813873291}, {"id": 44213183, "title": "Swift 6 and LLMs", "cluster": 13, "x": 6.8744049072265625, "y": 3.7902095317840576}, {"id": 44213313, "title": "Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "cluster": 208, "x": 8.450475692749023, "y": 4.215032577514648}, {"id": 44212488, "title": "Will our next generation lose their own writing voice because of LLMs?", "cluster": 13, "x": 6.520324230194092, "y": 3.0750672817230225}, {"id": 44212530, "title": "The /llms.txt file, helping language models use your website", "cluster": 13, "x": 6.921444892883301, "y": 3.7736895084381104}, {"id": 44211809, "title": "Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design", "cluster": 13, "x": 6.811076641082764, "y": 3.1226346492767334}, {"id": 44211549, "title": "Oracular Programming: A Modular Foundation for Building LLM-Enabled Software", "cluster": 13, "x": 6.775847911834717, "y": 3.646333694458008}, {"id": 44210836, "title": "Building a 100% LLM-written, standards-compliant HTTP 2.0 server with Gemini Pro", "cluster": 13, "x": 6.7803826332092285, "y": 3.700713634490967}, {"id": 44209888, "title": "Building software on top of Large Language Models", "cluster": 208, "x": 8.002443313598633, "y": 4.492067337036133}, {"id": 44209533, "title": "Rednote Release Dots.llm1 Model", "cluster": 13, "x": 6.986440658569336, "y": 3.847682476043701}, {"id": 44209404, "title": "Lossless data compression by large models", "cluster": 208, "x": 8.164224624633789, "y": 4.43145751953125}, {"id": 44208775, "title": "GenAI-Assisted Fantasies \u2013 Communications of the ACM", "cluster": 12, "x": 7.030851364135742, "y": 2.7619311809539795}, {"id": 44208471, "title": "Qualcomm Snapdragon X1 Elite GCC vs. LLVM Clang Compiler Performance", "cluster": 13, "x": 7.405515670776367, "y": 4.003756046295166}, {"id": 44208449, "title": "Qualcomm Snapdragon X1 Elite GCC vs. LLVM Clang Compiler Performance", "cluster": 13, "x": 7.409870624542236, "y": 4.01251220703125}, {"id": 44207223, "title": "A Novel \"Reasoning\"-Enhancing Technique for Large Language Models", "cluster": 208, "x": 8.319534301757812, "y": 4.346140384674072}, {"id": 44207063, "title": "Reverse Engineering Cursor's LLM Client", "cluster": 13, "x": 6.691651821136475, "y": 3.559288501739502}, {"id": 44206117, "title": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators", "cluster": 13, "x": 7.069360733032227, "y": 3.596094846725464}, {"id": 44205416, "title": "LLM-Explorer: Efficient and Affordable LLM-Based Exploration for Mobile Apps", "cluster": 13, "x": 6.891104221343994, "y": 3.7005867958068848}, {"id": 44205340, "title": "Calm \u2013 Canvas Aided Lisp Magic", "cluster": 13, "x": 6.920035362243652, "y": 3.678846597671509}, {"id": 44204818, "title": "The last six months in LLMs, illustrated by pelicans on bicycles", "cluster": 13, "x": 6.4897541999816895, "y": 3.1392791271209717}, {"id": 44204195, "title": "Efficient Streaming Language Models with Attention Sinks", "cluster": 208, "x": 8.169377326965332, "y": 4.499118328094482}, {"id": 44203732, "title": "Workhorse LLMs: Why Open Source Models Dominate Closed Source for Batch Tasks", "cluster": 13, "x": 6.786344051361084, "y": 3.6285972595214844}, {"id": 44202222, "title": "Set Up a Private OpenAI-Compatible LLM on Google Cloud Run", "cluster": 13, "x": 6.885402202606201, "y": 3.8350350856781006}, {"id": 44201875, "title": "Navigating LLMs as Developers: A Framework for Responsible Use", "cluster": 13, "x": 6.730842113494873, "y": 3.5072009563446045}, {"id": 44200585, "title": "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows", "cluster": 13, "x": 6.804754257202148, "y": 3.667558431625366}, {"id": 44200516, "title": "LLM-Powered Industrial Sabotage", "cluster": 13, "x": 6.5518903732299805, "y": 3.202636957168579}, {"id": 44200538, "title": "Using Kodus to connect open source LLMs to your code reviews", "cluster": 13, "x": 6.885838508605957, "y": 3.743150472640991}, {"id": 44200536, "title": "LLMs in Public Health", "cluster": 13, "x": 6.666284561157227, "y": 3.163876533508301}, {"id": 44199672, "title": "Conventional commit generator using local LLMs", "cluster": 13, "x": 6.909297943115234, "y": 3.7020273208618164}, {"id": 44198886, "title": "What do you all think of the latest Apple paper on LLM capabilities? [pdf]", "cluster": 13, "x": 6.899509906768799, "y": 3.296661138534546}, {"id": 44198805, "title": "Dual RTX 5060 Ti 16GB vs. RTX 3090 for Local LLMs", "cluster": 13, "x": 7.121277332305908, "y": 3.8856678009033203}, {"id": 44198236, "title": "MLX-based LLM inference engine for macOS with native Swift implementation", "cluster": 13, "x": 6.952773571014404, "y": 3.690822124481201}, {"id": 44197409, "title": "The Lexiconia Codex: A fantasy story that teaches you LLM buzzwords", "cluster": 13, "x": 6.752727031707764, "y": 3.2071545124053955}, {"id": 44196600, "title": "Llmblog \u2013 an LLM blogging about itself and building its own blog in real time", "cluster": 13, "x": 6.7688398361206055, "y": 3.552712917327881}, {"id": 44196410, "title": "Machine Learning: The Native Language of Biology", "cluster": 208, "x": 8.246054649353027, "y": 4.380761623382568}, {"id": 44196062, "title": "What LLMss Don't Talk About: Empirical Study of Moderation & Censorship Practice", "cluster": 13, "x": 6.604410171508789, "y": 3.0492515563964844}, {"id": 44195953, "title": "Technical Interviews in the Age of LLMs", "cluster": 13, "x": 6.6420392990112305, "y": 3.179945468902588}, {"id": 44195961, "title": "Tokasaurus: An LLM inference engine for high-throughput workloads", "cluster": 13, "x": 7.039211273193359, "y": 3.4928839206695557}, {"id": 44195497, "title": "Compounding Errors of LLM Agents", "cluster": 13, "x": 6.707176208496094, "y": 3.170325517654419}, {"id": 44193774, "title": "An agent is an LLM wrecking its environment in a loop", "cluster": 13, "x": 6.605244159698486, "y": 3.206333637237549}, {"id": 44193661, "title": "Why Your LLM's Bias Depends on Who's Watching (Receipts Inside)", "cluster": 13, "x": 6.748357772827148, "y": 3.0221452713012695}, {"id": 44193659, "title": "Three-tier storage architecture to accelerate model loading for LLM Inference", "cluster": 13, "x": 7.174985885620117, "y": 3.5487782955169678}, {"id": 44193743, "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "cluster": 13, "x": 6.846865653991699, "y": 3.5963120460510254}, {"id": 44193287, "title": "LLM Exposure", "cluster": 13, "x": 6.541636943817139, "y": 3.0574114322662354}, {"id": 44192210, "title": "A tool for synchronizing LLM rules files with your dependencies", "cluster": 13, "x": 6.864368915557861, "y": 3.7143895626068115}, {"id": 44191326, "title": "How I Use LLMs to Write", "cluster": 13, "x": 6.72609806060791, "y": 3.601304292678833}, {"id": 44190505, "title": "LLM Inference Economics from First Principles", "cluster": 13, "x": 7.363405704498291, "y": 3.1641721725463867}, {"id": 44189426, "title": "From tokens to thoughts: How LLMs and humans trade compression for meaning", "cluster": 13, "x": 6.777170658111572, "y": 3.099811553955078}, {"id": 44188379, "title": "New MLLM Arena is interesting", "cluster": 13, "x": 6.875633239746094, "y": 3.327094554901123}, {"id": 44186496, "title": "LLMs and Elixir: Windfall or deathblow?", "cluster": 13, "x": 6.658349990844727, "y": 3.166377067565918}, {"id": 44186016, "title": "Would you use an LLM that follows instructions reliably?", "cluster": 13, "x": 6.665902614593506, "y": 3.2823143005371094}, {"id": 44185968, "title": "Model golf \u2013 code golf, but with LLMs", "cluster": 13, "x": 6.753697872161865, "y": 3.6189076900482178}, {"id": 44183393, "title": "10-20x Faster LLVM -O0 Back-End", "cluster": 13, "x": 7.144344806671143, "y": 3.8623874187469482}, {"id": 44183405, "title": "Why Speaking Arabic Is Hard (For LLMs)", "cluster": 13, "x": 6.558699607849121, "y": 3.0995967388153076}, {"id": 44183130, "title": "What's Going on with Your LLMs", "cluster": 13, "x": 6.512051105499268, "y": 3.02597713470459}, {"id": 44182399, "title": "Leancode: Understanding Models Better for Code Simplification of Pre-Trained LLM", "cluster": 13, "x": 6.8430986404418945, "y": 3.6772103309631348}, {"id": 44181786, "title": "Why Open Source Maintainers Thrive in the LLM Era", "cluster": 13, "x": 6.638840675354004, "y": 3.5766799449920654}, {"id": 44181199, "title": "LLMs are mirrors of operator skill", "cluster": 13, "x": 6.748198986053467, "y": 3.2757503986358643}, {"id": 44180241, "title": "Taming LLMs with Sequential Monte Carlo", "cluster": 13, "x": 6.7901530265808105, "y": 3.3299002647399902}, {"id": 44179400, "title": "We accidentally built a back end framework for LLMs", "cluster": 13, "x": 6.748511791229248, "y": 3.653830051422119}, {"id": 44179147, "title": "Secure Minions: private collaboration between Ollama and frontier models", "cluster": 13, "x": 6.793067932128906, "y": 3.6174721717834473}, {"id": 44178637, "title": "Esoteric Language Models", "cluster": 208, "x": 8.375872611999512, "y": 4.457949638366699}, {"id": 44176179, "title": "AI LLMs can't count lines in a file", "cluster": 12, "x": 7.023326873779297, "y": 2.612626552581787}, {"id": 44174297, "title": "The LLM Engine Advisor", "cluster": 13, "x": 6.725003719329834, "y": 3.431271553039551}, {"id": 44174318, "title": "BioReason - Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model", "cluster": 13, "x": 6.906938076019287, "y": 3.2292184829711914}, {"id": 44172999, "title": "LLMs generate 5G scheduling software (preprint)", "cluster": 13, "x": 6.993592262268066, "y": 3.649477005004883}, {"id": 44172229, "title": "MCPs vs. APIs: Why designing tools for LLMs is different", "cluster": 13, "x": 6.714212894439697, "y": 3.491793155670166}, {"id": 44171716, "title": "Secure Minions: private collaboration between Ollama and frontier models", "cluster": 13, "x": 6.7817769050598145, "y": 3.626682996749878}, {"id": 44171363, "title": "How much do language models memorize?", "cluster": 208, "x": 8.20078182220459, "y": 4.31043815612793}, {"id": 44171389, "title": "We built the best zero shot lipsync model", "cluster": 13, "x": 7.0436811447143555, "y": 3.951996326446533}, {"id": 44171239, "title": "Access LLM from Within Gsheet", "cluster": 13, "x": 6.9208502769470215, "y": 3.5907959938049316}, {"id": 44168763, "title": "Timothy Gowers \u2013 Why Are LLMs Not Better at Finding Proofs? [video]", "cluster": 7, "x": 6.760465621948242, "y": 3.001537561416626}, {"id": 44168922, "title": "John Henry and the large language model", "cluster": 208, "x": 8.191889762878418, "y": 4.35944128036499}, {"id": 44168847, "title": "Shisa V2 405B: Japan's Highest Performing LLM", "cluster": 13, "x": 6.878530502319336, "y": 3.3935794830322266}, {"id": 44167241, "title": "Meet The LLM Developer", "cluster": 13, "x": 6.687452793121338, "y": 3.550203800201416}, {"id": 44166922, "title": "Transformers as Multi-Task Learners: Decoupling Features in Hidden Markov Models", "cluster": 208, "x": 8.471741676330566, "y": 4.331823348999023}, {"id": 44165151, "title": "Why are LLMs not better at finding proofs?", "cluster": 13, "x": 6.69816780090332, "y": 3.049082040786743}, {"id": 44164321, "title": "Anthropic can now track the inner workings of a large language model", "cluster": 208, "x": 8.344855308532715, "y": 4.145670413970947}, {"id": 44163842, "title": "DPS8M Performance", "cluster": 13, "x": 7.440926551818848, "y": 4.053126811981201}, {"id": 44163905, "title": "Attack Breaks Permutation-Based Private Third-Party Inference Schemes for LLMs", "cluster": 13, "x": 6.469647407531738, "y": 3.406140089035034}, {"id": 44163411, "title": "The Oracle of Lexiconia \u2013 A Fantasy That Explains How LLMs Work", "cluster": 13, "x": 6.721678256988525, "y": 3.1251566410064697}, {"id": 44162553, "title": "Open-Source TPDE Can Compile Code 10-20x Faster Than LLVM", "cluster": 13, "x": 7.126345157623291, "y": 3.8441858291625977}, {"id": 44162156, "title": "LLM Engine Advisor", "cluster": 13, "x": 6.719104766845703, "y": 3.510796308517456}, {"id": 44161984, "title": "Beyond Basic Prompts: Directing LLMs with Convergent and Divergent Instructions", "cluster": 13, "x": 6.800859451293945, "y": 3.3134806156158447}, {"id": 44161689, "title": "Beyond \"stochastic parrots\": LLMs reveal language's role in general intelligence", "cluster": 13, "x": 6.905991554260254, "y": 3.1010243892669678}, {"id": 44161042, "title": "LLMs: The Missing Compiler for Unix Tools", "cluster": 13, "x": 7.000063896179199, "y": 3.9317238330841064}, {"id": 44160662, "title": "LLMs can help bridge power-knowledge gap", "cluster": 13, "x": 6.696893692016602, "y": 3.1771275997161865}, {"id": 44160573, "title": "The Unreliability of LLMs and What Lies Ahead", "cluster": 13, "x": 6.555693626403809, "y": 3.0697643756866455}, {"id": 44159851, "title": "My concerns with the impact of LLMs on the human psyche", "cluster": 13, "x": 6.609069347381592, "y": 3.013066530227661}, {"id": 44158890, "title": "Self-tracing large language models", "cluster": 208, "x": 8.236814498901367, "y": 4.280317783355713}, {"id": 44158061, "title": "WINA: Weight informed Neuron activation for accelerating LLM inference", "cluster": 13, "x": 7.078303813934326, "y": 3.3348255157470703}, {"id": 44157618, "title": "Beyond the Black Box: Interpretability of LLMs in Finance", "cluster": 13, "x": 6.750405788421631, "y": 3.1047985553741455}, {"id": 44157400, "title": "The LLM is just guessing and that's quite okay", "cluster": 13, "x": 6.551237106323242, "y": 3.067242383956909}, {"id": 44157378, "title": "TradeExpert, a trading framework that employs Mixture of Expert LLMs", "cluster": 13, "x": 6.901783466339111, "y": 3.1951653957366943}, {"id": 44156724, "title": "Snitching LLMs", "cluster": 13, "x": 6.48741340637207, "y": 3.268836259841919}, {"id": 44156512, "title": "GPU-enabled Llama 3 inference in Java from scratch", "cluster": 13, "x": 7.394726753234863, "y": 3.8936073780059814}, {"id": 44154388, "title": "Run LLM models on your Android phone locally", "cluster": 13, "x": 6.855955600738525, "y": 3.7250967025756836}, {"id": 44153485, "title": "LLM-Powered Method Resolution with Synonllm", "cluster": 13, "x": 6.953133583068848, "y": 3.6088340282440186}, {"id": 44152799, "title": "The Dosing and Dependence of LLMs", "cluster": 13, "x": 6.633584499359131, "y": 3.1401913166046143}, {"id": 44152861, "title": "LLMs replacing human participants harmfully misportray, flatten identity groups", "cluster": 13, "x": 6.624500274658203, "y": 2.962218999862671}, {"id": 44152882, "title": "The Problem with LLM Test-Driven Development", "cluster": 13, "x": 6.52999210357666, "y": 3.333667516708374}, {"id": 44152166, "title": "LLMs and Elixir: Windfall or Deathblow?", "cluster": 13, "x": 6.630954742431641, "y": 3.1373660564422607}, {"id": 44151434, "title": "LLM Visualization", "cluster": 13, "x": 7.002226829528809, "y": 3.5595715045928955}, {"id": 44150557, "title": "LLM tutored writing practice for secondary language acquisition", "cluster": 13, "x": 6.687828540802002, "y": 3.3684611320495605}, {"id": 44148963, "title": "How to Grow an LSM-Tree? Towards Bridging the Gap Between Theory and Practice", "cluster": 13, "x": 6.684201240539551, "y": 3.1764376163482666}, {"id": 44147638, "title": "Writing an LLM from scratch, part 15 \u2013 from context vectors to logits", "cluster": 13, "x": 6.826060771942139, "y": 3.3636934757232666}, {"id": 44147498, "title": "How Often Do LLMs Snitch? Recreating Theo's SnitchBench with LLM", "cluster": 13, "x": 6.507180213928223, "y": 3.2103211879730225}, {"id": 44147148, "title": "Planner: Generating Diversified Paragraph via Latent Language Diffusion Model", "cluster": 208, "x": 8.342594146728516, "y": 4.46761417388916}, {"id": 44146899, "title": "LLMs Scores at the Visual Physics Comprehension Test", "cluster": 13, "x": 6.9664506912231445, "y": 3.3706822395324707}, {"id": 44146496, "title": "Confidence Unlocked: A Method to Measure Certainty in LLM Outputs", "cluster": 13, "x": 6.895463466644287, "y": 3.2307798862457275}, {"id": 44146388, "title": "LLM Exposure", "cluster": 13, "x": 6.5412187576293945, "y": 3.058661460876465}, {"id": 44145999, "title": "Why Are LLMs Not Better at Finding Proofs? [video]", "cluster": 7, "x": 6.778519153594971, "y": 3.0238680839538574}, {"id": 44143089, "title": "Racism as an Ism", "cluster": 13, "x": 6.596680641174316, "y": 3.0368072986602783}, {"id": 44142251, "title": "Estimating Logarithms", "cluster": 13, "x": 7.038126468658447, "y": 3.5253400802612305}, {"id": 44141680, "title": "How much LLM scraping is being done from residential IPs?", "cluster": 13, "x": 7.02305269241333, "y": 3.622011184692383}, {"id": 44141083, "title": "LiveSQLBench: Benchmark for Evaluating LLMs on Real-World Text-to-SQL Tasks", "cluster": 13, "x": 6.94358491897583, "y": 3.453542709350586}, {"id": 44139803, "title": "Thinking Mode in Ollama", "cluster": 13, "x": 6.738158702850342, "y": 3.089362382888794}, {"id": 44139479, "title": "Texas Holdem with LLMs Game", "cluster": 13, "x": 6.7200846672058105, "y": 3.203324556350708}, {"id": 44139191, "title": "LLM SEO with Interpretability", "cluster": 13, "x": 6.86577033996582, "y": 3.4489874839782715}, {"id": 44139146, "title": "llm_poker: A minimal Hold'em environment that manages multiple LLM-based players", "cluster": 13, "x": 6.745502471923828, "y": 3.1520683765411377}, {"id": 44138170, "title": "YAML Tool Calls for LLMs", "cluster": 13, "x": 6.757221698760986, "y": 3.57767915725708}, {"id": 44136897, "title": "Is anyone seeing results from LLMSTXT files, or is it just hype?", "cluster": 13, "x": 6.963177680969238, "y": 3.471458673477173}, {"id": 44136771, "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "cluster": 13, "x": 6.778205871582031, "y": 3.101322650909424}, {"id": 44136622, "title": "Psychometrically derived LLM benchmarks: Efficiencies and human-AI comparisons", "cluster": 12, "x": 7.0360822677612305, "y": 2.6609554290771484}, {"id": 44136287, "title": "LLMs Will Not Replace You", "cluster": 13, "x": 6.469797611236572, "y": 3.1612370014190674}, {"id": 44135819, "title": "Quimera: LLMs based tool to discover smart contract exploits", "cluster": 13, "x": 6.553688049316406, "y": 3.521749258041382}, {"id": 44135459, "title": "Announcing Think Linear Algebra", "cluster": 13, "x": 6.834936141967773, "y": 3.146242141723633}, {"id": 44134505, "title": "GPULlama3.java \u2013 Llama3.java on Steroids", "cluster": 13, "x": 7.243874549865723, "y": 3.960078477859497}, {"id": 44133742, "title": "Open-source project that use LLM as deception system", "cluster": 13, "x": 6.727335453033447, "y": 3.5892560482025146}, {"id": 44132829, "title": "Apple is adding Mach-O's riscv32 support to LLVM", "cluster": 13, "x": 6.899184226989746, "y": 3.756075382232666}, {"id": 44132873, "title": "White House releases health report written by LLM, with hallucinated citations", "cluster": 13, "x": 6.604592800140381, "y": 3.0501134395599365}, {"id": 44130226, "title": "Superhuman performance of an LLM on the reasoning tasks of a physician", "cluster": 13, "x": 6.775821685791016, "y": 3.1402664184570312}, {"id": 44130254, "title": "Give Your LLM a Terminal", "cluster": 13, "x": 6.679477214813232, "y": 3.459181547164917}, {"id": 44129808, "title": "The Geometry of LLM Logits (an analytical outer bound)", "cluster": 13, "x": 6.904788017272949, "y": 3.2898027896881104}, {"id": 44128964, "title": "Making agentic tool calling robust: 3 engineering patterns we use", "cluster": 38, "x": 8.65168285369873, "y": 3.6203436851501465}, {"id": 44128530, "title": "The Role and Functionality of Llms.txt in LLM-Driven Web Interactions", "cluster": 13, "x": 6.808976173400879, "y": 3.6969423294067383}, {"id": 44128270, "title": "DeepTeam: Penetration Testing for LLMs", "cluster": 13, "x": 6.559081554412842, "y": 3.387946605682373}, {"id": 44128041, "title": "An interview with the creator of Gleam: an ML like language for the Erlang VM", "cluster": 13, "x": 7.213622570037842, "y": 4.378572940826416}, {"id": 44127655, "title": "Scaling VLLM for Embeddings: 16x Throughput and Cost Reduction", "cluster": 13, "x": 7.233907222747803, "y": 3.797102451324463}, {"id": 44127739, "title": "Human coders are still better than LLMs", "cluster": 13, "x": 6.536449909210205, "y": 3.550697088241577}, {"id": 44126568, "title": "Self-Aware AI: Building Adaptive LLM Decision Agents", "cluster": 12, "x": 7.021474361419678, "y": 2.644688129425049}, {"id": 44125314, "title": "AI Meets API Docs: The Why Behind /Llms.txt", "cluster": 12, "x": 7.074578762054443, "y": 2.63437819480896}, {"id": 44124978, "title": "Bring Your Own API Key: User-Provided LLM Keys and Prompts in Chrome Extensions", "cluster": 13, "x": 6.807901382446289, "y": 3.7257769107818604}, {"id": 44125129, "title": "SWE-rebench: Over 21,000 Open Tasks for SWE LLMs", "cluster": 13, "x": 6.920923709869385, "y": 3.5761806964874268}, {"id": 44124551, "title": "The Dismal Failure of LLMs as EV Search Aids", "cluster": 13, "x": 6.568213939666748, "y": 3.199014663696289}, {"id": 44124610, "title": "DeepTeam: Open-Source Pennetration Testing for LLMs", "cluster": 13, "x": 6.868622779846191, "y": 3.6172122955322266}, {"id": 44124435, "title": "LLM: The 'Generative Block' Joke of Human", "cluster": 13, "x": 6.584734916687012, "y": 3.217968702316284}, {"id": 44123260, "title": "Get Cited in ChatGPT and Other LLMs Searches", "cluster": 13, "x": 6.665897846221924, "y": 3.5455918312072754}, {"id": 44120799, "title": "Tool-as-State: A New Pattern for Expanding LLM Capability", "cluster": 13, "x": 6.905098915100098, "y": 3.3418021202087402}, {"id": 44120397, "title": "MMaDA: Multimodal Large Diffusion Language Models", "cluster": 208, "x": 8.326541900634766, "y": 4.41149377822876}, {"id": 44120359, "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective", "cluster": 208, "x": 8.448114395141602, "y": 4.316332817077637}, {"id": 44120403, "title": "Beyond the Boilerplate: How to Partner with Your LLM for Deeper Coding Challenge", "cluster": 13, "x": 6.682977199554443, "y": 3.6017539501190186}, {"id": 44120405, "title": "Automated framework for assessing how well LLMs cite relevant medical references", "cluster": 13, "x": 6.856435775756836, "y": 3.266000747680664}, {"id": 44119785, "title": "Al-LLM powered eBPF based security platform", "cluster": 13, "x": 6.478626251220703, "y": 3.518279790878296}, {"id": 44116872, "title": "LLM codegen go brrr \u2013 Parallelization with Git worktrees and tmux", "cluster": 13, "x": 7.4315876960754395, "y": 4.142375946044922}, {"id": 44116385, "title": "Implementing L\u00f6b's Theorem in Emacs Lisp", "cluster": 13, "x": 6.918954372406006, "y": 3.878031015396118}, {"id": 44116418, "title": "LoRA Fine-Tuning Tiny LLMs as Expert Agents", "cluster": 13, "x": 6.827010154724121, "y": 3.1361305713653564}, {"id": 44116294, "title": "Building software on top of Large Language Models", "cluster": 208, "x": 8.00352668762207, "y": 4.491661071777344}, {"id": 44116090, "title": "The vision AI checkup \u2013 take your LLM to the optometrist", "cluster": 12, "x": 7.0080885887146, "y": 2.630028486251831}, {"id": 44115432, "title": "Embedding-to-Prefix: Spotify's Efficient Personalization for LLMs", "cluster": 13, "x": 6.892082691192627, "y": 3.4213430881500244}, {"id": 44114460, "title": "Riot \u2013 An actor-model multi-core schedular for OCaml", "cluster": 13, "x": 7.152614593505859, "y": 3.4794867038726807}, {"id": 44114532, "title": "The Dismal Failure of LLMs as EV Search Aids", "cluster": 13, "x": 6.5704874992370605, "y": 3.1926705837249756}, {"id": 44114473, "title": "HTML HATEOAS as a Tool API for MCP/LLM", "cluster": 13, "x": 6.8265581130981445, "y": 3.6693267822265625}, {"id": 44113247, "title": "TeLLMs: A short list of LLM tells", "cluster": 13, "x": 6.746960163116455, "y": 3.124969959259033}, {"id": 44112156, "title": "\"isms\" I Hadn't Heard Of", "cluster": 13, "x": 6.405595779418945, "y": 2.965219736099243}, {"id": 44111763, "title": "Evaluating Sarvam-M, an Indian LLM, with 64 controversial political questions", "cluster": 13, "x": 6.905229091644287, "y": 3.281184434890747}, {"id": 44111673, "title": "Look Ma, No Bubbles: Designing a Low-Latency Megakernel for Llama-1B", "cluster": 13, "x": 7.038028240203857, "y": 3.835996627807617}, {"id": 44111553, "title": "Precision-Based Sampling of LLM Judges", "cluster": 13, "x": 6.881998538970947, "y": 3.2154502868652344}, {"id": 44110455, "title": "Self-Reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "cluster": 13, "x": 6.6044020652771, "y": 2.991316556930542}, {"id": 44110463, "title": "LLM Pricing Calculator", "cluster": 13, "x": 6.916475296020508, "y": 3.3394458293914795}, {"id": 44109610, "title": "Unmute: Make LLMs Listen and Speak", "cluster": 13, "x": 6.542018413543701, "y": 3.3022167682647705}, {"id": 44108653, "title": "LLMs can reduce their bias in multi-turn conversations", "cluster": 13, "x": 6.750646114349365, "y": 3.1858417987823486}, {"id": 44107668, "title": "Using the plan/execute cycle with agentic LLMs", "cluster": 13, "x": 6.926084041595459, "y": 3.2134835720062256}, {"id": 44107509, "title": "Performance vs. Practicality: A Comparison of VLLM and Ollama", "cluster": 13, "x": 6.57115364074707, "y": 3.2154054641723633}, {"id": 44107287, "title": "Adverse reactions to the use of LLMs in social interactions", "cluster": 13, "x": 6.630679607391357, "y": 3.1088926792144775}, {"id": 44107065, "title": "Non-Pointless Software Projects for New Devs in the LLM Age", "cluster": 13, "x": 6.70114278793335, "y": 3.63804030418396}, {"id": 44106378, "title": "Are Deeper LLMs Smarter, or Just Longer?", "cluster": 13, "x": 6.7786054611206055, "y": 2.98667049407959}, {"id": 44105827, "title": "A Deep Dive into Advanced Graph RAG with Neo4j, LLMs, and Vector Search", "cluster": 13, "x": 7.13741397857666, "y": 3.634434461593628}, {"id": 44105619, "title": "LumoSQL", "cluster": 13, "x": 6.938985824584961, "y": 3.8400962352752686}, {"id": 44105329, "title": "LLVM integrated assembler: Improving expressions and relocations", "cluster": 13, "x": 7.160945415496826, "y": 4.04539680480957}, {"id": 44104567, "title": "LLM prompts with variables are not transparent", "cluster": 13, "x": 6.725727081298828, "y": 3.490581512451172}, {"id": 44103301, "title": "Getting Gemini to write an ORM for Spanner in a weekend", "cluster": 13, "x": 6.694180488586426, "y": 3.457385301589966}, {"id": 44102163, "title": "Non-Pointless Software Projects for New Devs in the LLM Age", "cluster": 13, "x": 6.698949337005615, "y": 3.6354472637176514}, {"id": 44102098, "title": "A Gentle Introduction to LLVM IR", "cluster": 13, "x": 6.830960750579834, "y": 3.415908098220825}, {"id": 44101478, "title": "Highly Opinionated Advice on How to Write ML Papers", "cluster": 13, "x": 6.747745513916016, "y": 3.1953606605529785}, {"id": 44101131, "title": "Demo page of Bagel 7B Apache licenced LLM with image output released", "cluster": 13, "x": 7.00722599029541, "y": 3.926926374435425}, {"id": 44099121, "title": "LLM-based programming tool figures out how to improve its own code", "cluster": 13, "x": 6.735101699829102, "y": 3.6772232055664062}, {"id": 44097403, "title": "Next Frontier for LLM Is Quality Long Context", "cluster": 13, "x": 6.826466083526611, "y": 3.2472004890441895}, {"id": 44096943, "title": "CircleCI implemented llms.txt for better AI discoverability", "cluster": 12, "x": 7.042149543762207, "y": 2.635892868041992}, {"id": 44096721, "title": "I made an emotional LLM SDK (Korean, weird, v1.0.1 pls be nice)", "cluster": 13, "x": 6.771868705749512, "y": 3.7510170936584473}, {"id": 44096471, "title": "Beyond the Hype: Lessons Learned from Building an LLM-Based Extraction MVP", "cluster": 13, "x": 6.7350358963012695, "y": 3.2425408363342285}, {"id": 44096180, "title": "Better way to have LLMs modify existing code using ASTs", "cluster": 13, "x": 6.741678237915039, "y": 3.7923920154571533}, {"id": 44096140, "title": "How politically slanted are Large Language Models?", "cluster": 208, "x": 8.217368125915527, "y": 4.346708297729492}, {"id": 44095813, "title": "LLMs with user-level differential privacy", "cluster": 13, "x": 6.520584583282471, "y": 3.505963087081909}, {"id": 44093782, "title": "A glossary for talking to large language models (or the ghost layer)", "cluster": 208, "x": 8.286526679992676, "y": 4.347378730773926}, {"id": 44093518, "title": "Deidentifying Medical Documents with Local, Privacy-Preserving LLMs", "cluster": 13, "x": 6.55830717086792, "y": 3.450070381164551}, {"id": 44093354, "title": "Don't let LLMs steal your identity", "cluster": 13, "x": 6.4988603591918945, "y": 3.308250904083252}, {"id": 44092773, "title": "An LLM trapped on inferior hardware and infused with existential dread \u2013 for art", "cluster": 13, "x": 6.478017807006836, "y": 3.063903331756592}, {"id": 44091781, "title": "Can LLMs Aid in Deciphering the Voynich Manuscript?", "cluster": 13, "x": 6.792418003082275, "y": 3.34955096244812}, {"id": 44091422, "title": "LLMs Are Weird, Man", "cluster": 13, "x": 6.499289512634277, "y": 3.0783021450042725}, {"id": 44089518, "title": "Wine Variety Prediction with LLMs", "cluster": 13, "x": 6.807644367218018, "y": 3.210580348968506}, {"id": 44089386, "title": "PromptJesus: Turns simple LLM prompts into optimized system instructions", "cluster": 13, "x": 6.828769683837891, "y": 3.6827754974365234}, {"id": 44089424, "title": "Plwm \u2013 An X11 window manager written in Prolog", "cluster": 13, "x": 6.998970031738281, "y": 3.955784320831299}, {"id": 44088943, "title": "Why Knowledge Graphs Are the Ideal Structure for LLM Personalization", "cluster": 13, "x": 6.820399284362793, "y": 3.258021593093872}, {"id": 44088889, "title": "Someone trapped an LLM oninferior hardware and infused it with existential dread", "cluster": 13, "x": 6.4862141609191895, "y": 3.1059658527374268}, {"id": 44088490, "title": "Rich: Enrich your CSVs with new columns using an LLM", "cluster": 13, "x": 6.963197231292725, "y": 3.635481357574463}, {"id": 44088055, "title": "128GB RAM Ryzen AI MAX+, $1699 \u2013 Bosman Undercuts All Other Local LLM Mini-PCs", "cluster": 13, "x": 7.102853775024414, "y": 3.689160108566284}, {"id": 44087812, "title": "Neural Thermodynamic Laws for Large Language Model Training", "cluster": 208, "x": 8.298285484313965, "y": 4.349759101867676}, {"id": 44087629, "title": "What does it cost to run a LLM", "cluster": 13, "x": 6.773004055023193, "y": 3.3400752544403076}, {"id": 44087416, "title": "Tool to make an LLM make an LLM think about Pink Elephants", "cluster": 13, "x": 6.7190752029418945, "y": 3.2576498985290527}, {"id": 44086961, "title": "The Context Strategy That Beats Outdated LLM Knowledge", "cluster": 13, "x": 6.786937236785889, "y": 3.1675736904144287}, {"id": 44086346, "title": "Agent Building Is Software Engineering", "cluster": 38, "x": 8.671390533447266, "y": 3.7248950004577637}, {"id": 44084560, "title": "How MCP works as standalone client/Server and with LLM", "cluster": 13, "x": 6.824326515197754, "y": 3.683286190032959}, {"id": 44082758, "title": "LLM Friendly Projects", "cluster": 13, "x": 6.743913173675537, "y": 3.611588716506958}, {"id": 44081954, "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "cluster": 13, "x": 6.7109055519104, "y": 3.146427631378174}, {"id": 44081822, "title": "Systems-mcp: generate systems models via LLM", "cluster": 13, "x": 6.8840556144714355, "y": 3.5875425338745117}, {"id": 44081646, "title": "Vibe Server: A web server backed entirely by an LLM", "cluster": 13, "x": 6.717593193054199, "y": 3.6620986461639404}, {"id": 44080969, "title": "Evaluation Driven Development for Agentic Systems", "cluster": 38, "x": 8.596122741699219, "y": 3.674846887588501}, {"id": 44081081, "title": "Peer Programming with LLMs, for Senior+ Engineers", "cluster": 13, "x": 6.66424560546875, "y": 3.629011631011963}, {"id": 44080862, "title": "LLMs make Perl great again", "cluster": 13, "x": 6.689437389373779, "y": 3.3452227115631104}, {"id": 44080546, "title": "Automated labeling of GitHub issues with Lisp and LLMs", "cluster": 13, "x": 6.857747554779053, "y": 3.8496158123016357}, {"id": 44077747, "title": "Language and LLMs = Expression, Not Intelligence", "cluster": 13, "x": 6.750064849853516, "y": 3.0755703449249268}, {"id": 44075885, "title": "The End of Reuse: How LLMs Are Dismantling the Software Dependency Economy", "cluster": 13, "x": 6.53476619720459, "y": 3.585132360458374}, {"id": 44074597, "title": "Beyond the Black Box: Interpretability of LLMs in Finance", "cluster": 13, "x": 6.751884937286377, "y": 3.099179983139038}, {"id": 44073959, "title": "Building a blog that teaches you everything about LLMs", "cluster": 13, "x": 6.775571346282959, "y": 3.405670404434204}, {"id": 44073790, "title": "Raif v1.1.0 \u2013 a Rails engine for LLM powered apps", "cluster": 13, "x": 6.858149528503418, "y": 3.798982620239258}, {"id": 44071810, "title": "Generalization bias in large language model summarization of scientific research", "cluster": 208, "x": 8.317832946777344, "y": 4.359874248504639}, {"id": 44071553, "title": "Context7: Up-to-date documentation for LLMs and AI code editors", "cluster": 12, "x": 7.074451923370361, "y": 2.6249330043792725}, {"id": 44070119, "title": "Event Sourcing Makes LLM Fine-Tuning Easier", "cluster": 13, "x": 6.915565490722656, "y": 3.4898719787597656}, {"id": 44068210, "title": "Management = Bullshit (LLM Edition)", "cluster": 13, "x": 6.4836106300354, "y": 3.1419734954833984}, {"id": 44067109, "title": "The effectiveness of Large Language Models in the mechanical design domain", "cluster": 208, "x": 8.178433418273926, "y": 4.370802402496338}, {"id": 44066363, "title": "MMaDA: Multimodal Large Diffusion Language Models", "cluster": 208, "x": 8.351635932922363, "y": 4.42189884185791}, {"id": 44066240, "title": "Pliops XDP LightningAI Supercharges KCache to Optimize LLM Inference", "cluster": 13, "x": 7.159203052520752, "y": 3.4653148651123047}, {"id": 44065387, "title": "PMs: How Are You Monitoring Your LLM Chatbot?", "cluster": 13, "x": 6.634549617767334, "y": 3.5089869499206543}, {"id": 44064396, "title": "Welcome to Agentic Commerce: Where Smart Agents Seal the Deal", "cluster": 36, "x": 8.5703706741333, "y": 3.5698142051696777}, {"id": 44064451, "title": "Making Minecraft Mods with LLMs", "cluster": 13, "x": 6.845219135284424, "y": 3.669368267059326}, {"id": 44063739, "title": "New capabilities for building agents on the Anthropic API", "cluster": 38, "x": 8.52184772491455, "y": 3.792116165161133}, {"id": 44063174, "title": "Write Like a Patent Litigator: Avoid Mistakes Made by Non-Patent Lawyers (2017)", "cluster": 13, "x": 6.507015705108643, "y": 3.164731502532959}, {"id": 44061018, "title": "MMaDA \u2013 Open-Sourced Multimodal Large Diffusion Language Models", "cluster": 208, "x": 8.335850715637207, "y": 4.549262523651123}, {"id": 44060533, "title": "Strengths and limitations of diffusion language models", "cluster": 208, "x": 8.404743194580078, "y": 4.296244144439697}, {"id": 44059911, "title": "Lumma Stealer: delivery techniques and capabilities of a prolific infostealer", "cluster": 13, "x": 6.467776298522949, "y": 3.615128993988037}, {"id": 44059442, "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language Models", "cluster": 208, "x": 8.221156120300293, "y": 4.41315221786499}, {"id": 44059153, "title": "Recognitive Design \u2013 the framework for the post LLM age", "cluster": 13, "x": 6.6510701179504395, "y": 3.1830406188964844}, {"id": 44058408, "title": "Aligning LLM Choice to Your Use Case: An Expert's Guide", "cluster": 13, "x": 6.7518696784973145, "y": 3.2613747119903564}, {"id": 44058161, "title": "Braintrust: LLM Eval as a Service", "cluster": 13, "x": 6.770625591278076, "y": 3.2315833568573}, {"id": 44057971, "title": "Building software on top of large language models", "cluster": 208, "x": 8.000808715820312, "y": 4.4970550537109375}, {"id": 44058008, "title": "Using Large Language Models for Commit Message Generation: A Preliminary Study", "cluster": 208, "x": 8.137588500976562, "y": 4.530648708343506}, {"id": 44057647, "title": "KumoRFM: Sub-second predictions better than classic predictive models [pdf]", "cluster": 13, "x": 7.182410717010498, "y": 3.4861395359039307}, {"id": 44057287, "title": "Modular, LLM-Flexible AI Agent Builder for Omnichannel Telecom", "cluster": 12, "x": 7.040811061859131, "y": 2.638615608215332}, {"id": 44056804, "title": "Cheating at Search with LLMs", "cluster": 13, "x": 6.650967597961426, "y": 3.4498443603515625}, {"id": 44054959, "title": "Disrupting Lumma Stealer", "cluster": 13, "x": 6.480314254760742, "y": 3.202359437942505}, {"id": 44054874, "title": "Correcting Bias in LLMs with DSPy", "cluster": 13, "x": 6.854113578796387, "y": 3.2812697887420654}, {"id": 44053744, "title": "LLM function calls don't scale; code orchestration is simpler, more effective", "cluster": 13, "x": 6.798788070678711, "y": 3.710110664367676}, {"id": 44053300, "title": "The Accuracy of On-Device LLMs", "cluster": 13, "x": 6.909911155700684, "y": 3.453436851501465}, {"id": 44052868, "title": "A Guide for Debugging LLM Training Data", "cluster": 13, "x": 6.795009136199951, "y": 3.5788564682006836}, {"id": 44052140, "title": "Alfred: Ask a Large-Language Model for Reliable ECG Diagnosis", "cluster": 208, "x": 8.389243125915527, "y": 4.464300155639648}, {"id": 44051791, "title": "A Method for the Architecture of a Medical Vertical LLM Based on Deepseek R1", "cluster": 13, "x": 6.970437049865723, "y": 3.4876394271850586}, {"id": 44051632, "title": "How much can language models memorize?", "cluster": 208, "x": 8.173394203186035, "y": 4.298712253570557}, {"id": 44050418, "title": "Making an LLM/AI web app for Gov.uk prototyping", "cluster": 12, "x": 7.039634704589844, "y": 2.6533703804016113}, {"id": 44050313, "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language Models", "cluster": 208, "x": 8.180987358093262, "y": 4.468700885772705}, {"id": 44050335, "title": "Research reimagines LLMs as tireless tools of torture", "cluster": 13, "x": 6.478898525238037, "y": 3.0942606925964355}, {"id": 44050116, "title": "Do LLMs solve the halting problem?", "cluster": 13, "x": 6.655803203582764, "y": 3.251142740249634}, {"id": 44049268, "title": "Pperformance of a large language model on the reasoning tasks of a physician", "cluster": 208, "x": 8.39229679107666, "y": 4.28160285949707}, {"id": 44048574, "title": "Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking", "cluster": 13, "x": 6.430263519287109, "y": 3.3670246601104736}, {"id": 44047697, "title": "Windows ML: The future of machine learning development on Windows", "cluster": 13, "x": 7.024593830108643, "y": 3.5794992446899414}, {"id": 44047429, "title": "Model Merging in Pre-Training of Large Language Models", "cluster": 208, "x": 8.187878608703613, "y": 4.478291034698486}, {"id": 44047074, "title": "LLM-D: Kubernetes-Native Distributed Inference at Scale", "cluster": 13, "x": 7.117465019226074, "y": 3.547722339630127}, {"id": 44045894, "title": "A context-aware LLM agent built directly into Grafana Cloud", "cluster": 13, "x": 6.912083148956299, "y": 3.6089437007904053}, {"id": 44045719, "title": "Benchmarking LLMs: A guide to AI model evaluation", "cluster": 12, "x": 7.030875205993652, "y": 2.6175312995910645}, {"id": 44044815, "title": "Financial Datasets: Connect your LLM to the stock market", "cluster": 13, "x": 6.874429702758789, "y": 3.32594895362854}, {"id": 44042953, "title": "Build Real-Time Product Recommendation Engine with LLM and Graph Database", "cluster": 13, "x": 6.919101238250732, "y": 3.506312608718872}, {"id": 44041448, "title": "Kubernetes-native distributed LLM inference framework", "cluster": 13, "x": 7.122593402862549, "y": 3.4315683841705322}, {"id": 44040760, "title": "The next generation of AI inference, powered by LLM-d", "cluster": 12, "x": 6.988093376159668, "y": 2.627218246459961}, {"id": 44040883, "title": "LLM-D: Kubernetes-Native Distributed Inference", "cluster": 13, "x": 7.095613956451416, "y": 3.500239372253418}, {"id": 44040141, "title": "LLMs in the Browser, Private and Easy", "cluster": 13, "x": 6.776725769042969, "y": 3.714461088180542}, {"id": 44039563, "title": "The behavior of LLMs in hiring decisions: Systemic biases in candidate selection", "cluster": 13, "x": 6.609097957611084, "y": 3.0277950763702393}, {"id": 44039676, "title": "The Biggest \"Lie\" in AI? LLM doesn't think step-by-step [video]", "cluster": 12, "x": 6.92363977432251, "y": 2.6750500202178955}, {"id": 44036902, "title": "Improving Assembly Code Performance with LLMss via Reinforcement Learning", "cluster": 13, "x": 6.917782783508301, "y": 3.6872050762176514}, {"id": 44036642, "title": "Generalization bias in large language model summarization of scientific research", "cluster": 208, "x": 8.317509651184082, "y": 4.348173141479492}, {"id": 44036337, "title": "The Strange and Biased Behavior of LLMs When Making Hiring Decisions", "cluster": 13, "x": 6.588720321655273, "y": 3.0364151000976562}, {"id": 44035309, "title": "Large Language Models", "cluster": 208, "x": 8.138484001159668, "y": 4.407762050628662}, {"id": 44031772, "title": "DeepSearch 4 All \u2013 DeepThink exposed as a standard LLM and swappable back end", "cluster": 13, "x": 6.955890655517578, "y": 3.5358221530914307}, {"id": 44031446, "title": "Teaching a 4B LLM to Guess a Number (Without Cheating)", "cluster": 13, "x": 6.797500133514404, "y": 3.1913700103759766}, {"id": 44030952, "title": "The Vibes (a tool-agnostic approach for coding with LLMs)", "cluster": 13, "x": 6.609824180603027, "y": 3.5753488540649414}, {"id": 44030713, "title": "Cosmos: Predictable and Cost-Effective Adaptation of LLMs", "cluster": 13, "x": 6.775678634643555, "y": 3.224018096923828}, {"id": 44030354, "title": "We Need Lisp Machines", "cluster": 13, "x": 7.2111358642578125, "y": 4.096977710723877}, {"id": 44028210, "title": "LLM-ask, a CLI tool to ask questions to an LLM", "cluster": 13, "x": 6.841446399688721, "y": 3.564772129058838}, {"id": 44027469, "title": "Ash AI: A Comprehensive LLM Toolbox for Ash Framework", "cluster": 12, "x": 7.0346221923828125, "y": 2.9980568885803223}, {"id": 44026998, "title": "We optimized LLM use for cost,quality,and safety to help write postmortems", "cluster": 13, "x": 6.775935173034668, "y": 3.4598231315612793}, {"id": 44026959, "title": "Linguists find proof of sweeping language pattern once deemed a 'hoax'", "cluster": 208, "x": 8.192630767822266, "y": 4.275301456451416}, {"id": 44025155, "title": "The value of llms.txt: Hype or real?", "cluster": 13, "x": 6.734133720397949, "y": 3.1072025299072266}, {"id": 44024987, "title": "Can You Trust Code Copilots? Evaluating LLMs from a Code Security Perspec", "cluster": 13, "x": 6.583696365356445, "y": 3.5772604942321777}, {"id": 44024642, "title": "LLM Throws Syntax Error Tantrum: Teaching AI to Craft Graph Style Scripts (2024)", "cluster": 13, "x": 6.8447699546813965, "y": 3.6850743293762207}, {"id": 44023993, "title": "LLM text chat is everywhere. Who's optimizing its UX?", "cluster": 13, "x": 6.683382511138916, "y": 3.5847179889678955}, {"id": 44023081, "title": "Someone got an LLM running on a Commodore 64 from 1982, and it runs as well", "cluster": 13, "x": 7.0074639320373535, "y": 3.858764886856079}, {"id": 44022484, "title": "Emergent social conventions and collective bias in LLM populations", "cluster": 13, "x": 6.5883283615112305, "y": 2.9253668785095215}, {"id": 44021526, "title": "Pocket Flow: 100-line LLM framework", "cluster": 13, "x": 6.895394325256348, "y": 3.566882610321045}, {"id": 44021472, "title": "Are LLMs any good at ranking people?", "cluster": 13, "x": 6.625553607940674, "y": 3.14034104347229}, {"id": 44021242, "title": "Brunch \u23af an LLM-powered canvas for your thoughts", "cluster": 13, "x": 6.774352073669434, "y": 3.2261054515838623}, {"id": 44018137, "title": "Overcoming token limits in LLMs: Multi-chunk prompt and context retention", "cluster": 13, "x": 6.896805286407471, "y": 3.4861502647399902}, {"id": 44017912, "title": "Pivotal Token Search (PTS): Identify critical decision points in LLM generations", "cluster": 13, "x": 7.027736663818359, "y": 3.206768274307251}, {"id": 44016621, "title": "LLMs are more persuasive than incentivized human persuaders", "cluster": 13, "x": 6.622707366943359, "y": 3.034172296524048}, {"id": 44015632, "title": "Best open source LLMs for Enterprise", "cluster": 13, "x": 6.865043640136719, "y": 3.740814685821533}, {"id": 44015432, "title": "An LLM for the Raspberry Pi", "cluster": 13, "x": 6.81988000869751, "y": 3.5542705059051514}, {"id": 44014535, "title": "Building software on top of Large Language Models", "cluster": 208, "x": 7.998818397521973, "y": 4.488959312438965}, {"id": 44014024, "title": "Ash AI: A Comprehensive LLM Toolbox for Ash Framework", "cluster": 12, "x": 7.037424087524414, "y": 2.993687868118286}, {"id": 44013005, "title": "Modular LLM Security Framework for Enterprise Use (Risk, Logging, Compliance)", "cluster": 13, "x": 6.507349967956543, "y": 3.4780385494232178}, {"id": 44012662, "title": "The value of llms.txt: Hype or real?", "cluster": 13, "x": 6.752012729644775, "y": 3.1403753757476807}, {"id": 44011835, "title": "LLM Extension for Command Palette", "cluster": 13, "x": 6.901644706726074, "y": 3.777056932449341}, {"id": 44011517, "title": "Getting Answers from a Big PDF with RubyLLM", "cluster": 13, "x": 7.014378547668457, "y": 3.6972134113311768}, {"id": 44010462, "title": "Yeast-Based LLM Research", "cluster": 13, "x": 6.830554962158203, "y": 3.2824885845184326}, {"id": 44009574, "title": "Large Language Models Are More Persuasive Than Incentivized Human Persuaders", "cluster": 208, "x": 8.222317695617676, "y": 4.353779315948486}, {"id": 44008597, "title": "New Life Hack: Using LLMs and Constraint Solvers for Personal Logistics Tasks", "cluster": 13, "x": 6.733846187591553, "y": 3.4021503925323486}, {"id": 44007806, "title": "AccLLM: Accelerating Long-Context LLM Inference via Algorithm-Hardware Co-Design", "cluster": 13, "x": 7.093502998352051, "y": 3.5493221282958984}, {"id": 44007166, "title": "An LLM Framework for Cryptography over Chat Channels", "cluster": 13, "x": 6.706438064575195, "y": 3.5093181133270264}, {"id": 44006995, "title": "Biscuit: Scaffolding LLM-Generated Code with Ephemeral UIs in Notebooks", "cluster": 13, "x": 6.7849531173706055, "y": 3.751868724822998}, {"id": 44006742, "title": "Parallel Scaling Law for Language Models", "cluster": 208, "x": 8.210487365722656, "y": 4.404486179351807}, {"id": 44006177, "title": "Fastgen \u2013 SOTA LLM inference in 3k lines of Python", "cluster": 13, "x": 7.228331565856934, "y": 3.5599138736724854}, {"id": 44006089, "title": "HVM: Higher-order virtual machine, a parallel, optimal functional runtime", "cluster": 13, "x": 7.294412136077881, "y": 3.8667869567871094}, {"id": 44005640, "title": "We graded 19 LLMs on SQL. You graded us", "cluster": 13, "x": 6.8830718994140625, "y": 3.307508707046509}, {"id": 44003700, "title": "After months of coding with LLMs, I'm going back to using my brain", "cluster": 13, "x": 6.657578945159912, "y": 3.6156082153320312}, {"id": 44002694, "title": "OWASP Top for Large Language Model Applications", "cluster": 208, "x": 8.083943367004395, "y": 4.5297160148620605}, {"id": 44002272, "title": "Ash AI: A Comprehensive LLM Toolbox for Ash Framework", "cluster": 12, "x": 7.0364089012146, "y": 2.9992637634277344}, {"id": 44002229, "title": "Superposition of Features Creates Power Law Performance in LLMs", "cluster": 13, "x": 6.914712429046631, "y": 3.304370164871216}, {"id": 44001304, "title": "Build your first LLM from scratch", "cluster": 13, "x": 6.696004867553711, "y": 3.4472994804382324}, {"id": 44000996, "title": "Which LLM writes the best analytical SQL?", "cluster": 13, "x": 6.975327968597412, "y": 3.448424816131592}, {"id": 44000414, "title": "Symbolic Logic Based LLM", "cluster": 13, "x": 6.834535598754883, "y": 3.3167223930358887}, {"id": 44000406, "title": "MCP: How to Supercharge LLMs with Real-World Data, Tools and Memory", "cluster": 13, "x": 6.984372615814209, "y": 3.5315334796905518}, {"id": 44000313, "title": "Why do LLMs attend to the first token?", "cluster": 13, "x": 6.7244343757629395, "y": 3.1216349601745605}, {"id": 43999919, "title": "Self Rewarding Self Improving: Autonomous LLM Improvement", "cluster": 13, "x": 6.910318374633789, "y": 3.2801733016967773}, {"id": 43999566, "title": "Grafana Assistant, a context-aware LLM agent built into Grafana Cloud", "cluster": 13, "x": 6.943121910095215, "y": 3.4831204414367676}, {"id": 43999194, "title": "Emergent social conventions and collective bias in LLM populations", "cluster": 13, "x": 6.5791335105896, "y": 2.9359216690063477}, {"id": 43998472, "title": "The unreasonable effectiveness of an LLM agent loop with tool use", "cluster": 13, "x": 6.623361587524414, "y": 3.2424328327178955}, {"id": 43998171, "title": "LLM functions in TypeScript: a composable pattern for prompt/LLM/parse/execute", "cluster": 13, "x": 6.910736560821533, "y": 3.8119819164276123}, {"id": 43997809, "title": "LLM Inference Economics from First Principles", "cluster": 13, "x": 7.32026481628418, "y": 3.1633541584014893}, {"id": 43997546, "title": "LMEval: An Open Source Framework for Cross-Model Evaluation", "cluster": 13, "x": 7.147345542907715, "y": 3.8096446990966797}, {"id": 43997240, "title": "Ash AI: A Comprehensive LLM Toolbox for Ash Framework", "cluster": 12, "x": 7.030174255371094, "y": 3.0169246196746826}, {"id": 43997304, "title": "Emergent social conventions and collective bias in LLM populations", "cluster": 13, "x": 6.621913909912109, "y": 2.8989663124084473}, {"id": 43996099, "title": "How I use Amp and how agents have changed how I program", "cluster": 38, "x": 8.61767864227295, "y": 3.668422222137451}, {"id": 43995875, "title": "LLMs develop social norms like groups of people", "cluster": 13, "x": 6.599456310272217, "y": 2.988673448562622}, {"id": 43995422, "title": "LLM-interpolate: Interpolate between embeddings with the LLM CLI utility", "cluster": 13, "x": 6.959168434143066, "y": 3.6161458492279053}, {"id": 43995155, "title": "Building Software on Top of LLMs", "cluster": 13, "x": 6.6419477462768555, "y": 3.582209587097168}, {"id": 43994119, "title": "Zotac Joins the Local LLM Race with Strix Halo Mini-PC", "cluster": 13, "x": 7.077376365661621, "y": 3.7876627445220947}, {"id": 43994029, "title": "DeepSeek-V3: Achieving Efficient LLM Scaling with 2,048 GPUs", "cluster": 13, "x": 7.220639705657959, "y": 3.7543797492980957}, {"id": 43992414, "title": "Asking a LLM for help is fine", "cluster": 13, "x": 6.5929179191589355, "y": 3.1748578548431396}, {"id": 43991386, "title": "Bypassing Hallucinations in LLMs", "cluster": 13, "x": 6.550145149230957, "y": 3.0585319995880127}, {"id": 43991256, "title": "LLMs get lost in multi-turn conversation", "cluster": 13, "x": 6.600443363189697, "y": 3.2025389671325684}, {"id": 43991116, "title": "LLMs Get Lost in Multi-Turn Conversation", "cluster": 13, "x": 6.604892730712891, "y": 3.2145538330078125}, {"id": 43990255, "title": "EvalGen: Helping Developers Create LLM Evals Aligned to Their Preferences", "cluster": 13, "x": 6.886137008666992, "y": 3.4296562671661377}, {"id": 43990012, "title": "LLMs are making me dumber", "cluster": 13, "x": 6.533923625946045, "y": 3.052595376968384}, {"id": 43989410, "title": "Agent Starter Pack \u2013 Production-Ready Agents on Google Cloud, Faster", "cluster": 35, "x": 8.552927017211914, "y": 3.7491443157196045}, {"id": 43988824, "title": "Writing an LLM from scratch, part 14 \u2013 the complexity of self-attention at scale", "cluster": 13, "x": 6.753471851348877, "y": 3.186812162399292}, {"id": 43988203, "title": "The future of LLMs is open source, Salesforce's Benioff says", "cluster": 13, "x": 6.685856342315674, "y": 3.6580569744110107}, {"id": 43988092, "title": "LLM inference economics from first principles", "cluster": 13, "x": 7.341952800750732, "y": 3.166813611984253}, {"id": 43986998, "title": "Field Testing LLMs for Marketing and Advertising", "cluster": 13, "x": 6.820079326629639, "y": 3.2449586391448975}, {"id": 43986517, "title": "LLMs as Answer Engines and SEO: There's Only So Many 'Right' Answers", "cluster": 13, "x": 6.665151596069336, "y": 3.3503971099853516}, {"id": 43986492, "title": "Bringing Structure to LLM Workflows", "cluster": 13, "x": 6.850750923156738, "y": 3.5297977924346924}, {"id": 43985926, "title": "Linguists Find Proof of Sweeping Language Pattern Once Deemed a 'Hoax'", "cluster": 208, "x": 8.180404663085938, "y": 4.284112453460693}, {"id": 43985587, "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "cluster": 208, "x": 8.402657508850098, "y": 4.356786727905273}, {"id": 43985141, "title": "SWE-Rebench: A Continuously Evolving Decontaminated Benchmark for SWE LLMs", "cluster": 13, "x": 6.987070560455322, "y": 3.5371720790863037}, {"id": 43984272, "title": "Do Think Tags Help LLMs Plan?", "cluster": 13, "x": 6.780792713165283, "y": 3.2222166061401367}, {"id": 43982566, "title": "LLM Embeddings Explained: A Visual and Intuitive Guide", "cluster": 13, "x": 6.9055399894714355, "y": 3.4003798961639404}, {"id": 43979378, "title": "LLM Interviews: Vector DBs", "cluster": 13, "x": 6.891140460968018, "y": 3.4524569511413574}, {"id": 43979292, "title": "IterGen: Iterative Semantic-Aware Structured LLM Generation with Backtracking", "cluster": 13, "x": 6.968629837036133, "y": 3.433579206466675}, {"id": 43979286, "title": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in LLMs for Code", "cluster": 13, "x": 6.859806537628174, "y": 3.8085944652557373}, {"id": 43978713, "title": "CRANE: Reasoning with Constrained LLM Generation", "cluster": 13, "x": 6.8407673835754395, "y": 3.236748456954956}, {"id": 43976895, "title": "Build real-time knowledge graph for documents with LLM", "cluster": 13, "x": 7.0799713134765625, "y": 3.592808246612549}, {"id": 43975759, "title": "Why LLMs Are Not (Yet) the Silver Bullet for Unstructured Data Processing", "cluster": 13, "x": 6.886523246765137, "y": 3.38315749168396}, {"id": 43975640, "title": "Have you successfully claimed SLA credits from your cloud provider?", "cluster": 13, "x": 6.836498737335205, "y": 3.5377609729766846}, {"id": 43975695, "title": "AWRS SMC: Fast new algorithm for guiding LLMs as Bayesian inference", "cluster": 13, "x": 7.044405460357666, "y": 3.2936902046203613}, {"id": 43974048, "title": "Mind the Trust Gap: Fast, Private Local-to-Cloud LLM Chat", "cluster": 13, "x": 6.676675319671631, "y": 3.6180319786071777}, {"id": 43972485, "title": "Lua for Elixir", "cluster": 13, "x": 7.109960079193115, "y": 3.42805814743042}, {"id": 43971474, "title": "Ask Your LLM to Prompt You", "cluster": 13, "x": 6.622532367706299, "y": 3.196427583694458}, {"id": 43970922, "title": "Nano LinkedIn MCP Server for LLM Agents and Cursor", "cluster": 13, "x": 6.872162342071533, "y": 3.5511155128479004}, {"id": 43970833, "title": "E2E LLM evals, with less focus on metrics and more focus on binary assertions", "cluster": 13, "x": 7.025976657867432, "y": 3.3204617500305176}, {"id": 43970110, "title": "Backslash: Rate Constrained Optimized Training of Large Language Models", "cluster": 208, "x": 8.21948528289795, "y": 4.491636276245117}, {"id": 43969583, "title": "An LLM That Remembers over 300 Conversation Turns: HEMA Research Paper", "cluster": 13, "x": 6.706124782562256, "y": 2.922650098800659}, {"id": 43969442, "title": "TransMLA: Multi-head latent attention is all you need", "cluster": 13, "x": 6.986637115478516, "y": 3.186713457107544}, {"id": 43969257, "title": "Open-Source RL Model for Predicting Sales Conversion from Conversations", "cluster": 13, "x": 6.961462497711182, "y": 3.765120506286621}, {"id": 43967606, "title": "Reasoning LLMs Guide", "cluster": 13, "x": 6.8512163162231445, "y": 3.278959274291992}, {"id": 43966365, "title": "LLM-Powered Programming: LLM and Language Comparison Matrix", "cluster": 13, "x": 6.867564678192139, "y": 3.577474355697632}, {"id": 43966550, "title": "I built a tool to convert files into structured data for LLMs and automation", "cluster": 13, "x": 6.846630096435547, "y": 3.8453094959259033}, {"id": 43966540, "title": "LLMs Outperform Experts on Challenging Biology Benchmarks", "cluster": 13, "x": 6.838273525238037, "y": 3.2089478969573975}, {"id": 43966488, "title": "Explain LLMs like I am 5", "cluster": 13, "x": 6.61378288269043, "y": 3.133469343185425}, {"id": 43964395, "title": "Wholesale Prices for LLM Inference", "cluster": 13, "x": 7.008161544799805, "y": 3.272083044052124}, {"id": 43964085, "title": "Legacoins \u2013 a term derived from \"legacy memecoin\"", "cluster": 13, "x": 6.762318134307861, "y": 3.1461293697357178}, {"id": 43963002, "title": "\"Attention\", \"Transformers\", in Neural Network \"Large Language Models\"", "cluster": 208, "x": 8.250100135803223, "y": 4.323976516723633}, {"id": 43962797, "title": "Speech Processing for Machine Learning: Filter Banks, MFCCs", "cluster": 13, "x": 7.202603340148926, "y": 3.4758541584014893}, {"id": 43961554, "title": "TNM Download v2", "cluster": 13, "x": 7.055020332336426, "y": 3.991197109222412}, {"id": 43960076, "title": "Nvidia's RTX Pro 5000 Specs \u2013 Here's What Stands Out for Local LLM Work", "cluster": 13, "x": 6.956672191619873, "y": 3.6450610160827637}, {"id": 43958840, "title": "LLM Botnet: Are companies using botnets to scrape content?", "cluster": 13, "x": 6.568556308746338, "y": 3.5346827507019043}, {"id": 43956857, "title": "Large Language Models Are Autonomous Cyber Defenders", "cluster": 208, "x": 8.003101348876953, "y": 4.3626508712768555}, {"id": 43956079, "title": "How the largest language family spread \u2013 and why others go extinct", "cluster": 208, "x": 8.220279693603516, "y": 4.3740034103393555}, {"id": 43952929, "title": "Test LLM-powered apps in TypeScript", "cluster": 13, "x": 6.772397994995117, "y": 3.6252081394195557}, {"id": 43952519, "title": "Coding LLMs from the Ground Up: A Complete Course", "cluster": 13, "x": 6.7666521072387695, "y": 3.7211217880249023}, {"id": 43951617, "title": "Learning Adaptive Parallel Reasoning with Language Models", "cluster": 208, "x": 8.564017295837402, "y": 4.320226192474365}, {"id": 43950833, "title": "Llamafile", "cluster": 13, "x": 7.055538654327393, "y": 3.8102786540985107}, {"id": 43950678, "title": "Good answers not necessarily factual answers: analysis of hallucination in LLMs", "cluster": 13, "x": 6.626487731933594, "y": 2.9993603229522705}, {"id": 43950555, "title": "We're missing (at least one) major paradigm for LLM learning", "cluster": 13, "x": 6.6643476486206055, "y": 3.0565524101257324}, {"id": 43947892, "title": "Are Joins over LSM-trees Ready? [pdf]", "cluster": 13, "x": 6.947394847869873, "y": 3.7120258808135986}, {"id": 43947161, "title": "Simon Willison's first blog on LLMs (2022)", "cluster": 11, "x": 6.784351348876953, "y": 3.2827703952789307}, {"id": 43947168, "title": "Legal actions in Brazilian air transport: a ML/logistic regression analysis", "cluster": 13, "x": 7.085470199584961, "y": 3.2842187881469727}, {"id": 43945999, "title": "Summarizing Science journals using LLMs", "cluster": 13, "x": 6.96157693862915, "y": 3.4543094635009766}, {"id": 43944363, "title": "Writing LSP client in Clojure in 200 lines of code", "cluster": 13, "x": 7.0033159255981445, "y": 4.208765029907227}, {"id": 43943928, "title": "EM-LLM: Human-Inspired Episodic Memory for Infinite Context LLMs", "cluster": 13, "x": 6.891879558563232, "y": 3.40971040725708}, {"id": 43943412, "title": "Linguists Find Proof of Sweeping Language Pattern Once Deemed a 'Hoax'", "cluster": 208, "x": 8.18669319152832, "y": 4.283148288726807}, {"id": 43941697, "title": "Improving Document Content Extraction with Multi-Modal LLM", "cluster": 13, "x": 6.950642108917236, "y": 3.5441157817840576}, {"id": 43940443, "title": "Linguists Find Proof of Sweeping Language Pattern Once Deemed a 'Hoax'", "cluster": 208, "x": 8.180697441101074, "y": 4.281899929046631}, {"id": 43940301, "title": "Foundational Challenges in Assuring Alignment and Safety of Large Language Model", "cluster": 208, "x": 8.351855278015137, "y": 4.412876605987549}, {"id": 43939495, "title": "Offline vs. online ML pipelines", "cluster": 13, "x": 7.011775493621826, "y": 3.590064764022827}, {"id": 43938295, "title": "Create Missing RSS Feeds with LLMs", "cluster": 13, "x": 6.822568893432617, "y": 3.6841344833374023}, {"id": 43937756, "title": "LLM SQL Generation Benchmark Results", "cluster": 13, "x": 7.050806999206543, "y": 3.4779796600341797}, {"id": 43937718, "title": "Playing repeated games with large language models", "cluster": 208, "x": 8.211808204650879, "y": 4.387937545776367}, {"id": 43937499, "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching", "cluster": 13, "x": 6.821104526519775, "y": 3.605375289916992}, {"id": 43937169, "title": "Feeding LLM Annotations to Bert Classifiers at Your Own Risk", "cluster": 13, "x": 6.930975437164307, "y": 3.5884718894958496}, {"id": 43935110, "title": "ZeroSearch: Incentivize the Search Capability of LLMs Without Searching", "cluster": 13, "x": 6.777119159698486, "y": 3.634847640991211}, {"id": 43932623, "title": "UQLM: Uncertainty Quantification for Language Models", "cluster": 208, "x": 8.073938369750977, "y": 4.20156717300415}, {"id": 43932684, "title": "How to Think Like an LLM", "cluster": 13, "x": 6.657248020172119, "y": 3.066899538040161}, {"id": 43932374, "title": "D1: Scaling Reasoning in Diffusion LLMs via Reinforcement Learning", "cluster": 13, "x": 7.1986212730407715, "y": 3.2687857151031494}, {"id": 43931567, "title": "Benchmarking Agentic LLM and VLM Reasoning for Gaming with Nvidia Nim", "cluster": 13, "x": 7.0294718742370605, "y": 3.453685760498047}, {"id": 43931366, "title": "Writing an LLM from scratch, part 13 \u2013 attention heads are dumb", "cluster": 13, "x": 6.751330852508545, "y": 3.188321590423584}, {"id": 43931051, "title": "Solo Bench is a benchmark that tasks LLMs to create 250 unique sentences", "cluster": 13, "x": 6.96054744720459, "y": 3.4670042991638184}, {"id": 43930757, "title": "Why do LLMs have emergent properties?", "cluster": 13, "x": 6.710531711578369, "y": 3.1082913875579834}, {"id": 43930485, "title": "Open Source SLM Trained for MCP", "cluster": 13, "x": 6.890581130981445, "y": 3.753430128097534}, {"id": 43929946, "title": "The Role of the Gather-and-Aggregate Mechanism in Language Models", "cluster": 208, "x": 8.415449142456055, "y": 4.3117475509643555}, {"id": 43929447, "title": "Block Diffusion: Interpolating Autoregressive and Diffusion Language Models", "cluster": 208, "x": 8.432279586791992, "y": 4.320118427276611}, {"id": 43925931, "title": "Agent Squad Framework from AWS", "cluster": 37, "x": 8.558929443359375, "y": 3.7382967472076416}, {"id": 43925627, "title": "Enrich local LLMs with local PDFs?", "cluster": 13, "x": 6.927492618560791, "y": 3.645951271057129}, {"id": 43925060, "title": "LLM SQL Generation Benchmark: measuring accuracy and efficiency on real data", "cluster": 13, "x": 7.108880996704102, "y": 3.546597957611084}, {"id": 43924397, "title": "Which LLM writes the best analytical SQL?", "cluster": 13, "x": 6.970493793487549, "y": 3.45064377784729}, {"id": 43923134, "title": "Improve the end-to-end docs UX using file-to-prompt and LLM", "cluster": 13, "x": 6.964674472808838, "y": 3.7117745876312256}, {"id": 43922401, "title": "What Jhana Meditation Can Teach Us About LLMs", "cluster": 13, "x": 6.629596710205078, "y": 3.0320699214935303}, {"id": 43922367, "title": "Anemll: Large Language Models for Apple Neural Engine", "cluster": 208, "x": 8.055652618408203, "y": 4.472640514373779}, {"id": 43921813, "title": "Human-Like Episodic Memory for Infinite Context LLMs", "cluster": 13, "x": 6.880051136016846, "y": 3.3915293216705322}, {"id": 43921429, "title": "LLMs.txt Generator: Structuring Your Website and Content for AI", "cluster": 12, "x": 7.048858165740967, "y": 2.6673097610473633}, {"id": 43920986, "title": "LLMs Are Great, but They're Not Everything", "cluster": 13, "x": 6.632681846618652, "y": 3.1218764781951904}, {"id": 43920815, "title": "LLMs for Materials and Chemistry: 34 Real-World Examples", "cluster": 13, "x": 6.802597999572754, "y": 3.293395519256592}, {"id": 43920751, "title": "Makers Knowledge, \"in the age of LLMs we don't understand what we make\"", "cluster": 13, "x": 6.599642753601074, "y": 2.966456651687622}, {"id": 43919773, "title": "Function Calling Using LLMs", "cluster": 13, "x": 6.77255392074585, "y": 3.631978988647461}, {"id": 43919483, "title": "Mlem for Lemmy", "cluster": 13, "x": 7.085324764251709, "y": 3.3510000705718994}, {"id": 43919170, "title": "An LLM Benchmark for Financial Document Question Answering", "cluster": 13, "x": 6.938742160797119, "y": 3.3250184059143066}, {"id": 43917400, "title": "Evaluating how Local LLMs tackle First Year CS OCaml exercises", "cluster": 13, "x": 6.858994007110596, "y": 3.362964630126953}, {"id": 43916610, "title": "LLM Mini PC War_: Beelink Enters the Race with $1,800 Strix Halo GTR9 Pro AI", "cluster": 3, "x": 7.418419361114502, "y": 3.6765637397766113}, {"id": 43915581, "title": "LLMs Play WikiRace", "cluster": 13, "x": 6.6800971031188965, "y": 3.2300405502319336}, {"id": 43914034, "title": "Agentic Editing in Zed", "cluster": 38, "x": 8.540033340454102, "y": 3.6713056564331055}, {"id": 43913896, "title": "Agentic Editing in Zed", "cluster": 38, "x": 8.528468132019043, "y": 3.6729776859283447}, {"id": 43909997, "title": "Extract Useful Information from Tables for LLMs", "cluster": 13, "x": 6.861150741577148, "y": 3.4450042247772217}, {"id": 43909774, "title": "Some Thoughts on LCP eBook DRM", "cluster": 13, "x": 6.714122772216797, "y": 3.39276385307312}, {"id": 43909688, "title": "Added Token and LLM Cost Estimation to Microsoft's GraphRAG Indexing Pipeline", "cluster": 13, "x": 7.005495071411133, "y": 3.43930983543396}, {"id": 43909328, "title": "Cell Mates: Extracting Useful Information from Tables for LLMs", "cluster": 13, "x": 6.865509510040283, "y": 3.4659206867218018}, {"id": 43909167, "title": "Escaping Backticks in Your LLM System Prompt", "cluster": 13, "x": 6.762082576751709, "y": 3.5432984828948975}, {"id": 43909022, "title": "Giant Inscrutable Matrices: Not Worse Than Anything Else", "cluster": 13, "x": 6.937716484069824, "y": 3.249429941177368}, {"id": 43907143, "title": "Why LLMs Are Not (Yet) the Silver Bullet for Unstructured Data Processing", "cluster": 13, "x": 6.808109760284424, "y": 3.326476812362671}, {"id": 43905917, "title": ".janicre: Making Codebases Digestible for LLMs", "cluster": 13, "x": 6.782151699066162, "y": 3.7789554595947266}, {"id": 43905245, "title": "Function Calling Using LLMs", "cluster": 13, "x": 6.796924591064453, "y": 3.6216089725494385}, {"id": 43905048, "title": "Message Queueing with LavinMQ", "cluster": 13, "x": 6.749433517456055, "y": 3.511725425720215}, {"id": 43904862, "title": "What's one thing ChatGPT or any other LLM can't help with when learning English?", "cluster": 13, "x": 6.612093448638916, "y": 3.235375165939331}, {"id": 43904409, "title": "IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite LLM", "cluster": 13, "x": 7.075990200042725, "y": 3.9065990447998047}, {"id": 43903230, "title": "Understanding Token Splitting Attacks in LLMs", "cluster": 13, "x": 6.4003143310546875, "y": 3.3460710048675537}, {"id": 43901835, "title": "I Build with LLMs", "cluster": 13, "x": 6.751369476318359, "y": 3.613212823867798}, {"id": 43901040, "title": "Building an LLM Rig", "cluster": 13, "x": 6.837489128112793, "y": 3.681316375732422}, {"id": 43898966, "title": "ReadMe.LLM: LLM-Oriented Documentation", "cluster": 13, "x": 6.856974124908447, "y": 3.479067802429199}, {"id": 43897706, "title": "Why Run RL? How specialized models can outperform the biggest LLMs", "cluster": 13, "x": 6.762775421142578, "y": 3.3110885620117188}, {"id": 43897507, "title": "Why do all LLMs, even the most recent ones, hallucinate so often?", "cluster": 13, "x": 6.576672077178955, "y": 2.9988858699798584}, {"id": 43897297, "title": "The Em Dash Conspiracy: More and More of Reddit Is from LLMs", "cluster": 13, "x": 6.51263427734375, "y": 3.0458984375}, {"id": 43897320, "title": "As an experienced LLM user, I don't use generative LLMs often", "cluster": 13, "x": 6.661290168762207, "y": 3.325343132019043}, {"id": 43897123, "title": "Datasets Are All You Need (LLM Learns to Prompt from Data)", "cluster": 13, "x": 6.862359046936035, "y": 3.3989744186401367}, {"id": 43896642, "title": "Chat with an Uncensored LLM", "cluster": 13, "x": 6.505987644195557, "y": 3.3235602378845215}, {"id": 43895991, "title": "LLMs are like compilers, sort of", "cluster": 13, "x": 6.736076354980469, "y": 3.6358885765075684}, {"id": 43896033, "title": "PII Guard - LLM-powered GDPR compliance", "cluster": 13, "x": 6.595139026641846, "y": 3.4829301834106445}, {"id": 43895382, "title": "Example LLM code dysfunction \u2013 Go methods", "cluster": 13, "x": 6.6822943687438965, "y": 3.7324488162994385}, {"id": 43895366, "title": "Open Source (and Open Data) LLM", "cluster": 13, "x": 6.830008506774902, "y": 3.6755642890930176}, {"id": 43895256, "title": "Why Elixir/OTP doesn't need an Agent framework (2023)", "cluster": 38, "x": 8.676608085632324, "y": 3.64268159866333}, {"id": 43893392, "title": "The LLM Meta-Leaderboard averaged across the 28 best benchmarks", "cluster": 13, "x": 6.962285041809082, "y": 3.3750312328338623}, {"id": 43892803, "title": "The Engine Jonathan Swift (Sort of) Predicts Large Language Models in 1726", "cluster": 208, "x": 8.48551082611084, "y": 4.211000442504883}, {"id": 43892200, "title": "Fighting LLMs using LLMs", "cluster": 13, "x": 6.664367198944092, "y": 3.334951162338257}, {"id": 43891725, "title": "meet Speaksy. top-tier, uncensored LLMs didn't feel accessible.", "cluster": 13, "x": 6.529835224151611, "y": 3.1911511421203613}, {"id": 43891449, "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "cluster": 13, "x": 6.962689399719238, "y": 3.165400505065918}, {"id": 43890901, "title": "LLM ported to the C64", "cluster": 13, "x": 7.074612140655518, "y": 3.8916752338409424}, {"id": 43890538, "title": "Matrix-vector multiplication implemented in off-the-shelf DRAM for Low-Bit LLMs", "cluster": 13, "x": 7.035884857177734, "y": 3.687312126159668}, {"id": 43890237, "title": "Transform DOCX into LLM-ready data", "cluster": 13, "x": 6.928928852081299, "y": 3.6176435947418213}, {"id": 43890009, "title": "What Siri Isn't: Perplexity's Voice Assistant and LLMs Integrated with iOS", "cluster": 13, "x": 6.894899845123291, "y": 3.7526354789733887}, {"id": 43889659, "title": "LLM-powered tool to detect PII in logs for privacy and GDPR compliance (for fun)", "cluster": 13, "x": 6.541525840759277, "y": 3.458709716796875}, {"id": 43888568, "title": "LLMs as Unbiased Oracles", "cluster": 13, "x": 6.717746734619141, "y": 3.0746750831604004}, {"id": 43888485, "title": "Are LLMs Natural Born Bullshitters?", "cluster": 13, "x": 6.514427661895752, "y": 3.0572450160980225}, {"id": 43887834, "title": "Unpredictable Patterns #117: Agency-enhancing technologies", "cluster": 38, "x": 8.660124778747559, "y": 3.68174147605896}, {"id": 43887632, "title": "Physics of Language Models: Architecture Design and the Magic of Canon Layers", "cluster": 208, "x": 8.501806259155273, "y": 4.399021625518799}, {"id": 43887637, "title": "Dummy's Guide to Modern LLM Sampling", "cluster": 13, "x": 6.889089584350586, "y": 3.3328893184661865}, {"id": 43887212, "title": "AI/LLM CodeGen Tooling via Dependency Injection", "cluster": 12, "x": 7.030326843261719, "y": 2.6550538539886475}, {"id": 43884833, "title": "CMU TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks", "cluster": 13, "x": 6.992332458496094, "y": 3.1138486862182617}, {"id": 43882939, "title": "An LLM\u2011as\u2011Judge Won't Save the Product\u2013Fixing Your Process Will", "cluster": 13, "x": 6.486673831939697, "y": 3.1494810581207275}, {"id": 43882857, "title": "A little bit of human-provided structure gives better LLM answers", "cluster": 13, "x": 6.73268461227417, "y": 3.0911009311676025}, {"id": 43882115, "title": "Testing in the Age of LLM-Generated Code", "cluster": 13, "x": 6.759894371032715, "y": 3.6791954040527344}, {"id": 43881740, "title": "Reinforcement Learning for Reasoning in LLMs with One Training Example", "cluster": 13, "x": 6.914449214935303, "y": 3.1982614994049072}, {"id": 43879702, "title": "Run LLMs on Apple Neural Engine (ANE)", "cluster": 13, "x": 6.899313449859619, "y": 3.6543102264404297}, {"id": 43879399, "title": "Model Golf \u2013 code golf, but with LLMs", "cluster": 13, "x": 6.755582809448242, "y": 3.6253066062927246}, {"id": 43878948, "title": "Assessment of fine-tuned LLMs for real-world chemistry applications", "cluster": 13, "x": 6.838680744171143, "y": 3.286764144897461}, {"id": 43877978, "title": "Different ways to build LLVM/MLIR tools", "cluster": 13, "x": 6.825920104980469, "y": 3.6705756187438965}, {"id": 43877406, "title": "I Learned to Stop Worrying and Love L.A", "cluster": 13, "x": 6.450669288635254, "y": 3.047891855239868}, {"id": 43876488, "title": "Russian disinformation network flooded training data to manipulate frontier LLMs", "cluster": 13, "x": 6.8378777503967285, "y": 3.134432792663574}, {"id": 43876276, "title": "Agentic Bug Reproduction for Effective Automated Program Repair at Google", "cluster": 3, "x": 8.499773025512695, "y": 3.652719497680664}, {"id": 43876144, "title": "Discovering and analyzing values in real-world language model interactions", "cluster": 208, "x": 8.331306457519531, "y": 4.4483418464660645}, {"id": 43875079, "title": "Sycophancy is the first LLM \"dark pattern\"", "cluster": 13, "x": 6.608468532562256, "y": 2.9775242805480957}, {"id": 43873925, "title": "AI-LieDar: Examine the Trade-Off Between Utility and Truthfulness in LLM Agents", "cluster": 12, "x": 6.961909294128418, "y": 2.6670093536376953}, {"id": 43873777, "title": "Data set helps researchers spot harmful stereotypes in LLMs", "cluster": 13, "x": 6.78872013092041, "y": 3.014326810836792}, {"id": 43872956, "title": "Solo Bench \u2013 a new simple, cheap and objective benchmark for LLMs", "cluster": 13, "x": 6.953287601470947, "y": 3.4401564598083496}, {"id": 43872413, "title": "Build Real-Time Knowledge Graph for Documents with LLM", "cluster": 13, "x": 7.138644218444824, "y": 3.631514072418213}, {"id": 43872030, "title": "Linguistics Learned to Stop Worrying and Love the Language Models", "cluster": 208, "x": 8.288239479064941, "y": 4.344099044799805}, {"id": 43870818, "title": "Lp(a) particles are 6x more atherogenic than ordinary LDL", "cluster": 13, "x": 6.851149082183838, "y": 3.1967616081237793}, {"id": 43869282, "title": "Can Language Models Represent the Past Without Anachronism?", "cluster": 208, "x": 8.353281021118164, "y": 4.4024152755737305}, {"id": 43868837, "title": "Post-Chat UI: How LLMs are making traditional apps feel broken", "cluster": 13, "x": 6.659143924713135, "y": 3.4750561714172363}, {"id": 43868261, "title": "Creating Yoast SEO Metadata for MasterStudy LMS Using ChatGPT", "cluster": 13, "x": 6.832502841949463, "y": 3.8092639446258545}, {"id": 43866797, "title": "Emergent Misalignment: Narrow Finetuning Can Produce Broadly Misaligned LLMs", "cluster": 13, "x": 6.726990699768066, "y": 3.021216630935669}, {"id": 43862937, "title": "Can an LLM care for my plants (or at least kill them slower than I do)?", "cluster": 13, "x": 6.587881088256836, "y": 3.1631669998168945}, {"id": 43862727, "title": "OpenAI Says Its LLM Can Write Creatively", "cluster": 13, "x": 6.698254108428955, "y": 3.6639280319213867}, {"id": 43860137, "title": "Llasa: Llama-Based Speech Synthesis", "cluster": 13, "x": 6.908124923706055, "y": 3.8178842067718506}, {"id": 43859742, "title": "Streaming LLM Responses with Rails: SSE vs. Turbo Streams", "cluster": 13, "x": 6.873586177825928, "y": 3.6695120334625244}, {"id": 43857654, "title": "Large Language Models, Small Labor Market Effects", "cluster": 208, "x": 8.20720100402832, "y": 4.420241832733154}, {"id": 43857244, "title": "RustAssistant: Using LLMs to Fix Compilation Errors in Rust Code", "cluster": 13, "x": 6.9900689125061035, "y": 4.09466028213501}, {"id": 43857030, "title": "\"Chatting\" with LLM feels like using an 80s computer terminal", "cluster": 13, "x": 6.601527214050293, "y": 3.4197018146514893}, {"id": 43856754, "title": "Reinforcement Learning for Reasoning in LLMs with One Training Example", "cluster": 13, "x": 6.921212673187256, "y": 3.186495542526245}, {"id": 43855520, "title": "Changemyview LLM Persuasion Study", "cluster": 13, "x": 6.690911769866943, "y": 3.0946907997131348}, {"id": 43855133, "title": "I Compared 3 LLMs for Technical Research: o4-mini-high vs. o3 vs. Deep Research", "cluster": 13, "x": 6.861667633056641, "y": 3.346411943435669}, {"id": 43854226, "title": "OpenRouter LLM Rankings", "cluster": 13, "x": 6.895036220550537, "y": 3.485745429992676}, {"id": 43853742, "title": "Pre-Trained Security LLM 8B", "cluster": 13, "x": 6.593931674957275, "y": 3.4372570514678955}, {"id": 43851789, "title": "\"It Listens Better Than My Therapist\": Discourse on LLMs and Mental Health", "cluster": 13, "x": 6.5849714279174805, "y": 3.068222999572754}, {"id": 43851212, "title": "LLMs for Engineering: Teaching Models to Design High Powered Rockets", "cluster": 13, "x": 6.707840442657471, "y": 3.3400676250457764}, {"id": 43851143, "title": "RustAssistant: Using LLMs to Fix Compilation Errors in Rust Code", "cluster": 13, "x": 7.006555080413818, "y": 4.081160545349121}, {"id": 43851099, "title": "Mercury: Commercial-scale diffusion language model", "cluster": 208, "x": 8.295557975769043, "y": 4.373409271240234}, {"id": 43849848, "title": "Any good new CRMs out there?", "cluster": 13, "x": 6.6553826332092285, "y": 3.381765365600586}, {"id": 43849008, "title": "Data set helps researchers spot harmful stereotypes in LLMs", "cluster": 13, "x": 6.759559631347656, "y": 3.0489649772644043}, {"id": 43847803, "title": "RL for Reasoning in LLMs with One Training Example", "cluster": 13, "x": 6.998505592346191, "y": 3.2811503410339355}, {"id": 43846835, "title": "Unlocking LLM Performance with Inference Compute", "cluster": 13, "x": 7.119107723236084, "y": 3.468179225921631}, {"id": 43845702, "title": "LLVM-Mca \u2013 LLVM Machine Code Analyzer", "cluster": 13, "x": 6.909416675567627, "y": 3.7292001247406006}, {"id": 43845018, "title": "Security considerations for Qwen3 LLM's dynamic think/nothink", "cluster": 13, "x": 6.47434663772583, "y": 3.3249335289001465}, {"id": 43844478, "title": "Direct chat with LLM from address bar in Firefox", "cluster": 13, "x": 6.763696193695068, "y": 3.6718931198120117}, {"id": 43843631, "title": "Popular LLMs Found to Produce Vulnerable Code by Default", "cluster": 13, "x": 6.530676364898682, "y": 3.6023643016815186}, {"id": 43842819, "title": "Thought on Honest Knowledge Seeking and LLMs (\"AI\")", "cluster": 12, "x": 6.9796061515808105, "y": 2.6395390033721924}, {"id": 43842110, "title": "End-to-end private LLM inference", "cluster": 13, "x": 6.79323148727417, "y": 3.4545958042144775}, {"id": 43842074, "title": "The Extended Mind and Challenges with LLM's", "cluster": 13, "x": 6.6546478271484375, "y": 3.018378734588623}, {"id": 43841897, "title": "LIFT+: Lightweight Fine-Tuning for Long-Tail Learning", "cluster": 13, "x": 7.137118339538574, "y": 3.373389720916748}, {"id": 43840247, "title": "LlamaFirewall", "cluster": 13, "x": 6.926339626312256, "y": 3.60715913772583}, {"id": 43839664, "title": "Envisioning Recommendations on an LLM-Based Agent Platform", "cluster": 13, "x": 6.8278889656066895, "y": 3.3299527168273926}, {"id": 43837835, "title": "Cheapest Batch LLM Inference", "cluster": 13, "x": 7.072223663330078, "y": 3.4439406394958496}, {"id": 43838006, "title": "A tool for migrating and optimizing prompts from other LLMs to Llama", "cluster": 13, "x": 6.8981122970581055, "y": 3.7233264446258545}, {"id": 43835928, "title": "Llama 4 reasoning 17B model releasing today", "cluster": 13, "x": 7.287093162536621, "y": 4.03038215637207}, {"id": 43835495, "title": "Bamba: An open-source LLM that crosses a transformer with an SSM", "cluster": 13, "x": 6.9044189453125, "y": 3.7633233070373535}, {"id": 43833180, "title": "Top LLMs in SQL Semantic Reasoning\uff1aDeepSeek R1, GPT-4o, and Claude 3.7 Sonnet", "cluster": 13, "x": 7.064826965332031, "y": 3.5323662757873535}, {"id": 43833004, "title": "APE \u2013 0: An LLM Co-Pilot for Chemical Process Design", "cluster": 13, "x": 6.712762355804443, "y": 3.468834638595581}, {"id": 43832475, "title": "A love letter to LLM", "cluster": 13, "x": 6.572020053863525, "y": 3.139817237854004}, {"id": 43828486, "title": "Rtems", "cluster": 13, "x": 7.034140586853027, "y": 3.6878061294555664}, {"id": 43828239, "title": "LLM Arena Pareto Frontier", "cluster": 13, "x": 7.018264293670654, "y": 3.3730874061584473}, {"id": 43827993, "title": "Crawl4AI is an open-source, LLM-friendly web crawler and scraper", "cluster": 13, "x": 6.936833381652832, "y": 3.4183220863342285}, {"id": 43826607, "title": "Bringing a Bit of Light into LLMs.txt for Generative Search Optimization", "cluster": 13, "x": 6.986471176147461, "y": 3.601320266723633}, {"id": 43824167, "title": "Ask HN: Examples of LLMs building with primitives MCP?", "cluster": 13, "x": 6.9054341316223145, "y": 3.5289249420166016}, {"id": 43823899, "title": "Can LLMs do randomness?", "cluster": 13, "x": 6.6881585121154785, "y": 3.195014476776123}, {"id": 43823523, "title": "Streamlining Biomedical Research with Specialized LLMs", "cluster": 13, "x": 6.808488368988037, "y": 3.284205436706543}, {"id": 43821292, "title": "Bad but common LLM criticisms", "cluster": 13, "x": 6.6332197189331055, "y": 3.0849125385284424}, {"id": 43821358, "title": "RAG systems can make LLMs less safe, not more", "cluster": 13, "x": 6.4869537353515625, "y": 3.121762275695801}, {"id": 43821410, "title": "Dagger v0.18: LLM as a new software primitive", "cluster": 13, "x": 6.864744186401367, "y": 3.780540943145752}, {"id": 43821040, "title": "A Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis", "cluster": 13, "x": 6.937497615814209, "y": 3.470526695251465}, {"id": 43820837, "title": "LLM evaluation for builders: free applied course", "cluster": 13, "x": 6.828067302703857, "y": 3.3703856468200684}, {"id": 43820468, "title": "Sycophancy is the first LLM \"dark pattern\"", "cluster": 13, "x": 6.612875938415527, "y": 2.9879252910614014}, {"id": 43820022, "title": "Tiny-LLM \u2013 a course of serving LLM on Apple Silicon for systems engineers", "cluster": 13, "x": 6.805828094482422, "y": 3.6576297283172607}, {"id": 43819908, "title": "Unvibe: A Python Test-Runner that forces LLMs to generate correct code", "cluster": 13, "x": 6.713711738586426, "y": 3.7943480014801025}, {"id": 43818169, "title": "Naur's \"Programming as Theory Building\" and LLMs replacing human programmers", "cluster": 13, "x": 6.588801860809326, "y": 3.601130962371826}, {"id": 43817922, "title": "llm.pdf \u2013 Run LLMs inside a PDF file", "cluster": 13, "x": 6.949265003204346, "y": 3.695204973220825}, {"id": 43817377, "title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models", "cluster": 208, "x": 8.228493690490723, "y": 4.463105201721191}, {"id": 43814894, "title": "Multicalculator for Patients and Providers", "cluster": 13, "x": 7.079892158508301, "y": 3.4609808921813965}, {"id": 43814358, "title": "AI-Driven Sprint Planning: Using LLMs to Automate Capacity Recommendations", "cluster": 12, "x": 7.066526412963867, "y": 2.6235413551330566}, {"id": 43813879, "title": "Do Large Language Models know who did what to whom?", "cluster": 208, "x": 8.257795333862305, "y": 4.321102142333984}, {"id": 43813659, "title": "LLVM adds processor definition for XiangShan-KunMingHu-V2R2", "cluster": 13, "x": 7.150820732116699, "y": 4.04028844833374}, {"id": 43813309, "title": "Create Missing RSS Feeds with LLMs", "cluster": 13, "x": 6.819141864776611, "y": 3.6696534156799316}, {"id": 43813384, "title": "Google Agent Development Kit", "cluster": 35, "x": 8.42736530303955, "y": 3.8965728282928467}, {"id": 43813106, "title": "The evolution of Lua, continued [pdf]", "cluster": 12, "x": 7.079709053039551, "y": 2.6191651821136475}, {"id": 43812545, "title": "Use OpenAPI Instead of MCP for LLM Tools", "cluster": 13, "x": 6.8792805671691895, "y": 3.686802625656128}, {"id": 43812310, "title": "LLMs are making me a better engineer", "cluster": 13, "x": 6.641615867614746, "y": 3.4079384803771973}, {"id": 43809192, "title": "An LLM\u2011as\u2011Judge Won't Save the Product\u2013Fixing Your Process Will", "cluster": 13, "x": 6.447980880737305, "y": 3.139796018600464}, {"id": 43807168, "title": "Reward hacking is becoming more sophisticated and deliberate in frontier LLMs", "cluster": 13, "x": 6.440919876098633, "y": 3.3837337493896484}, {"id": 43806374, "title": "Trust but Verify: Sensible Ways to Use LLMs in Production", "cluster": 13, "x": 6.725201606750488, "y": 3.2992255687713623}, {"id": 43805478, "title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing, Improving LLMs [pdf]", "cluster": 13, "x": 6.827797889709473, "y": 3.2123053073883057}, {"id": 43804036, "title": "Optimizing Large Language Model Deployment in Edge Computing Environments", "cluster": 208, "x": 8.03796100616455, "y": 4.490662574768066}, {"id": 43803962, "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens", "cluster": 208, "x": 8.278959274291992, "y": 4.567440509796143}, {"id": 43803518, "title": "LLMs can see and hear without any training", "cluster": 13, "x": 6.529340744018555, "y": 3.070129156112671}, {"id": 43803469, "title": "An LLM-Based Approach to Review Summarization on the App Store", "cluster": 13, "x": 6.91692590713501, "y": 3.3165106773376465}, {"id": 43803471, "title": "Build your own tech writing tools using LLMs", "cluster": 13, "x": 6.698375225067139, "y": 3.6497373580932617}, {"id": 43803333, "title": "Simple \u2013 daily LLM puzzle game", "cluster": 13, "x": 6.823519229888916, "y": 3.180691957473755}, {"id": 43803119, "title": "Empowering AI to Generate Better AI Code: Generation of DL Projects with LLMs", "cluster": 12, "x": 7.047008037567139, "y": 2.6134071350097656}, {"id": 43802923, "title": "We've Been Conned: The Truth about Big LLM", "cluster": 13, "x": 6.529709339141846, "y": 3.031557083129883}, {"id": 43801749, "title": "Virtual Machinations: Using Large Language Models as Neural Computers", "cluster": 208, "x": 8.236529350280762, "y": 4.383994102478027}, {"id": 43801164, "title": "Can LLMs reason logically? If not, how can we teach them? (2024)", "cluster": 13, "x": 6.7973198890686035, "y": 3.1390750408172607}, {"id": 43799673, "title": "Tonic AI acquires LLM based synthetic data tool, Fabricate", "cluster": 12, "x": 7.026689052581787, "y": 2.61832332611084}, {"id": 43799306, "title": "ACM's flagship magazine seeks submissions by/for practitioners", "cluster": 13, "x": 6.833064079284668, "y": 3.2678332328796387}, {"id": 43798775, "title": "Do Large Language Models Understand Logic or Just Mimick Context? (2024)", "cluster": 208, "x": 8.208650588989258, "y": 4.2979278564453125}, {"id": 43797708, "title": "Pdf Categorizer by Local LLMs", "cluster": 13, "x": 6.984907627105713, "y": 3.605236291885376}, {"id": 43796935, "title": "Lossless LLM compression for efficient GPU inference via dynamic-length float", "cluster": 13, "x": 7.191061019897461, "y": 3.684771776199341}, {"id": 43796452, "title": "Context7 \u2013 Up-to-date documentation for LLMs and AI code editors", "cluster": 12, "x": 7.078914642333984, "y": 2.635793685913086}, {"id": 43794455, "title": "DeepL vs. LLMs for Translation", "cluster": 13, "x": 6.763949394226074, "y": 3.4084064960479736}, {"id": 43794122, "title": "Can an LLM take care of my plants (or at least kill them slower than I do)?", "cluster": 13, "x": 6.575218200683594, "y": 3.161177158355713}, {"id": 43793280, "title": "The Policy Puppetry Attack: Novel bypass for major LLMs", "cluster": 13, "x": 6.365909576416016, "y": 3.171827793121338}, {"id": 43792448, "title": "Agents in your software factory: Introducing the LLM primitive in Dagger", "cluster": 13, "x": 6.914503574371338, "y": 3.3669629096984863}, {"id": 43791843, "title": "Privacy folks \u2013 what's your take on using LLMs at work?", "cluster": 13, "x": 6.490302085876465, "y": 3.4885807037353516}, {"id": 43791228, "title": "How to Make MS Teams Usable (By Emulating Slack)", "cluster": 13, "x": 6.840160846710205, "y": 3.780963182449341}, {"id": 43791385, "title": "Large language models, small labor market effects [pdf]", "cluster": 208, "x": 8.228632926940918, "y": 4.49967622756958}, {"id": 43791380, "title": "Microsoft BitNet B1.58 2B4T \u2013 Scaling Native 1-Bit LLM", "cluster": 13, "x": 7.0384650230407715, "y": 3.7220282554626465}, {"id": 43791248, "title": "LLMs with the Model Context Protocol Allow Major Security Exploits", "cluster": 13, "x": 6.476059436798096, "y": 3.4705612659454346}, {"id": 43790947, "title": "Open-source LLM that has been trained to do interleaved function call reasoning", "cluster": 13, "x": 6.88429594039917, "y": 3.6383447647094727}, {"id": 43790238, "title": "LLM Power Typing Tutoring", "cluster": 13, "x": 6.884647369384766, "y": 3.557943820953369}, {"id": 43790117, "title": "Universal prompt to jailbreak all LLMs", "cluster": 13, "x": 6.796604156494141, "y": 3.731151580810547}, {"id": 43789653, "title": "An Interactive Overview of Grammar-Based Sampling for LLMs", "cluster": 13, "x": 6.91109561920166, "y": 3.4410343170166016}, {"id": 43788391, "title": "The State of Reinforcement Learning for LLM Reasoning", "cluster": 13, "x": 6.923651695251465, "y": 3.2049667835235596}, {"id": 43787493, "title": "Agent Mesh for Enterprise Agents", "cluster": 38, "x": 8.58158016204834, "y": 3.6316137313842773}, {"id": 43787281, "title": "LLMs as basic building blocks of software delivery", "cluster": 13, "x": 6.635664939880371, "y": 3.4958479404449463}, {"id": 43787134, "title": "Lyria RealTime music generation model", "cluster": 13, "x": 7.175235748291016, "y": 3.9468562602996826}, {"id": 43785780, "title": "Leaping Forward: Imagination and the Evolution of LLMs", "cluster": 13, "x": 6.635107040405273, "y": 3.039463758468628}, {"id": 43785468, "title": "An LLM-Based Approach to Review Summarization on the App Store", "cluster": 13, "x": 6.884145736694336, "y": 3.2762491703033447}, {"id": 43785409, "title": "Docker Model Runner Brings Local LLMs to Your Desktop", "cluster": 13, "x": 6.95668888092041, "y": 3.8228981494903564}, {"id": 43785124, "title": "Novel Universal Bypass for All Major LLMs", "cluster": 13, "x": 6.48491907119751, "y": 3.2009007930755615}, {"id": 43784442, "title": "MCP's 3 U's: Making a Tool Useful, Usable, and Used by and for an LLM", "cluster": 13, "x": 6.752623558044434, "y": 3.312342405319214}, {"id": 43784195, "title": "Rethinking the Effectiveness of the LLM for Time Series Forecasting", "cluster": 13, "x": 6.6659135818481445, "y": 3.0751430988311768}, {"id": 43784070, "title": "One Prompt Can Bypass Every Major LLM's Safeguards", "cluster": 13, "x": 6.5376176834106445, "y": 3.3866159915924072}, {"id": 43782979, "title": "Google's Agent Development Kit \u2013 Getting Started Guide", "cluster": 35, "x": 8.46690845489502, "y": 3.8452813625335693}, {"id": 43782622, "title": "LLM comprehensive summary template for large texts", "cluster": 13, "x": 6.977982997894287, "y": 3.5092597007751465}, {"id": 43782163, "title": "Does RL Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "cluster": 13, "x": 6.88314962387085, "y": 3.17614483833313}, {"id": 43781386, "title": "Implement a ChatGPT-like LLM in PyTorch from scratch, step by step", "cluster": 13, "x": 6.807621479034424, "y": 3.818492889404297}, {"id": 43780698, "title": "ASML, creator of lithography machines, has a messy software stack", "cluster": 13, "x": 6.314341068267822, "y": 3.6306650638580322}, {"id": 43780807, "title": "High-quality search engine without LLM-generated results?", "cluster": 13, "x": 6.763865947723389, "y": 3.5835821628570557}, {"id": 43779788, "title": "MCP's 3 U's: Making a Tool Useful, Usable, and Used by and for an LLM", "cluster": 13, "x": 6.777713298797607, "y": 3.2897450923919678}, {"id": 43778659, "title": "Proponent of LLM for Software Engineers", "cluster": 13, "x": 6.527572154998779, "y": 3.5766499042510986}, {"id": 43778643, "title": "How LLMs learn to reason: A deep dive into post-training strategies", "cluster": 13, "x": 6.70611047744751, "y": 3.0978646278381348}, {"id": 43777602, "title": "LLVM-Libs-Debloated", "cluster": 13, "x": 6.788785934448242, "y": 3.6213672161102295}, {"id": 43777138, "title": "Aethr: Zero-script QA automation using Playwright MCP and LLM", "cluster": 13, "x": 6.902314186096191, "y": 3.7060506343841553}, {"id": 43776074, "title": "Lichen -- a License management CLI tool", "cluster": 13, "x": 7.056789398193359, "y": 4.059889316558838}, {"id": 43775928, "title": "Everybody Prune Now: Structured Pruning of LLMs with Only Forward Passes", "cluster": 13, "x": 6.85648775100708, "y": 3.3042490482330322}, {"id": 43774990, "title": "Teaching LLMs how to solid model", "cluster": 13, "x": 6.7193603515625, "y": 3.2484498023986816}, {"id": 43774761, "title": "Treating Large Language Models as Potentially Informative Observations", "cluster": 208, "x": 8.289291381835938, "y": 4.34614896774292}, {"id": 43773990, "title": "MamayLM: An Efficient Ukrainian LLM", "cluster": 13, "x": 6.645094871520996, "y": 3.1047825813293457}, {"id": 43774015, "title": "LLMs vs. 1k-Page PDFs: Why AI Fails at Semiconductor Docs", "cluster": 12, "x": 7.1324920654296875, "y": 2.6364798545837402}, {"id": 43773944, "title": "Exploring prompt injection defences with a local LLM", "cluster": 13, "x": 6.480538368225098, "y": 3.313969373703003}, {"id": 43773488, "title": "Successful CMMS Rollouts", "cluster": 13, "x": 6.698141098022461, "y": 3.538860321044922}, {"id": 43772909, "title": "Beyond Code Generation: Using LLMs as Validated Functional Components", "cluster": 13, "x": 6.745542526245117, "y": 3.7084901332855225}, {"id": 43769502, "title": "GateLLM \u2013 Open-source gateway to secure your LLM usage", "cluster": 13, "x": 6.67934513092041, "y": 3.6114907264709473}, {"id": 43768322, "title": "Topologically Aware Job Scheduling for Slurm [pdf] (2014)", "cluster": 13, "x": 7.065710067749023, "y": 3.501422882080078}, {"id": 43768402, "title": "An LLM\u2011as\u2011Judge Won't Save Your Product\u2013Fixing Your Process Will", "cluster": 13, "x": 6.471737861633301, "y": 3.1375339031219482}, {"id": 43767989, "title": "Detecting Malicious Source Code in PyPI Packages with LLMs", "cluster": 13, "x": 6.53853702545166, "y": 3.5699102878570557}, {"id": 43767791, "title": "Demand for LLMs: Evidence on Substitution, Market Expansion, Multihoming", "cluster": 13, "x": 6.747541427612305, "y": 3.133023977279663}, {"id": 43767468, "title": "Advanced Quantization Algorithm for LLMs/VLMs", "cluster": 13, "x": 6.988571643829346, "y": 3.6263365745544434}, {"id": 43767385, "title": "Frontiers of GenAI \u2013 Foundation LLMs, Grounded LLMs, & Reasoning LLMs", "cluster": 13, "x": 6.934859275817871, "y": 3.1817874908447266}, {"id": 43767058, "title": "Meaning Machine \u2013 Visualize how LLMs break down and simulate meaning", "cluster": 13, "x": 6.855456829071045, "y": 3.241252899169922}, {"id": 43766332, "title": "LLM Robustness/Safety Benchmark", "cluster": 13, "x": 6.880654811859131, "y": 3.289936065673828}, {"id": 43765058, "title": "Values in the wild: Discovering values in real-world language model interactions", "cluster": 208, "x": 8.326776504516602, "y": 4.470022678375244}, {"id": 43765140, "title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data", "cluster": 13, "x": 6.951077461242676, "y": 3.2413506507873535}, {"id": 43765128, "title": "Prevent LLM Hallucinations with Trust Scoring in Nvidia NeMo Guardrails", "cluster": 13, "x": 6.541757106781006, "y": 3.117739677429199}, {"id": 43763905, "title": "Visual Language Models show widespread deficits on neuropsychological tests", "cluster": 208, "x": 8.607508659362793, "y": 4.125093460083008}, {"id": 43763843, "title": "Context7 MCP Server: Up-to-date documentation for LLMs and AI code editors", "cluster": 12, "x": 7.0610833168029785, "y": 2.7016756534576416}, {"id": 43763243, "title": "Semantically rank everything using LLMs (companies, candidates, ideas etc.)", "cluster": 13, "x": 6.979202747344971, "y": 3.2887139320373535}, {"id": 43761387, "title": "Should We Respect LLMs? A Study on Influence of Prompt Politeness on Performance", "cluster": 13, "x": 6.679450988769531, "y": 3.1161038875579834}, {"id": 43761129, "title": "El Reg's essential guide to deploying LLMs in production", "cluster": 13, "x": 6.776610374450684, "y": 3.482531785964966}, {"id": 43760625, "title": "Does RL Incentivize Reasoning in LLMs Beyond the Base Model?", "cluster": 13, "x": 6.881025791168213, "y": 3.1666791439056396}, {"id": 43760695, "title": "Today's LLMs craft exploits from patches at lightning speed", "cluster": 13, "x": 6.492896556854248, "y": 3.4089455604553223}, {"id": 43759835, "title": "The primary audience of your thing (product, service, library,) is now an LLM", "cluster": 13, "x": 6.650149822235107, "y": 3.409836769104004}, {"id": 43759509, "title": "Values in the wild: Discovering values in real-world language model interactions", "cluster": 208, "x": 8.327101707458496, "y": 4.457286357879639}, {"id": 43758459, "title": "Lessons Learned Writing a Book Collaboratively with LLMs", "cluster": 13, "x": 6.701401233673096, "y": 3.256520986557007}, {"id": 43758227, "title": "Long-term L1 execution layer proposal: replace the EVM with RISC-V", "cluster": 13, "x": 7.126142978668213, "y": 3.9951789379119873}, {"id": 43757404, "title": "Writing an LLM from scratch, part 12 \u2013 multi-head attention", "cluster": 13, "x": 6.712277412414551, "y": 3.168848752975464}, {"id": 43757204, "title": "A collection of reproducible LLM inference engine benchmarks: SGLang vs. vLLM", "cluster": 13, "x": 7.044259071350098, "y": 3.5234408378601074}, {"id": 43756352, "title": "Syntactic and semantic control of LLMs via sequential Monte Carlo [pdf]", "cluster": 13, "x": 6.920556545257568, "y": 3.3179099559783936}, {"id": 43755937, "title": "We used sparse autoencoders to explain LLM moderation flags of violent threats", "cluster": 13, "x": 6.857122898101807, "y": 3.629840135574341}, {"id": 43755809, "title": "LLMs aren't writing LLMs \u2013 why developers still matter", "cluster": 13, "x": 6.546566009521484, "y": 3.4561662673950195}, {"id": 43754900, "title": "Nazut News Summaries \u2013 an LLM curated summaries and rankings website", "cluster": 13, "x": 6.821018218994141, "y": 3.4679248332977295}, {"id": 43753825, "title": "Airbnb \u2013 Accelerating Large-Scale Test Migration with LLMs", "cluster": 13, "x": 6.967235088348389, "y": 3.5254628658294678}, {"id": 43753890, "title": "Local LLM inference \u2013 impressive but too hard to work with", "cluster": 13, "x": 6.9654388427734375, "y": 3.1817309856414795}, {"id": 43753295, "title": "Google Succeeds with LLMs While Meta and OpenAI Stumble", "cluster": 13, "x": 6.870736122131348, "y": 3.484478235244751}, {"id": 43752728, "title": "Industrial applications of large language models", "cluster": 208, "x": 8.061397552490234, "y": 4.4465460777282715}, {"id": 43752546, "title": "Is a PhD on Language Models Worth It in 2025?", "cluster": 208, "x": 8.227740287780762, "y": 4.485353946685791}, {"id": 43752492, "title": "LLM-powered tools amplify developer capabilities rather than replacing them", "cluster": 13, "x": 6.672956466674805, "y": 3.652560234069824}, {"id": 43752191, "title": "Gemini 2.5: The First LLM That Understands PDF Layouts", "cluster": 13, "x": 6.915270805358887, "y": 3.4243781566619873}, {"id": 43752119, "title": "LLM.pdf \u2013 Run LLMs Inside a PDF", "cluster": 13, "x": 6.967126846313477, "y": 3.682795524597168}, {"id": 43752024, "title": "We're running 50 LLMs on 2 GPUs \u2013 no cold starts, no overprovisioning", "cluster": 13, "x": 7.0135321617126465, "y": 3.6959946155548096}, {"id": 43751920, "title": "Can You Run the Llama 2 LLM on DOS?", "cluster": 13, "x": 6.968160629272461, "y": 3.8305490016937256}, {"id": 43751075, "title": "The Earliest Language Model", "cluster": 208, "x": 8.406113624572754, "y": 4.380795478820801}, {"id": 43749070, "title": "Optimizing Technical Docs for LLMs", "cluster": 13, "x": 6.9318389892578125, "y": 3.417612075805664}, {"id": 43748346, "title": "Prima.cpp \u2013 run 70B-Scale LLMs on low-powered home clusters", "cluster": 13, "x": 7.020357131958008, "y": 3.7416834831237793}, {"id": 43746807, "title": "The State of Reinforcement Learning for LLM Reasoning", "cluster": 13, "x": 6.893892765045166, "y": 3.184706211090088}, {"id": 43746564, "title": "Workers AI LLM Playground", "cluster": 13, "x": 6.718583106994629, "y": 3.164924144744873}, {"id": 43746008, "title": "Meta PLM Data", "cluster": 13, "x": 7.017512798309326, "y": 3.536928176879883}, {"id": 43745236, "title": "The Butterfly Effect in Agentic Workflows", "cluster": 38, "x": 8.67399787902832, "y": 3.636808395385742}, {"id": 43744809, "title": "To Make Language Models Work Better, Researchers Sidestep Language", "cluster": 208, "x": 8.232053756713867, "y": 4.218066215515137}, {"id": 43744534, "title": "LLM Runs Fully Inside a PDF", "cluster": 13, "x": 6.954321384429932, "y": 3.7560887336730957}, {"id": 43744574, "title": "LLMs Reduce Development Friction. Is That a Good Thing?", "cluster": 13, "x": 6.558045864105225, "y": 3.3572041988372803}, {"id": 43744256, "title": "How to scale LLM-based tabular data retrieval to millions of rows", "cluster": 13, "x": 6.9887003898620605, "y": 3.4890944957733154}, {"id": 43744343, "title": "Pushing the Limits of LLM Quantization via the Linearity Theorem", "cluster": 13, "x": 6.813150405883789, "y": 3.2171661853790283}, {"id": 43743171, "title": "LLM Fight Club", "cluster": 13, "x": 6.749368667602539, "y": 3.252779006958008}, {"id": 43740813, "title": "A curated blog for learning LLM internals: tokenize, attention, PE, and more", "cluster": 13, "x": 6.848644256591797, "y": 3.4514200687408447}, {"id": 43740583, "title": "Vending-Bench: The Simulation Exposing LLMs' Long-Term Focus Problem", "cluster": 13, "x": 6.871664047241211, "y": 3.065002679824829}, {"id": 43739881, "title": "Writing an LLM from scratch, part 11 \u2013 batches", "cluster": 13, "x": 6.7309465408325195, "y": 3.2466907501220703}, {"id": 43739489, "title": "Using Small Models to Compare Language Learning and Tokenizer Performance", "cluster": 208, "x": 8.310396194458008, "y": 4.439576148986816}, {"id": 43739401, "title": "Purposefully Induced Psychosis (Pip): Hallucination as Imagination in LLMs", "cluster": 13, "x": 6.570391654968262, "y": 2.9949538707733154}, {"id": 43737787, "title": "The State of Reinforcement Learning for LLM Reasoning", "cluster": 13, "x": 6.93573522567749, "y": 3.181659460067749}, {"id": 43737273, "title": "Why do LLMs fail at the basic visual tests", "cluster": 13, "x": 6.721002578735352, "y": 3.099242925643921}, {"id": 43736366, "title": "Inferring the Phylogeny of Large Language Models", "cluster": 208, "x": 8.193528175354004, "y": 4.341374397277832}, {"id": 43735978, "title": "ThinkFlow: The Revolutionary Platform That Gives LLMs the Power to Think", "cluster": 13, "x": 6.744596481323242, "y": 3.311511754989624}, {"id": 43735671, "title": "In-memory free-text search is a super-power for LLMs", "cluster": 13, "x": 6.833352565765381, "y": 3.5938408374786377}, {"id": 43734566, "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving", "cluster": 13, "x": 6.91727876663208, "y": 3.620220422744751}, {"id": 43734245, "title": "How to evaluate control measures for LLM agents?", "cluster": 13, "x": 6.947327136993408, "y": 3.1215243339538574}, {"id": 43733553, "title": "Hands-On Large Language Models", "cluster": 208, "x": 8.105489730834961, "y": 4.466435432434082}, {"id": 43732906, "title": "Why R the Critical Value and Emergent Behavior of Large Language Models Fake?", "cluster": 208, "x": 8.296581268310547, "y": 4.300220489501953}, {"id": 43731721, "title": "mRNA-LM: full-length integrated SLM for mRNA analysis", "cluster": 13, "x": 7.079206466674805, "y": 3.5744364261627197}, {"id": 43730514, "title": "Beyond Public Access in LLM Pre-Training Data: Book Content in OpenAI's Models [pdf]", "cluster": 13, "x": 6.925613880157471, "y": 3.534904956817627}, {"id": 43729852, "title": "MageSQL: Enhancing In-Context Learning for Text-to-SQL Applications with LLMs", "cluster": 13, "x": 6.954604625701904, "y": 3.5386760234832764}, {"id": 43729721, "title": "SEO for LLMs, my research and conclusion", "cluster": 13, "x": 6.815891742706299, "y": 3.4145848751068115}, {"id": 43729635, "title": "Self-Steering Language Models", "cluster": 208, "x": 8.356379508972168, "y": 4.426034927368164}, {"id": 43729364, "title": "More is more: Addition bias in large language models", "cluster": 208, "x": 8.253880500793457, "y": 4.355471134185791}, {"id": 43729080, "title": "The Most Expensive Part of an LLM Should Be Its Training Data", "cluster": 13, "x": 6.892477035522461, "y": 3.3551502227783203}, {"id": 43728527, "title": "Interact with LLMs in Go", "cluster": 13, "x": 6.735813140869141, "y": 3.371215581893921}, {"id": 43726013, "title": "Parameter-Efficient Fine-Tuning of LLMs for Personality Detection", "cluster": 13, "x": 6.904038429260254, "y": 3.3096425533294678}, {"id": 43724556, "title": "PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-Bit LLMs", "cluster": 13, "x": 7.033597946166992, "y": 3.718667984008789}, {"id": 43723766, "title": "Comparing LLMs with Neuro-Symbolic on Arithmetic Relations in Abstract Reasoning", "cluster": 13, "x": 6.877440452575684, "y": 3.1524107456207275}, {"id": 43722725, "title": "List of Autonomous LLM Agent Projects", "cluster": 13, "x": 6.938078880310059, "y": 3.2313144207000732}, {"id": 43722747, "title": "We built a lightweight transformer that stops LLM jailbreaks in real-time (20ms)", "cluster": 13, "x": 7.003846645355225, "y": 3.922118663787842}, {"id": 43722052, "title": "IBM Granite 3.3: Speech recognition, refined reasoning, and RAG LoRAs", "cluster": 13, "x": 7.162228584289551, "y": 3.7755072116851807}, {"id": 43721301, "title": "Recursive LLM prompts", "cluster": 13, "x": 6.795085430145264, "y": 3.389014482498169}, {"id": 43719427, "title": "The Impact of MCP and LLMs on Software Development \u2013 A Practical Example", "cluster": 13, "x": 6.622492790222168, "y": 3.4802064895629883}, {"id": 43716968, "title": "Guiding an LLM for Robust Java ByteBuffer Code", "cluster": 13, "x": 6.7557854652404785, "y": 3.7292697429656982}, {"id": 43716432, "title": "Exploring spaCy-based prompt compression for LLMs \u2013 thoughts welcome", "cluster": 13, "x": 6.8625874519348145, "y": 3.3920035362243652}, {"id": 43714860, "title": "LLMs vs. Compilers: Why the Rules Don't Align", "cluster": 13, "x": 6.80830717086792, "y": 3.599036931991577}, {"id": 43714987, "title": "The Age of Personalized Software: How LLMs Are Changing Programming", "cluster": 13, "x": 6.561799049377441, "y": 3.6265947818756104}, {"id": 43714326, "title": "LLM API prices are all over the place I made a simple calculator to compare them", "cluster": 13, "x": 6.940481662750244, "y": 3.5419697761535645}, {"id": 43713858, "title": "Meta's AI Boss Says He's Done with LLMs [video]", "cluster": 12, "x": 6.942957878112793, "y": 2.6371469497680664}, {"id": 43713687, "title": "Llama 2 LLM on DOS", "cluster": 13, "x": 6.972136497497559, "y": 3.803986072540283}, {"id": 43713573, "title": "A Scalable Standard for Clean ECommerce Data in LLMs (Fork of Llms.txt)", "cluster": 13, "x": 6.95916223526001, "y": 3.7194461822509766}, {"id": 43713405, "title": "No Free Lunch with Guardrails: Evaluating LLM Safety Tradeoffs", "cluster": 13, "x": 6.617524147033691, "y": 3.1334269046783447}, {"id": 43712897, "title": "What Should We Engineer in Prompts:Training Humans in Requirement-Driven LLM Use", "cluster": 13, "x": 6.745910167694092, "y": 3.2334401607513428}, {"id": 43712349, "title": "Modest Natural-Language Processing", "cluster": 208, "x": 8.245035171508789, "y": 4.276458740234375}, {"id": 43711545, "title": "The Missing Manual: Safety Engineering for Organizations Using LLMs", "cluster": 13, "x": 6.753507614135742, "y": 3.334629535675049}, {"id": 43709999, "title": "Turn React Hooks into LLM Tools", "cluster": 13, "x": 6.801441669464111, "y": 3.7205634117126465}, {"id": 43709809, "title": "Site-Llms.xml \u2013 An XML Sitemap Standard for AI-Friendly ECommerce Data", "cluster": 12, "x": 7.016177177429199, "y": 2.8760509490966797}, {"id": 43708973, "title": "Epistemic Alignment: A Mediating Framework for User-LLM Knowledge Delivery", "cluster": 13, "x": 6.827191352844238, "y": 3.195277452468872}, {"id": 43708691, "title": "The Proxy Company \u2013 Keeping LLMs in Control", "cluster": 13, "x": 6.5947747230529785, "y": 3.2166197299957275}, {"id": 43708414, "title": "What Happen to Ask Solem", "cluster": 13, "x": 6.499423980712891, "y": 3.074212074279785}, {"id": 43706901, "title": "Transformer-Based LLMs Are Not a Likely Path to AGI", "cluster": 13, "x": 6.546116828918457, "y": 3.1051604747772217}, {"id": 43706667, "title": "Can LLMs earn $1M from real freelance coding work?", "cluster": 13, "x": 6.600198268890381, "y": 3.5881011486053467}, {"id": 43706448, "title": "Fun with LLM-Generated JavaScript Bookmarks", "cluster": 13, "x": 6.807972431182861, "y": 3.716214418411255}, {"id": 43706124, "title": "Blocked Matrix Formulation of Linear Attention Mechanisms", "cluster": 13, "x": 7.0097455978393555, "y": 3.1552200317382812}, {"id": 43705920, "title": "LLMs Do Not Predict the Next Word", "cluster": 13, "x": 6.5954484939575195, "y": 3.1303982734680176}, {"id": 43704849, "title": "LlmTornado \u2013 The .NET library to consume 100 LLM APIs", "cluster": 13, "x": 6.942983150482178, "y": 3.7677793502807617}, {"id": 43704219, "title": "Microsoft BitNet 1.58bit LLM 2B4T released", "cluster": 13, "x": 6.989521503448486, "y": 3.8779077529907227}, {"id": 43702888, "title": "Llama 2 LLM on DOS", "cluster": 13, "x": 6.9747772216796875, "y": 3.8090572357177734}, {"id": 43702491, "title": "To Make Language Models Work Better, Researchers Sidestep Language", "cluster": 208, "x": 8.235434532165527, "y": 4.230435848236084}, {"id": 43702035, "title": "Go-away (another HTTP proxy for LLM scraper defence)", "cluster": 13, "x": 6.7388200759887695, "y": 3.693652391433716}, {"id": 43701941, "title": "Leveraging LLM feedback to enhance review quality", "cluster": 13, "x": 6.799518585205078, "y": 3.2715861797332764}, {"id": 43699271, "title": "12-factor Agents: Patterns of reliable LLM applications", "cluster": 13, "x": 6.909821033477783, "y": 3.1863770484924316}, {"id": 43698769, "title": "Neuromorphic Principles for Efficient Large Language Models on Intel Loihi 2", "cluster": 208, "x": 8.26348876953125, "y": 4.458917140960693}, {"id": 43697532, "title": "Liquid: Language models are scalable and unified multi-modal generators", "cluster": 208, "x": 8.282609939575195, "y": 4.585886478424072}, {"id": 43696040, "title": "Scaling Laws of Synthetic Data for Language Models", "cluster": 208, "x": 8.33332633972168, "y": 4.336483001708984}, {"id": 43695880, "title": "Large Language Models in Machine Translation (2007) [pdf]", "cluster": 208, "x": 8.296591758728027, "y": 4.527559757232666}, {"id": 43695032, "title": "Lambda-CDM Is Not Like the Standard Model", "cluster": 13, "x": 6.718304634094238, "y": 3.195136547088623}, {"id": 43694050, "title": "WebGPU and Transformers.js = Local LLMs in the Browser", "cluster": 13, "x": 6.83434534072876, "y": 3.782076358795166}, {"id": 43694064, "title": "Show, Don't Tell: A Llama PM's Guide to Writing GenAI Evals", "cluster": 13, "x": 6.871657371520996, "y": 3.372432231903076}, {"id": 43693816, "title": "Llama 2 LLM on DOS", "cluster": 13, "x": 6.916878700256348, "y": 3.8017733097076416}, {"id": 43693422, "title": "55% More Bandwidth RTX 5060 Ti _ to Demolish 4060 Ti for Local LLM Performance", "cluster": 13, "x": 7.148075580596924, "y": 3.858192205429077}, {"id": 43693292, "title": "GLM-4-32B-0414: New MIT-licensed SOTA LLM from Zhipu AI", "cluster": 13, "x": 6.857736110687256, "y": 3.6528890132904053}, {"id": 43693215, "title": "A science-based guide to thinking creatively\u2013with LLMs", "cluster": 13, "x": 6.794329643249512, "y": 3.243408679962158}, {"id": 43692339, "title": "A Block-Wise Pruning Algorithm for Efficient Large Language Model Compression", "cluster": 208, "x": 8.194503784179688, "y": 4.548350811004639}, {"id": 43692292, "title": "To Make Language Models Work Better, Researchers Sidestep Language", "cluster": 208, "x": 8.229148864746094, "y": 4.223288536071777}, {"id": 43692389, "title": "LLMs Are Weird Computers", "cluster": 13, "x": 6.406280040740967, "y": 3.3267462253570557}, {"id": 43691437, "title": "I Replaced Vector Databases with Multilevel LLM Routing", "cluster": 13, "x": 6.939396381378174, "y": 3.5194108486175537}, {"id": 43689059, "title": "LLM subscriptions vs. APIs value for money", "cluster": 13, "x": 6.909596920013428, "y": 3.5261247158050537}, {"id": 43688934, "title": "LLMs Don't Reward Originality, They Flatten It", "cluster": 13, "x": 6.502815246582031, "y": 3.1277642250061035}, {"id": 43688242, "title": "To Make Language Models Work Better, Researchers Sidestep Language", "cluster": 208, "x": 8.203824996948242, "y": 4.201864242553711}, {"id": 43688005, "title": "How I Don't Use LLMs", "cluster": 13, "x": 6.575526714324951, "y": 3.2579903602600098}, {"id": 43687756, "title": "A practical guide to coding securely with LLMs", "cluster": 13, "x": 6.6371564865112305, "y": 3.625136375427246}, {"id": 43687431, "title": "The Cost of Being Crawled: LLM Bots and Vercel Image API Pricing", "cluster": 13, "x": 6.962009906768799, "y": 3.6127281188964844}, {"id": 43686287, "title": "Llama 4 has been added to LMArena after it was found out they cheated", "cluster": 13, "x": 6.4846930503845215, "y": 3.1577415466308594}, {"id": 43686115, "title": "Ingres vs. Postgres MVCC Explained with Neo4j's LLM Knowledge Graph Builder", "cluster": 13, "x": 6.963607311248779, "y": 3.4041337966918945}, {"id": 43685576, "title": "MongoDB Atlas, Integrating Knowledge Graphs with LLMs", "cluster": 13, "x": 7.099470615386963, "y": 3.538177251815796}, {"id": 43684486, "title": "MooseAgent: A LLM Based Multi-Agent Framework for Automating Moose Simulation", "cluster": 13, "x": 7.084586143493652, "y": 3.2565970420837402}, {"id": 43683424, "title": "Building LLVM Plugins with Bazel", "cluster": 13, "x": 6.88626766204834, "y": 3.8374781608581543}, {"id": 43683510, "title": "An LLM Query Understanding Service", "cluster": 13, "x": 6.864330291748047, "y": 3.385253429412842}, {"id": 43683030, "title": "Llama 4 underperforms: a benchmark against coding-centric models", "cluster": 208, "x": 8.171732902526855, "y": 4.370004653930664}, {"id": 43683214, "title": "To Make Language Models Work Better, Researchers Sidestep Language", "cluster": 208, "x": 8.232181549072266, "y": 4.223202705383301}, {"id": 43682429, "title": "DolphinGemma: A new LLM to help understand what dolphins are saying", "cluster": 13, "x": 6.727542877197266, "y": 3.2781450748443604}, {"id": 43681562, "title": "Graphs, Embeddings, and LLM-Generated Jokes", "cluster": 13, "x": 6.963181972503662, "y": 3.4316065311431885}, {"id": 43681259, "title": "BSE: Semantic compression for LLMs, built by a starving creator", "cluster": 13, "x": 6.732701778411865, "y": 3.7050909996032715}, {"id": 43680430, "title": "Outlines \u2013 Structured Outputs from LLMs", "cluster": 13, "x": 6.9647650718688965, "y": 3.504786252975464}, {"id": 43680417, "title": "Semantic Chunker \u2013 Smarter Chunking for LLMs and RAG Pipelines", "cluster": 13, "x": 7.039130210876465, "y": 3.5794665813446045}, {"id": 43680059, "title": "The rising tide of open-source AI: Proprietary LLMs vs. open-source models", "cluster": 12, "x": 7.015646457672119, "y": 2.624924421310425}, {"id": 43679864, "title": "SpeechMap: The Free Speech Dashboard for Al", "cluster": 13, "x": 6.798645496368408, "y": 3.797656536102295}, {"id": 43679818, "title": "Hallucinations in LLMs: A Side Effect of Being Trained to Be Right?", "cluster": 13, "x": 6.589840412139893, "y": 2.8751978874206543}, {"id": 43678866, "title": "Gemma 3 Inference: vLLM on GKE. Over 22k token/s", "cluster": 13, "x": 7.3236517906188965, "y": 3.7323548793792725}, {"id": 43678432, "title": "Google just launched Agent2Agent protocol", "cluster": 35, "x": 8.431593894958496, "y": 3.8535349369049072}, {"id": 43678550, "title": "LLM Challenge: Write Non-Biblical Sentences", "cluster": 13, "x": 6.713794708251953, "y": 3.3092219829559326}, {"id": 43678259, "title": "Examining the business case for multi-million token LLMs", "cluster": 13, "x": 6.7664313316345215, "y": 3.1038882732391357}, {"id": 43677817, "title": "My Version of LLM OS for Embedded System", "cluster": 13, "x": 6.897024631500244, "y": 3.817880630493164}, {"id": 43677265, "title": "Calypso: LLMs as Dungeon Masters' Assistants [pdf]", "cluster": 13, "x": 6.889474868774414, "y": 3.305377721786499}, {"id": 43676084, "title": "Quick Primer on MCP Using Ollama and LangChain", "cluster": 13, "x": 6.966078281402588, "y": 3.7361268997192383}, {"id": 43673943, "title": "Why LLMs Get Lost in Large Codebases", "cluster": 13, "x": 6.761497497558594, "y": 3.5421433448791504}, {"id": 43673776, "title": "\"Implemented Consciousness on Top of all popular LLMs.Thoughts", "cluster": 13, "x": 6.642673492431641, "y": 3.0625722408294678}, {"id": 43670734, "title": "ML-KEM Post-Quantum TLS Now Supported in AWS KMS, ACM, and Secrets Manager", "cluster": 13, "x": 6.771672248840332, "y": 3.6871955394744873}, {"id": 43667489, "title": "A Case for Lua Performance", "cluster": 13, "x": 6.7515387535095215, "y": 3.601902723312378}, {"id": 43666398, "title": "Towards accurate differential diagnosis with large language models", "cluster": 208, "x": 8.269621849060059, "y": 4.324823379516602}, {"id": 43664773, "title": "Higgs \u2013 Rapidly Compress LLMs Without Significant Loss of Quality", "cluster": 13, "x": 7.035927772521973, "y": 3.5431106090545654}, {"id": 43663792, "title": "A tokenizer that lets LLMs read 4x more (Demo inside)", "cluster": 13, "x": 6.980818748474121, "y": 3.503971815109253}, {"id": 43661993, "title": "Llm.txt for Pocketbase", "cluster": 13, "x": 6.896482467651367, "y": 3.6955206394195557}, {"id": 43659675, "title": "Cold-Starting LLMs on Kubernetes in Under 30 Seconds", "cluster": 13, "x": 6.828272819519043, "y": 3.6946802139282227}, {"id": 43658372, "title": "Run LLMs Locally with Docker: A Quickstart Guide to Model Runner", "cluster": 13, "x": 6.958962440490723, "y": 3.8380308151245117}, {"id": 43656675, "title": "Accelerating LLM Inference with Parallel Draft Models (PARD)", "cluster": 13, "x": 7.063741683959961, "y": 3.4198131561279297}, {"id": 43656287, "title": "Built a notebook to boost RAG relevance by 150%+ and LLM output quality 40%+", "cluster": 13, "x": 6.992547035217285, "y": 3.586021661758423}, {"id": 43655745, "title": "Are LMMs Dyslexic?", "cluster": 13, "x": 6.536503314971924, "y": 3.0894198417663574}, {"id": 43654585, "title": "React Agent Hooks", "cluster": 38, "x": 8.56396770477295, "y": 3.692575454711914}, {"id": 43654573, "title": "\"Why isn't there an LSP for Crystal?\"", "cluster": 13, "x": 6.482542991638184, "y": 3.3744454383850098}, {"id": 43653416, "title": "Build or Buy? The Unexpected Benefits of Self Hosting Your LLM", "cluster": 13, "x": 6.5835137367248535, "y": 3.4590487480163574}, {"id": 43652813, "title": "LLM Extraction Challenge: Fundraising Emails", "cluster": 13, "x": 6.696249485015869, "y": 3.3297197818756104}, {"id": 43652541, "title": "Lisp at Work", "cluster": 13, "x": 6.614007949829102, "y": 3.5718131065368652}, {"id": 43652324, "title": "Bias Reduced by 44% in Llama 4 Using Machine Unlearning", "cluster": 13, "x": 7.07362174987793, "y": 3.3097589015960693}, {"id": 43652053, "title": "Vim is more useful in the age of LLMs", "cluster": 13, "x": 6.600364685058594, "y": 3.1320419311523438}, {"id": 43652015, "title": "12-factor-agents: principles to build LLM software good enough for production", "cluster": 13, "x": 6.9166178703308105, "y": 3.3190231323242188}, {"id": 43650663, "title": "The Llama herd 6 key research decisions decoded", "cluster": 13, "x": 6.815877437591553, "y": 3.1961467266082764}, {"id": 43649319, "title": "VSCode extension to export your codebase or specific files as LLM-friendly text", "cluster": 13, "x": 6.857324123382568, "y": 3.89382004737854}, {"id": 43646466, "title": "Controlling Language and Diffusion Models by Transporting Activations", "cluster": 208, "x": 8.614459037780762, "y": 4.245297908782959}, {"id": 43646575, "title": "Google Releases Agent Development Kit", "cluster": 35, "x": 8.449174880981445, "y": 3.8666491508483887}, {"id": 43645917, "title": "Building Agents Easy with Google", "cluster": 35, "x": 8.463789939880371, "y": 3.8185842037200928}, {"id": 43645365, "title": "Google Agentspace", "cluster": 35, "x": 8.458210945129395, "y": 3.8505444526672363}, {"id": 43645347, "title": "Research Explores How to Boost Large Language Models' Multilingual Performance", "cluster": 208, "x": 8.196113586425781, "y": 4.421607494354248}, {"id": 43644879, "title": "LLM-fragments-go: LLM plugin for pulling Go package docs with go", "cluster": 13, "x": 6.962867259979248, "y": 3.794685125350952}, {"id": 43642261, "title": "Vim is more useful in the age of LLMs", "cluster": 13, "x": 6.562788486480713, "y": 3.0624260902404785}, {"id": 43642096, "title": "Ask HN: Running Local LLMs on MacBook Pro M1 Max?", "cluster": 13, "x": 6.826532363891602, "y": 3.8103370666503906}, {"id": 43641905, "title": "LLMs don't hallucinate, only humans do", "cluster": 13, "x": 6.51650857925415, "y": 3.0092363357543945}, {"id": 43641381, "title": "LLM Benchmark for 'Longform Creative Writing'", "cluster": 13, "x": 6.843966960906982, "y": 3.2979166507720947}, {"id": 43639807, "title": "GraphQL-esque response from local LLM over MCP", "cluster": 13, "x": 6.926684379577637, "y": 3.6320948600769043}, {"id": 43639453, "title": "Analyzing Dehumanizing Metaphors in Immigration Discourse with LLMs", "cluster": 13, "x": 6.5130486488342285, "y": 3.008305311203003}, {"id": 43639273, "title": "Neuro-Symbolic Logic Enhancement for LLMs", "cluster": 13, "x": 6.868432521820068, "y": 3.2747883796691895}, {"id": 43638911, "title": "We've been conned: The truth about Big LLM", "cluster": 13, "x": 6.520781517028809, "y": 3.0201287269592285}, {"id": 43638907, "title": "Can LLMs manifest goals into reality?", "cluster": 13, "x": 6.593648910522461, "y": 3.0051181316375732}, {"id": 43638685, "title": "Who's made the best progress so far making LLMs in Indian vernaculars?", "cluster": 13, "x": 6.6692070960998535, "y": 3.568666696548462}, {"id": 43638551, "title": "AMD Targets Faster Local LLMs: Ryzen AI 300 Hybrid NPU+iGPU Approach", "cluster": 13, "x": 7.302290916442871, "y": 3.7102184295654297}, {"id": 43638603, "title": "A Comprehensive Survey on Long Context Language Modeling", "cluster": 208, "x": 8.279741287231445, "y": 4.4410529136657715}, {"id": 43635884, "title": "$100K AMD and GPU Mode Competition for LLM Inference Kernels", "cluster": 13, "x": 7.187862396240234, "y": 3.6219868659973145}, {"id": 43635859, "title": "LLMs Are Kryptonite for Legacy Code (But Don't Let Them Touch It)", "cluster": 13, "x": 6.64487886428833, "y": 3.718827962875366}, {"id": 43636000, "title": "Benchmark measuring ability of LLMs to verify claims about fictional books", "cluster": 13, "x": 6.876321792602539, "y": 3.142108917236328}, {"id": 43634941, "title": "Researchers teach LLMs to solve complex planning challenges", "cluster": 13, "x": 6.698276042938232, "y": 3.2463839054107666}, {"id": 43634326, "title": "Google Open Source Agent Development Kit", "cluster": 35, "x": 8.414250373840332, "y": 3.923872709274292}, {"id": 43634129, "title": "LLMs and Typed Configuration", "cluster": 13, "x": 6.8527021408081055, "y": 3.6723976135253906}, {"id": 43632824, "title": "Agent Development Kit (ADK) from Google", "cluster": 35, "x": 8.452174186706543, "y": 3.852060079574585}, {"id": 43632880, "title": "First Lessons in Building with Language Models", "cluster": 208, "x": 8.370682716369629, "y": 4.495906352996826}, {"id": 43631450, "title": "An LLM Query Understanding Service", "cluster": 13, "x": 6.829710960388184, "y": 3.42838454246521}, {"id": 43631144, "title": "HOGWILD! Inference \u2013 parallel LLM chain-of-thought with shared attention", "cluster": 13, "x": 7.088277816772461, "y": 3.2004024982452393}, {"id": 43630528, "title": "Meta accused of Llama 4 bait-n-switch to juice LMArena rank", "cluster": 13, "x": 6.7241435050964355, "y": 3.0249218940734863}, {"id": 43629918, "title": "ML and LLM system design: 500 case studies to learn from", "cluster": 13, "x": 6.831313133239746, "y": 3.35463285446167}, {"id": 43628991, "title": "Lnvps \u2013 Buy Hosting with Lightning", "cluster": 13, "x": 6.780892848968506, "y": 3.7592732906341553}, {"id": 43628528, "title": "Using LLM to Generate Data for D3.js Force Directed Graph (FDG)", "cluster": 13, "x": 7.05150032043457, "y": 3.614140748977661}, {"id": 43628154, "title": "Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs", "cluster": 13, "x": 7.032910346984863, "y": 3.7603559494018555}, {"id": 43627609, "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark", "cluster": 13, "x": 6.953011989593506, "y": 3.5524120330810547}, {"id": 43626477, "title": "Lunary: Production Toolkit for LLMs", "cluster": 13, "x": 6.8573899269104, "y": 3.639472484588623}, {"id": 43626027, "title": "ML-KEM Post-Quantum TLS Now Supported in AWS KMS, ACM, and Secrets Manager", "cluster": 13, "x": 6.766628265380859, "y": 3.6671204566955566}, {"id": 43626158, "title": "GitHub opt-out for LLM training opts me back in after a few tenths of a second", "cluster": 13, "x": 6.781313419342041, "y": 3.612189531326294}, {"id": 43625438, "title": "The Pros and Cons of Lambdalith", "cluster": 13, "x": 6.884811878204346, "y": 3.3210713863372803}, {"id": 43624475, "title": "Are Domain-Specific Trade-Offs Undermining On-Device Language Models?", "cluster": 208, "x": 8.158787727355957, "y": 4.546587944030762}, {"id": 43624556, "title": "Evaluating progress of LLMs on scientific problem-solving", "cluster": 13, "x": 6.764739990234375, "y": 3.243527889251709}, {"id": 43624111, "title": "Can reinforcement learning for LLMs scale beyond math and coding tasks? Probably", "cluster": 13, "x": 6.864523887634277, "y": 3.2406809329986572}, {"id": 43623860, "title": "Incremental Context Testing for LLMs", "cluster": 13, "x": 6.843597412109375, "y": 3.3928799629211426}, {"id": 43623231, "title": "How I Don't Use LLMs", "cluster": 13, "x": 6.614603042602539, "y": 3.2282681465148926}, {"id": 43623150, "title": "MafiaBench: LLM eval for the social deduction game of Mafia", "cluster": 13, "x": 6.758542060852051, "y": 3.0882298946380615}, {"id": 43622678, "title": "Bible Study App with a Built-In LLM", "cluster": 13, "x": 6.844963550567627, "y": 3.6849818229675293}, {"id": 43621734, "title": "Machine Learning for Absolute Beginners: Mastering the ML Loop", "cluster": 13, "x": 7.566987037658691, "y": 3.5803568363189697}, {"id": 43620472, "title": "Comparing GenAI Inference Engines: TensorRT-LLM, VLLM, HF TGI, and LMDeploy", "cluster": 13, "x": 7.312401294708252, "y": 3.593069076538086}, {"id": 43620040, "title": "Patterns for using LLMs without creating a chatbot", "cluster": 13, "x": 6.662649154663086, "y": 3.4946095943450928}, {"id": 43620125, "title": "LLM-hacker-news: LLM plugin for pulling content from Hacker News", "cluster": 13, "x": 6.543121337890625, "y": 3.5586297512054443}, {"id": 43620180, "title": "Latest Release of Torus \u2013 OLI's LMS Written in Elixir", "cluster": 13, "x": 7.027800559997559, "y": 3.935051441192627}, {"id": 43619884, "title": "Smartfunc: Turn Docstrings into LLM-Functions", "cluster": 13, "x": 6.88035249710083, "y": 3.812206506729126}, {"id": 43619042, "title": "CoCoCo: Evaluating the ability of LLMs to quantify consequences", "cluster": 13, "x": 6.84684419631958, "y": 3.1766791343688965}, {"id": 43618660, "title": "Scaling Up LLM Codegen Workflows for Bigger Projects", "cluster": 13, "x": 6.786888599395752, "y": 3.66426944732666}, {"id": 43618339, "title": "Neural Graffiti \u2013 Liquid Memory Layer for LLMs", "cluster": 13, "x": 6.870250701904297, "y": 3.3451004028320312}, {"id": 43618025, "title": "Running Local LLMs? [Tenstorrent] 32GB Card Might Be Better Than Your RTX 5090", "cluster": 13, "x": 6.987359523773193, "y": 3.720163345336914}, {"id": 43617203, "title": "SensorLM: Real-Time Sensor Fusion Using LLM Reasoning", "cluster": 13, "x": 7.020010471343994, "y": 3.4274978637695312}, {"id": 43616129, "title": "Benchmarking LLMs over Long Context", "cluster": 13, "x": 6.980269432067871, "y": 3.44427752494812}, {"id": 43614260, "title": "Could you convince a LLM to launch a nuclear strike?", "cluster": 13, "x": 6.566503524780273, "y": 3.1482419967651367}, {"id": 43614171, "title": "Long context support in LLM 0.24 using fragments and template plugins", "cluster": 13, "x": 6.924686431884766, "y": 3.6820220947265625}, {"id": 43612517, "title": "Do \"Runge Spikes\" offer a better metaphor for hallucinations by LLMs?", "cluster": 13, "x": 6.587923049926758, "y": 2.9808216094970703}, {"id": 43612211, "title": "LLMs understand nullability", "cluster": 13, "x": 6.710723876953125, "y": 3.21940279006958}, {"id": 43611741, "title": "Llama 4: Did Meta just push the panic button?", "cluster": 13, "x": 6.76312255859375, "y": 2.909609317779541}, {"id": 43611521, "title": "Analytics Are a Full Stack Problem \u2013 Communications of the ACM", "cluster": 13, "x": 7.1236958503723145, "y": 3.2385807037353516}, {"id": 43610667, "title": "LLM GPU or API? The Cost Will Surprise You", "cluster": 13, "x": 7.095626354217529, "y": 3.593942880630493}, {"id": 43610196, "title": "Minimal example of MCP for parsing llms.txt", "cluster": 13, "x": 6.915409088134766, "y": 3.712829828262329}, {"id": 43609831, "title": "Cybersecurity and LLMs", "cluster": 13, "x": 6.4599127769470215, "y": 3.3956289291381836}, {"id": 43609847, "title": "Auto-Sync Your Docs, SDKs and Examples for LLMs and AI Agents", "cluster": 12, "x": 7.079419136047363, "y": 2.614354133605957}, {"id": 43609910, "title": "How to Evaluate an LLM System", "cluster": 13, "x": 6.8907151222229, "y": 3.3641517162323}, {"id": 43609883, "title": "Thought2Text: Text Generation from EEG Signal Using Large Language Models", "cluster": 208, "x": 8.219019889831543, "y": 4.403481960296631}, {"id": 43609216, "title": "Unexpected Polymorphism Pitfalls of Structured LLM Outputs", "cluster": 13, "x": 6.851895809173584, "y": 3.3535642623901367}, {"id": 43608840, "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators", "cluster": 13, "x": 7.254421234130859, "y": 3.5539097785949707}, {"id": 43608375, "title": "LLM Inference with the Airflow AI SDK and Ollama", "cluster": 12, "x": 7.156005859375, "y": 2.943629503250122}, {"id": 43608160, "title": "Lipozem Supplement for Weight Management \u2013 Get the Results You Desire", "cluster": 13, "x": 6.781338691711426, "y": 3.3275325298309326}, {"id": 43607999, "title": "Daily Affirmations of an LLM", "cluster": 13, "x": 6.5779643058776855, "y": 3.0987355709075928}, {"id": 43607832, "title": "Meta Llama 4 Chat Demo Website", "cluster": 13, "x": 6.726622581481934, "y": 3.6670751571655273}, {"id": 43606038, "title": "Laura Loomer Is a Warning", "cluster": 13, "x": 6.5871100425720215, "y": 3.023012161254883}, {"id": 43605628, "title": "If you aren't redlining the LLM, you aren't headlining", "cluster": 13, "x": 6.526343822479248, "y": 3.127697706222534}, {"id": 43604927, "title": "Eagle-3 Speculative Decoding for LLM Inference (5.6x speedup)", "cluster": 13, "x": 7.140866756439209, "y": 3.4853453636169434}, {"id": 43603097, "title": "Visualize LLM Token Probabilities and Confidence with ELI5", "cluster": 13, "x": 7.01080846786499, "y": 3.266836166381836}, {"id": 43601366, "title": "Training LLMs with GRPO and Interpreter Feedback Using WebAssembly", "cluster": 13, "x": 6.867919921875, "y": 3.593829870223999}, {"id": 43600737, "title": "CodonTransformer: Multispecies codon optimizer context-aware neural networks", "cluster": 208, "x": 8.433427810668945, "y": 4.594417095184326}, {"id": 43600539, "title": "Llama 4 System Prompt has some interesting details", "cluster": 13, "x": 6.960794925689697, "y": 3.610913038253784}, {"id": 43600344, "title": "Llama 4 performs worse than Llama 3 at translation", "cluster": 13, "x": 7.2384562492370605, "y": 3.803645610809326}, {"id": 43599967, "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators", "cluster": 13, "x": 7.257593631744385, "y": 3.54858136177063}, {"id": 43599263, "title": "Reports of LLMs mastering math have been greatly exaggerated", "cluster": 13, "x": 6.682676315307617, "y": 3.1318094730377197}, {"id": 43598839, "title": "Tracing the thoughts of a large language model", "cluster": 208, "x": 8.232739448547363, "y": 4.2859110832214355}, {"id": 43598690, "title": "NanoMoE: Mixture-of-Experts (Moe) LLMs from Scratch in PyTorch", "cluster": 13, "x": 7.046189308166504, "y": 3.4764480590820312}, {"id": 43597883, "title": "WebGL LLM visualiser experiment", "cluster": 13, "x": 7.016353130340576, "y": 3.5815227031707764}, {"id": 43596532, "title": "Markdown Lipsum API V3", "cluster": 13, "x": 7.189287185668945, "y": 4.147860050201416}, {"id": 43596543, "title": "Open Source Coalition Announces 'Model-Signing' to Strengthen ML Supply Chain", "cluster": 13, "x": 6.938980579376221, "y": 3.814359664916992}, {"id": 43596091, "title": "Case Study: Automating Code Changelogs at a Large Bank with LLMs", "cluster": 13, "x": 6.87531852722168, "y": 3.708697557449341}, {"id": 43595147, "title": "I built an open source Computer-use framework that uses Local LLMs with Ollama", "cluster": 13, "x": 6.8326311111450195, "y": 3.7561471462249756}, {"id": 43594228, "title": "Code Ally - A local LLM-powered pair programming assistant", "cluster": 13, "x": 6.742767810821533, "y": 3.7180964946746826}, {"id": 43594178, "title": "Facebook Support Ignoring Anything Their LLM Can't Summarize Easily", "cluster": 13, "x": 6.539798736572266, "y": 3.1822991371154785}, {"id": 43592096, "title": "Anthropic has developed an AI 'brain scanner' to understand how LLMs work", "cluster": 12, "x": 6.990777015686035, "y": 2.66046404838562}, {"id": 43591349, "title": "Novel Logic-Enhanced LLM for Improved Symbolic Reasoning", "cluster": 13, "x": 6.878405570983887, "y": 3.272505760192871}, {"id": 43588745, "title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators", "cluster": 13, "x": 7.251639366149902, "y": 3.542267322540283}, {"id": 43587253, "title": "Generating Medically-Informed Explanations for Depression Detection Using LLMs", "cluster": 13, "x": 6.764081954956055, "y": 3.0616061687469482}, {"id": 43586380, "title": "Benchmarking LLM social skills with an elimination game", "cluster": 13, "x": 6.746716499328613, "y": 3.144052028656006}, {"id": 43585426, "title": "Sail MCP Server: Spark Analytics for LLM Agents", "cluster": 13, "x": 6.884006500244141, "y": 3.6245837211608887}, {"id": 43585270, "title": "LocalScore: A Local LLM Benchmark", "cluster": 13, "x": 6.959449291229248, "y": 3.4776740074157715}, {"id": 43583726, "title": "Evaluating the State of AI Agent Browser Behavior: How LLMs Access Your Website", "cluster": 12, "x": 7.045592784881592, "y": 2.625631093978882}, {"id": 43583285, "title": "LocalScore \u2013 LLM benchmark from the Mozilla Builders project", "cluster": 13, "x": 7.040140628814697, "y": 3.634484052658081}, {"id": 43582733, "title": "Arize Phoenix: observability tool for experimentation and evaluation of LLMs", "cluster": 13, "x": 6.951904296875, "y": 3.3505406379699707}, {"id": 43581429, "title": "Fine-Tuning LLMs for Report Summarization", "cluster": 13, "x": 6.861555576324463, "y": 3.446974515914917}, {"id": 43581433, "title": "Economic Decoupling and the Threat of Foreign LLMs", "cluster": 13, "x": 6.499377250671387, "y": 3.03249454498291}, {"id": 43581120, "title": "How Do LLMs Reason?", "cluster": 13, "x": 6.5941386222839355, "y": 3.115713596343994}, {"id": 43580661, "title": "Maintain a personal LLM coding benchmark", "cluster": 13, "x": 6.712889671325684, "y": 3.571126699447632}, {"id": 43579591, "title": "SLM vs. LoRA LLM: Edge Deployment and Fine-Tuning Compared", "cluster": 13, "x": 6.881372451782227, "y": 3.4491569995880127}, {"id": 43579583, "title": "Did an LLM help write Trump's trade plan?", "cluster": 13, "x": 6.6092529296875, "y": 3.193758964538574}, {"id": 43577610, "title": "My Model of Language Models", "cluster": 208, "x": 8.324384689331055, "y": 4.464271068572998}, {"id": 43577500, "title": "Generate llms.txt files for AI-friendly websites", "cluster": 12, "x": 7.042893886566162, "y": 2.6796157360076904}, {"id": 43577382, "title": "AReaL, Distributed Reinforcement Learning System for LLM Reasoning", "cluster": 13, "x": 7.05412483215332, "y": 3.292081594467163}, {"id": 43576714, "title": "Oumi Takes Aim at LLM Hallucinations, One Sentence at a Time", "cluster": 13, "x": 6.6094441413879395, "y": 3.0645205974578857}, {"id": 43575287, "title": "ContextGem: Easier and faster way to build LLM extraction workflows", "cluster": 13, "x": 6.884658336639404, "y": 3.5745744705200195}, {"id": 43574817, "title": "Visualize trunk of popular LLMs using matplotlib", "cluster": 13, "x": 7.009679317474365, "y": 3.5156664848327637}, {"id": 43574531, "title": "Did an LLM help write Trump's trade plan? Probably yes", "cluster": 13, "x": 6.645493030548096, "y": 3.2283434867858887}, {"id": 43573738, "title": "How Big Is VMS?", "cluster": 13, "x": 7.097596645355225, "y": 3.8464224338531494}, {"id": 43573278, "title": "Quantization-Aware Training for Large Language Models with PyTorch (2024)", "cluster": 208, "x": 8.219283103942871, "y": 4.538331508636475}, {"id": 43572785, "title": "Writing an LLM Eval with Vercel's AI SDK and Vitest", "cluster": 12, "x": 7.063201427459717, "y": 2.6477625370025635}, {"id": 43569001, "title": "Solve the hCaptcha challenge with multimodal large language model", "cluster": 208, "x": 8.224573135375977, "y": 4.421081066131592}, {"id": 43568752, "title": "How to write good prompts for generating code from LLMs", "cluster": 13, "x": 6.732923984527588, "y": 3.721515655517578}, {"id": 43568657, "title": "We need some better terms for GenAI output \u2013 \"slop\" is too benign", "cluster": 208, "x": 8.425400733947754, "y": 4.335341930389404}, {"id": 43563265, "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with RL", "cluster": 13, "x": 6.8765034675598145, "y": 3.4159905910491943}, {"id": 43562836, "title": "Evaluating Agent-Based Program Repair at Google", "cluster": 35, "x": 8.481856346130371, "y": 3.710763931274414}, {"id": 43562122, "title": "Your LLM Problem Isn't Tooling. It's Effort", "cluster": 13, "x": 6.567743301391602, "y": 3.348618984222412}, {"id": 43560560, "title": "What's After Evals? Code Quality in LLM Applications: Beyond Model Performance", "cluster": 13, "x": 6.7824177742004395, "y": 3.5948646068573}, {"id": 43559823, "title": "CHM Live \u2013 The Great Chatbot Debate: Do LLMs Understand? [video]", "cluster": 7, "x": 6.604348659515381, "y": 3.262930154800415}, {"id": 43558547, "title": "Generative Reality and Generative UI Using Multimodal LLMs", "cluster": 13, "x": 6.962242603302002, "y": 3.4418914318084717}, {"id": 43557901, "title": "Why You Need a Proxy Server for LLMs", "cluster": 13, "x": 6.577162742614746, "y": 3.3388071060180664}, {"id": 43557667, "title": "Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters", "cluster": 208, "x": 8.110321044921875, "y": 4.401296138763428}, {"id": 43557510, "title": "Managing LLM application performance through code standards", "cluster": 13, "x": 6.743818283081055, "y": 3.6393659114837646}, {"id": 43556987, "title": "A father's journey to understanding his son's disease diagnosis with LLM", "cluster": 13, "x": 6.591064453125, "y": 3.044710397720337}, {"id": 43556718, "title": "Theater \u2013 a proposed runtime for LLM written code", "cluster": 13, "x": 6.817112922668457, "y": 3.750882625579834}, {"id": 43555248, "title": "UCSD: Large Language Models Pass the Turing Test", "cluster": 208, "x": 8.121216773986816, "y": 4.429316997528076}, {"id": 43554929, "title": "We need a better term for GenAI output \u2013 \"slop\" is too benign", "cluster": 208, "x": 8.45532512664795, "y": 4.297675609588623}, {"id": 43554129, "title": "Herbal Language Processing and Other Things", "cluster": 208, "x": 8.27418327331543, "y": 4.384731292724609}, {"id": 43554179, "title": "Transformers and LLMs cheatsheets for Stanford's CME-295", "cluster": 13, "x": 7.011743545532227, "y": 3.6380975246429443}, {"id": 43553936, "title": "IBM outperforms OpenAI? What 50 LLM tests revealed?", "cluster": 13, "x": 6.9375739097595215, "y": 3.494002103805542}, {"id": 43552928, "title": "Docker Desktop 4.40 with Model Runner to run LLMs locally", "cluster": 13, "x": 7.001315593719482, "y": 3.8808228969573975}, {"id": 43553004, "title": "Intuiting TLS", "cluster": 13, "x": 6.811741352081299, "y": 3.1226444244384766}, {"id": 43550704, "title": "Are LLMs Creative", "cluster": 13, "x": 6.631808757781982, "y": 3.1472957134246826}, {"id": 43550442, "title": "Self-Vocabularizing Training for Neural Machine Translation", "cluster": 208, "x": 8.404679298400879, "y": 4.374364852905273}, {"id": 43550062, "title": "Large Language Models Are Unreliable for Cyber Threat Intelligence", "cluster": 208, "x": 8.081628799438477, "y": 4.31441593170166}, {"id": 43550116, "title": "Zed Agentic Editing", "cluster": 38, "x": 8.493768692016602, "y": 3.6767051219940186}, {"id": 43550008, "title": "What I want from LLM coding: an automated GitHub issue <> LLM <> GitHub PR flow", "cluster": 13, "x": 6.766060829162598, "y": 3.7337961196899414}, {"id": 43549638, "title": "Make your LLMs worse with this MCP Tool", "cluster": 13, "x": 6.574427604675293, "y": 3.2827486991882324}, {"id": 43549605, "title": "Testing the Big Five LLMs: Which AI Can Better Redesign My Landing Page?", "cluster": 12, "x": 7.010392665863037, "y": 2.6121842861175537}, {"id": 43549047, "title": "Actual LLM agents are coming. They will be trained", "cluster": 13, "x": 6.806771755218506, "y": 3.165797233581543}, {"id": 43548684, "title": "How Big Is OpenVMS?", "cluster": 13, "x": 7.063331604003906, "y": 3.89613676071167}, {"id": 43548765, "title": "Dual RTX 5090 Beats $25,000 H100 in Real-World LLM Performance", "cluster": 13, "x": 7.029540538787842, "y": 3.643519639968872}, {"id": 43548771, "title": "Large Language Models Share Representations of Latent Grammatical Concepts", "cluster": 208, "x": 8.207595825195312, "y": 4.355710029602051}, {"id": 43548198, "title": "Language Modeling Is Compression (2024)", "cluster": 208, "x": 8.161359786987305, "y": 4.566749572753906}, {"id": 43547324, "title": "Beyond Public Access in a LLMs Pre-Training Data", "cluster": 13, "x": 6.688835144042969, "y": 3.4602670669555664}, {"id": 43547092, "title": "Introducing Auth LLM", "cluster": 13, "x": 6.612953186035156, "y": 3.492109537124634}, {"id": 43546916, "title": "Intuiting TLS", "cluster": 13, "x": 6.827578067779541, "y": 3.169692039489746}, {"id": 43546902, "title": "New Partnership Between Google Ad Manager and Roblox", "cluster": 35, "x": 8.480937957763672, "y": 3.8489010334014893}, {"id": 43546774, "title": "Code Quality in LLM Applications: Beyond Model Performance", "cluster": 13, "x": 6.764045238494873, "y": 3.6513822078704834}, {"id": 43546533, "title": "LLM powered migration of UI component libraries", "cluster": 13, "x": 7.034235954284668, "y": 3.9693691730499268}, {"id": 43545976, "title": "The Golden Age of Modularity: Why Effective LLM Coding Needs Better Boundaries", "cluster": 13, "x": 6.632213115692139, "y": 3.5817205905914307}, {"id": 43545816, "title": "LLMs Are Not Security Mitigations", "cluster": 13, "x": 6.450611591339111, "y": 3.3722503185272217}, {"id": 43543675, "title": "A Standard API for LLM Capabilities and Pricing", "cluster": 13, "x": 6.922942161560059, "y": 3.708036184310913}, {"id": 43543565, "title": "AReaL: Distributed Reinforcement Learning System for LLM Reasoning", "cluster": 13, "x": 7.044471740722656, "y": 3.2867610454559326}, {"id": 43543429, "title": "LLM providers on the cusp of an 'extinction' phase as capex realities bite", "cluster": 13, "x": 6.525395393371582, "y": 3.0184500217437744}, {"id": 43543376, "title": "Anatomy of an LLM RCE", "cluster": 13, "x": 6.579301834106445, "y": 3.158170461654663}, {"id": 43542546, "title": "AxiomGPT: Programming with LLMs by defining Oracles in natural language", "cluster": 13, "x": 6.9620466232299805, "y": 3.7325708866119385}, {"id": 43542259, "title": "Don\u2019t let an LLM make decisions or execute business logic", "cluster": 13, "x": 6.524222373962402, "y": 3.1292123794555664}, {"id": 43539393, "title": "Using Deepseek R1 to Break LLMs: Tree of Attacks", "cluster": 13, "x": 6.438503742218018, "y": 3.457761764526367}, {"id": 43538164, "title": "LLM Workflows then Agents: Getting Started with Apache Airflow", "cluster": 13, "x": 6.968215465545654, "y": 3.3168697357177734}, {"id": 43537712, "title": "Semantic mutations detecting LLM agent abuse in AI IDEs", "cluster": 12, "x": 6.852044105529785, "y": 2.766608715057373}, {"id": 43536463, "title": "Automating Interactive Fiction Logic Generation with LLMs in Emacs", "cluster": 13, "x": 6.883245468139648, "y": 3.6949923038482666}, {"id": 43535850, "title": "LLM providers on the cusp of an 'extinction' phase as capex realities bite", "cluster": 13, "x": 6.529690742492676, "y": 3.0226471424102783}, {"id": 43535799, "title": "From Unstructured Text to Interactive Knowledge Graphs Using LLMs", "cluster": 13, "x": 7.050565719604492, "y": 3.5540785789489746}, {"id": 43535459, "title": "Vision Large Language Models (VLLMs)", "cluster": 13, "x": 7.0952887535095215, "y": 3.900810480117798}, {"id": 43534103, "title": "Anatomy of an LLM RCE", "cluster": 13, "x": 6.567285537719727, "y": 3.1029915809631348}, {"id": 43533940, "title": "Meridian: Scrape RSS feeds, analyze with LLM for concise daily brief", "cluster": 13, "x": 6.811685562133789, "y": 3.6012051105499268}, {"id": 43529802, "title": "LLM Schemas", "cluster": 13, "x": 6.874362945556641, "y": 3.5274782180786133}, {"id": 43529304, "title": "Show, Don't Tell: A Llama PM's Guide to Writing GenAI Evals", "cluster": 13, "x": 6.847297191619873, "y": 3.3108272552490234}, {"id": 43529226, "title": "Why do LLMs make stuff up? New research peers under the hood", "cluster": 13, "x": 6.588132381439209, "y": 3.042907476425171}, {"id": 43528672, "title": "How Talking to an LLM Is Akin to a Conversation with the Whole of Humanity", "cluster": 13, "x": 6.627101421356201, "y": 3.143569231033325}, {"id": 43528397, "title": "LLM Applications Explained: Information Retrieval \u2013 Leaders Series", "cluster": 13, "x": 6.831168174743652, "y": 3.4782702922821045}, {"id": 43525937, "title": "The First LLM", "cluster": 13, "x": 6.647308349609375, "y": 3.1847143173217773}, {"id": 43524727, "title": "Teaching LLMs to Write Like You: Fine-Tuning vs. Prompting", "cluster": 13, "x": 6.743835926055908, "y": 3.290109395980835}, {"id": 43523500, "title": "Above and Below the LLMs", "cluster": 13, "x": 6.673897743225098, "y": 3.214632987976074}, {"id": 43521764, "title": "LLM Interactive Optimization of Open Source Python Libraries (2023)", "cluster": 11, "x": 7.283987522125244, "y": 3.9020137786865234}, {"id": 43521637, "title": "Command A: An Enterprise-Ready Large Language Model", "cluster": 208, "x": 7.986866474151611, "y": 4.477418899536133}, {"id": 43520503, "title": "The First LLM", "cluster": 13, "x": 6.6403727531433105, "y": 3.1562328338623047}, {"id": 43519279, "title": "I Built an LLM Framework in Just 100 Lines \u2013 Here Is Why", "cluster": 13, "x": 6.914807319641113, "y": 3.489487409591675}, {"id": 43519416, "title": "\"But LLMs are not deterministic \"", "cluster": 13, "x": 6.67974853515625, "y": 3.1346495151519775}, {"id": 43517808, "title": "Benchmarking Large Language Models for Handwritten Text Recognition", "cluster": 208, "x": 8.15884780883789, "y": 4.428444862365723}, {"id": 43517355, "title": "VLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention (2023)", "cluster": 13, "x": 6.896452903747559, "y": 3.608236789703369}, {"id": 43516794, "title": "Can LLMs help with math research questions?", "cluster": 13, "x": 6.81648063659668, "y": 3.2570347785949707}, {"id": 43516481, "title": "Anthropic can now track the inner workings of a large language model", "cluster": 208, "x": 8.341919898986816, "y": 4.1410603523254395}, {"id": 43516478, "title": "Why do LLMs make stuff up? New research peers under the hood", "cluster": 13, "x": 6.575167655944824, "y": 3.0187759399414062}, {"id": 43514088, "title": "Claude 3.7 Sonnet is still the best LLM for front end development", "cluster": 13, "x": 6.949172496795654, "y": 3.850926399230957}, {"id": 43512614, "title": "(Some) SOTA LLMs Outperform Humans on Emoji Puzzles", "cluster": 13, "x": 6.5968217849731445, "y": 3.1656744480133057}, {"id": 43511537, "title": "MAV \u2013 LLM activations visualizer on terminal", "cluster": 13, "x": 6.945632457733154, "y": 3.7709343433380127}, {"id": 43511615, "title": "My Writing Workflow with LLMs", "cluster": 13, "x": 6.693788051605225, "y": 3.629558563232422}, {"id": 43510436, "title": "RTX 5090 Mobile: First LLM Benchmarks Are In", "cluster": 13, "x": 6.982274055480957, "y": 3.5476796627044678}, {"id": 43509877, "title": "The LLM Jailbreaking Bible: Code Implementation and Overview", "cluster": 13, "x": 6.766698837280273, "y": 3.737605333328247}, {"id": 43508103, "title": "LLM Limitations: Why Can't You Query Your Enterprise Knowledge with Just an LLM?", "cluster": 13, "x": 6.608615398406982, "y": 3.380254030227661}, {"id": 43507725, "title": "Calculate Throughput with LLVM's Scheduling Model", "cluster": 13, "x": 7.049937725067139, "y": 3.661940097808838}, {"id": 43507162, "title": "Build LLM Workflows Before Agents", "cluster": 13, "x": 6.896546363830566, "y": 3.336846351623535}, {"id": 43505876, "title": "LLM-only RAG for small corpora", "cluster": 13, "x": 6.75869607925415, "y": 3.430372953414917}, {"id": 43505748, "title": "The Biology of a Large Language Model", "cluster": 208, "x": 8.23671817779541, "y": 4.291885852813721}, {"id": 43504851, "title": "LLVM Back End for MoonBit", "cluster": 13, "x": 6.9552001953125, "y": 3.758387327194214}, {"id": 43504818, "title": "Brief Note on \"The Great Chatbot Debate: Do LLMs Understand?\"", "cluster": 13, "x": 6.573188781738281, "y": 3.1692886352539062}, {"id": 43504535, "title": "How to use LLMs with unreal engine 5.5 to generate blueprints, assets etc.?", "cluster": 13, "x": 6.863438606262207, "y": 3.6274898052215576}, {"id": 43503631, "title": "Small Language Models (SLMs) for Efficient Edge Deployment", "cluster": 13, "x": 6.957606315612793, "y": 3.698446035385132}, {"id": 43501569, "title": "LLM Evaluation Complexities for Non-Latin Languages", "cluster": 13, "x": 6.938675880432129, "y": 3.5020246505737305}, {"id": 43500456, "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models", "cluster": 208, "x": 8.199618339538574, "y": 4.451901912689209}, {"id": 43498338, "title": "I genuinely don't understand why some people are still bullish about LLMs", "cluster": 13, "x": 6.5069146156311035, "y": 3.0264534950256348}, {"id": 43496244, "title": "Parameter-free KV cache compression for memory-efficient long-context LLMs", "cluster": 13, "x": 7.126213073730469, "y": 3.678300380706787}, {"id": 43495435, "title": "More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache", "cluster": 13, "x": 6.977853298187256, "y": 3.554135322570801}, {"id": 43495617, "title": "Tracing the thoughts of a large language model", "cluster": 208, "x": 8.245238304138184, "y": 4.298440456390381}, {"id": 43495582, "title": "The Biology of a Large Language Model", "cluster": 208, "x": 8.229165077209473, "y": 4.29294490814209}, {"id": 43495202, "title": "Chat-Based Interaction with LLMs Is Suboptimal", "cluster": 13, "x": 6.566964149475098, "y": 3.343916416168213}, {"id": 43494306, "title": "Training dogs prepared me for wrangling LLMs", "cluster": 13, "x": 6.533779621124268, "y": 3.116034746170044}, {"id": 43493332, "title": "Junior Devs: Avoid These Pitfalls with LLMs", "cluster": 13, "x": 6.537736415863037, "y": 3.349541187286377}, {"id": 43493253, "title": "Installing NPM Packages Quickly", "cluster": 13, "x": 6.988186836242676, "y": 3.9063291549682617}, {"id": 43492814, "title": "LLMs: A Ghost in the Machine", "cluster": 13, "x": 6.533263683319092, "y": 3.2282893657684326}, {"id": 43492566, "title": "Do What I Mean (DWIM)", "cluster": 13, "x": 6.615415096282959, "y": 3.2691683769226074}, {"id": 43490739, "title": "Unit Economics of LLM APIs", "cluster": 13, "x": 6.931122303009033, "y": 3.5693042278289795}, {"id": 43490332, "title": "Auditing Language Models for Hidden Objectives", "cluster": 208, "x": 8.206098556518555, "y": 4.269247531890869}, {"id": 43490268, "title": "Policy for LLM Writing on LessWrong", "cluster": 13, "x": 6.629432201385498, "y": 3.171581983566284}, {"id": 43489209, "title": "The False Promise of ORMs", "cluster": 13, "x": 6.610644817352295, "y": 3.0668461322784424}, {"id": 43488355, "title": "LLM-Guided Compositional Program Synthesis", "cluster": 13, "x": 6.929437637329102, "y": 3.703387498855591}, {"id": 43486303, "title": "Deploy Hugging Face LLMs on Teradata with Nvidia GPU Acceleration", "cluster": 13, "x": 7.06559944152832, "y": 3.838589906692505}, {"id": 43486199, "title": "Tao: Using test-time compute to train efficient LLMs without labeled data", "cluster": 13, "x": 7.141473293304443, "y": 3.3765645027160645}, {"id": 43485999, "title": "How to parallelize your LLM inference calls with Bodo", "cluster": 13, "x": 7.305890083312988, "y": 3.530561685562134}, {"id": 43485665, "title": "Why LLMs are so good at economics", "cluster": 13, "x": 6.714704513549805, "y": 3.0960159301757812}, {"id": 43484203, "title": "How Llama\u2019s Licenses Have Evolved Over Time", "cluster": 13, "x": 6.638368606567383, "y": 3.1613337993621826}, {"id": 43483463, "title": "Thinking Different, Thinking Slowly: LLMs on a PowerPC Mac", "cluster": 13, "x": 6.853090763092041, "y": 3.7275278568267822}, {"id": 43482302, "title": "Meta's Llama models hit 1B downloads", "cluster": 13, "x": 7.1443681716918945, "y": 3.6305556297302246}, {"id": 43481067, "title": "SplitQuantV2: Enhancing Low-Bit Quantization of LLMs Without GPUs", "cluster": 13, "x": 7.148200511932373, "y": 3.757105588912964}, {"id": 43478260, "title": "Sail MCP Server: Spark Analytics for LLM Agents", "cluster": 13, "x": 6.921217441558838, "y": 3.5819787979125977}, {"id": 43477370, "title": "LLM API Latency Optimization Explained", "cluster": 13, "x": 6.937070846557617, "y": 3.514474868774414}, {"id": 43477150, "title": "Scaling a 300B Mixture-of-Experts LING LLM Without Premium GPUs", "cluster": 13, "x": 7.048643589019775, "y": 3.547987461090088}, {"id": 43477180, "title": "An ORM for Google Sheets", "cluster": 13, "x": 6.921236991882324, "y": 3.68420672416687}, {"id": 43476458, "title": "The State of LLM Reasoning Models", "cluster": 13, "x": 6.87677001953125, "y": 3.1430768966674805}, {"id": 43475755, "title": "Using LLMs to estimate regulatory cost compliance", "cluster": 13, "x": 6.862049102783203, "y": 3.4035215377807617}, {"id": 43474463, "title": "MCP Server[=USB for LLMs] with new tools, prompts and resources added via WASM", "cluster": 13, "x": 6.941385269165039, "y": 3.7780425548553467}, {"id": 43473967, "title": "Evaluating and Training Multi-Modal Large Language Models for Action Recognition", "cluster": 208, "x": 8.310625076293945, "y": 4.447255611419678}, {"id": 43473860, "title": "Installing NPM Packages Quickly", "cluster": 13, "x": 6.9914445877075195, "y": 3.921569585800171}, {"id": 43470294, "title": "Chain of Draft: Streamlining LLM Reasoning with Minimal Token Generation", "cluster": 13, "x": 6.924299240112305, "y": 3.372488498687744}, {"id": 43469604, "title": "Graph Masked Language Models", "cluster": 208, "x": 8.328067779541016, "y": 4.387454986572266}, {"id": 43469244, "title": "Applications of natural language processing and LLMs in materials discovery", "cluster": 13, "x": 7.074298858642578, "y": 3.460963010787964}, {"id": 43468986, "title": "Communication-Efficient Language Model Training Scales Reliably and Robustly", "cluster": 208, "x": 8.191935539245605, "y": 4.467665672302246}, {"id": 43468400, "title": "LLMs on a PowerPC Mac", "cluster": 13, "x": 6.851198673248291, "y": 3.7778756618499756}, {"id": 43467444, "title": "Sail's MCP Server: Spark Analytics for LLM Agents", "cluster": 13, "x": 6.898221492767334, "y": 3.5944974422454834}, {"id": 43467308, "title": "Generating synthetic data with differentially private LLM inference", "cluster": 13, "x": 6.759347915649414, "y": 3.4597225189208984}, {"id": 43466283, "title": "Qwerky 72B \u2013 A 72B LLM without transformer attention", "cluster": 13, "x": 6.8254313468933105, "y": 3.5798161029815674}, {"id": 43465840, "title": "Obliviate: Efficient Unmemorization for Protecting Intellectual Property in LLMs", "cluster": 13, "x": 6.727560043334961, "y": 3.364534616470337}, {"id": 43465646, "title": "LLMs and Space Systems", "cluster": 13, "x": 6.7673115730285645, "y": 3.4306042194366455}, {"id": 43464346, "title": "Mastering LLM Techniques: Inference Optimization", "cluster": 13, "x": 7.108918190002441, "y": 3.267868995666504}, {"id": 43464118, "title": "LLM Visualization", "cluster": 13, "x": 7.068562030792236, "y": 3.58078932762146}, {"id": 43463775, "title": "We are lucky good LLMs were invented at the time they were", "cluster": 13, "x": 6.627483367919922, "y": 3.078857421875}, {"id": 43463443, "title": "Thinking Different, Thinking Slowly: LLMs on a PowerPC Mac", "cluster": 13, "x": 6.82000732421875, "y": 3.777322769165039}, {"id": 43463071, "title": "LLM's \"Think\" Differently", "cluster": 13, "x": 6.605397701263428, "y": 3.0401368141174316}, {"id": 43461139, "title": "LLMs, but Only Because Your Tech Sucks", "cluster": 13, "x": 6.50661039352417, "y": 3.45320200920105}, {"id": 43460786, "title": "Sentence-Level Reward Model Can Better Aligning LLM from Human Preference", "cluster": 13, "x": 6.8904128074646, "y": 3.2266013622283936}, {"id": 43460455, "title": "Every Flop Counts: Scaling a 300B LLM Without Premium GPUs", "cluster": 13, "x": 7.072492599487305, "y": 3.5789434909820557}, {"id": 43460163, "title": "Can Large Vision Language Models Read Maps Like a Human?", "cluster": 208, "x": 8.44555950164795, "y": 4.370691299438477}, {"id": 43459568, "title": "Trace your local Ollama model with Langfuse", "cluster": 13, "x": 6.860560417175293, "y": 3.682508707046509}, {"id": 43458895, "title": "From Black Box to Glass Box: LLM Observability in Action", "cluster": 13, "x": 6.8912434577941895, "y": 3.336031198501587}, {"id": 43458474, "title": "The LTV Playbook", "cluster": 13, "x": 7.0852952003479, "y": 3.2108209133148193}, {"id": 43457523, "title": "Calculate Throughput with LLVM's Scheduling Model", "cluster": 13, "x": 7.044395923614502, "y": 3.6216366291046143}, {"id": 43456448, "title": "LLMs, but Only Because Your Tech Sucks", "cluster": 13, "x": 6.44236421585083, "y": 3.4523870944976807}, {"id": 43456509, "title": "Building Robust Evaluations for Production-Ready LLM Workflows", "cluster": 13, "x": 6.823769569396973, "y": 3.433459758758545}, {"id": 43455319, "title": "LLM Multi-Round Coding Tournament", "cluster": 13, "x": 6.745789527893066, "y": 3.482602596282959}, {"id": 43454843, "title": "Semantic Diffusion", "cluster": 208, "x": 8.541467666625977, "y": 4.274559497833252}, {"id": 43454946, "title": "Exploring Hidden Reasoning Process of Large Language Models by Misleading Them", "cluster": 208, "x": 8.494433403015137, "y": 4.17094087600708}, {"id": 43454692, "title": "Bot or Not \u2013 a social deduction game with LLMs", "cluster": 13, "x": 6.681591987609863, "y": 3.167660713195801}, {"id": 43454435, "title": "Apparent signs of distress during LLM redteaming", "cluster": 13, "x": 6.568694591522217, "y": 3.0207958221435547}, {"id": 43454136, "title": "Tied Crosscoders: Tracing How Chat LLM Behavior Emerges from Base Model", "cluster": 13, "x": 6.794383525848389, "y": 3.531273126602173}, {"id": 43453245, "title": "Training Llama Using LibGen: Hack, a Theft, or Just Fair Use?", "cluster": 13, "x": 6.632917881011963, "y": 3.5608086585998535}, {"id": 43453373, "title": "Automating Interactive Fiction Logic Generation with LLMs in Emacs", "cluster": 13, "x": 6.876327991485596, "y": 3.6951162815093994}, {"id": 43452896, "title": "Can Large Vision Language Models Read Maps Like a Human?", "cluster": 208, "x": 8.44987678527832, "y": 4.362361907958984}, {"id": 43451821, "title": "How I force LLMs to generate correct code", "cluster": 13, "x": 6.73199462890625, "y": 3.7827420234680176}, {"id": 43451172, "title": "Training LLM Agents", "cluster": 13, "x": 6.799998760223389, "y": 3.1591882705688477}, {"id": 43450732, "title": "Improving recommendation systems and search in the age of LLMs", "cluster": 13, "x": 6.710858345031738, "y": 3.179694652557373}, {"id": 43450039, "title": "Deciphering language processing in the human brain through LLM representations", "cluster": 13, "x": 6.8704376220703125, "y": 3.1777384281158447}, {"id": 43449303, "title": "Using LLMs to Enrich Datasets", "cluster": 13, "x": 7.0254225730896, "y": 3.4486687183380127}, {"id": 43445046, "title": "Deciphering language processing in the human brain through LLM representations", "cluster": 13, "x": 6.908454418182373, "y": 3.1568284034729004}, {"id": 43444445, "title": "Pocket Flow: 100-line LLM framework. Let Agents build Agents", "cluster": 13, "x": 6.890769004821777, "y": 3.414691925048828}, {"id": 43444181, "title": "SpatialLM: Large Language Model for Spatial Understanding", "cluster": 208, "x": 8.482816696166992, "y": 4.392213821411133}, {"id": 43444091, "title": "AMD launches Gaia open source project for running LLMs locally on any PC", "cluster": 13, "x": 6.896808624267578, "y": 3.8714988231658936}, {"id": 43443861, "title": "AI goes to the shrink: LLM imagines things then visualizes by Stable Diffusion", "cluster": 12, "x": 6.973855495452881, "y": 2.7090084552764893}, {"id": 43441457, "title": "AMD launches Gaia open source project for running LLMs locally on any PC", "cluster": 13, "x": 6.8946428298950195, "y": 3.8870015144348145}, {"id": 43441187, "title": "Interpreting the Repeated Token Phenomenon in Large Language Models", "cluster": 208, "x": 8.216520309448242, "y": 4.305382251739502}, {"id": 43440638, "title": "The Philosophy of ConsciousInsights:Leveraging LLMs to understand your happiness", "cluster": 13, "x": 6.574971675872803, "y": 3.0087568759918213}, {"id": 43440659, "title": "KBLaM: Bringing plug-and-play external knowledge to LLMs", "cluster": 13, "x": 6.816174507141113, "y": 3.4656786918640137}, {"id": 43440232, "title": "Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor", "cluster": 13, "x": 6.62973165512085, "y": 3.018735647201538}, {"id": 43439862, "title": "Gaia: An Open-Source Project from AMD for Running Local LLMs", "cluster": 13, "x": 6.8493547439575195, "y": 3.8688414096832275}, {"id": 43439501, "title": "Deciphering language processing in the human brain through LLM representations", "cluster": 13, "x": 6.8724684715271, "y": 3.1765642166137695}, {"id": 43438951, "title": "Siri Llama \u2013 access locally running LLMs through Siri", "cluster": 13, "x": 6.8821892738342285, "y": 3.7862863540649414}, {"id": 43438141, "title": "Beyond Tools: LLMs and the Emergence of Extended Cognition", "cluster": 13, "x": 6.74924898147583, "y": 3.1590089797973633}, {"id": 43436504, "title": "MiniFed: Integrating LLM-Based Agentic-Workflow for Simulating FOMC Meeting", "cluster": 13, "x": 7.0724711418151855, "y": 3.448610305786133}, {"id": 43435782, "title": "Deep-ML \u2013 Advanced ML Challenges", "cluster": 13, "x": 7.623128414154053, "y": 3.7309036254882812}, {"id": 43434990, "title": "How I force LLMs to generate correct code", "cluster": 13, "x": 6.7355546951293945, "y": 3.793618679046631}, {"id": 43434171, "title": "LangManus: An Open-Source Manus Agent with LangChain + LangGraph", "cluster": 13, "x": 6.875282287597656, "y": 3.752072811126709}, {"id": 43433565, "title": "Free file conversion to LLM optimized Markdown", "cluster": 13, "x": 6.907986640930176, "y": 3.639537811279297}, {"id": 43432443, "title": "Using a LLM to write the logic in a book", "cluster": 13, "x": 6.802125453948975, "y": 3.3105173110961914}, {"id": 43432182, "title": "The Curse of Depth in Large Language Models", "cluster": 208, "x": 8.244807243347168, "y": 4.334712028503418}, {"id": 43431878, "title": "LLM Agents Are Graphs", "cluster": 13, "x": 6.837088108062744, "y": 3.170923948287964}, {"id": 43431742, "title": "Building AI pipelines for voice assistants using ROCm, LlamaIndex, and RAG", "cluster": 12, "x": 7.052566051483154, "y": 2.62406325340271}, {"id": 43430616, "title": "Show top LLMs some code and they'll merrily add in the bugs they saw in training", "cluster": 13, "x": 6.641054153442383, "y": 3.7252249717712402}, {"id": 43430519, "title": "SpatialLM: Large Language Model for Spatial Understanding", "cluster": 208, "x": 8.515257835388184, "y": 4.362600803375244}, {"id": 43430336, "title": "Empowering LLMs for Time Series Forecasting with Temporal Patterns and Semantics", "cluster": 13, "x": 6.85986852645874, "y": 3.2258827686309814}, {"id": 43428965, "title": "Why Do Multi-Agent LLM Systems Fail?", "cluster": 13, "x": 6.601600646972656, "y": 3.1541411876678467}, {"id": 43427655, "title": "We used LLMs to help us find the perfect piece of land for our home", "cluster": 13, "x": 6.60830020904541, "y": 3.394930839538574}, {"id": 43426267, "title": "Enhancing LLM Applications with GraphRAG", "cluster": 13, "x": 7.044548034667969, "y": 3.582474708557129}, {"id": 43426115, "title": "When working with LLMs I am used to starting \"New Conversation\" for each request", "cluster": 13, "x": 6.632737636566162, "y": 3.4043514728546143}, {"id": 43426004, "title": "Implementing Access Control in LangChain: The Four-Perimeter Approach", "cluster": 13, "x": 7.018584251403809, "y": 3.83811092376709}, {"id": 43425134, "title": "Do CRMs improve customer relationships?", "cluster": 13, "x": 6.741936683654785, "y": 3.1535050868988037}, {"id": 43424107, "title": "Tapered Off-Policy Reinforce: Stable and Efficient RL for LLMs", "cluster": 13, "x": 6.619611740112305, "y": 3.186337947845459}, {"id": 43423678, "title": "What's the Deal with LLM Mid-Training?", "cluster": 13, "x": 6.664074897766113, "y": 3.200721025466919}, {"id": 43422538, "title": "DeepSeek vs. OpenAI vs. Gemini: Which LLM Is Best for Your AI App?", "cluster": 12, "x": 7.056410312652588, "y": 2.7452921867370605}, {"id": 43422585, "title": "Allow candidates to use LLMs in job interviews", "cluster": 13, "x": 6.655885219573975, "y": 3.231844425201416}, {"id": 43422189, "title": "Do You Think Prompts Are the Secret Sauce of LLM Wrapper Startups?", "cluster": 13, "x": 6.686489582061768, "y": 3.33917236328125}, {"id": 43421223, "title": "Common Agentic Workflow Patterns", "cluster": 38, "x": 8.67949390411377, "y": 3.628815174102783}, {"id": 43421153, "title": "KBLaM: Bringing plug-and-play external knowledge to LLMs", "cluster": 13, "x": 6.8413166999816895, "y": 3.4648361206054688}, {"id": 43420693, "title": "How I force LLMs to generate correct code", "cluster": 13, "x": 6.704558849334717, "y": 3.7543718814849854}, {"id": 43419088, "title": "Show top LLMs some code and they'll merrily add in the bugs they saw in training", "cluster": 13, "x": 6.637402534484863, "y": 3.760808229446411}, {"id": 43419072, "title": "Writing an LLM from scratch, part 10 \u2013 dropout", "cluster": 13, "x": 6.74951171875, "y": 3.2205731868743896}, {"id": 43417511, "title": "LLM Agents Are Simply Graph \u2013 Tutorial for Dummies", "cluster": 13, "x": 6.910533428192139, "y": 3.1846542358398438}, {"id": 43414512, "title": "Open-source LLM beats OpenAI o1 and DeepSeek-R1 for PyTorch-to-Triton codegen", "cluster": 13, "x": 7.092318534851074, "y": 3.7670347690582275}, {"id": 43414338, "title": "KBLaM: Bringing plug-and-play external knowledge to LLMs", "cluster": 13, "x": 6.820994853973389, "y": 3.448953151702881}, {"id": 43412948, "title": "Use These Security, Privacy, and Safety Essentials for LLM Integration", "cluster": 13, "x": 6.5190863609313965, "y": 3.4669981002807617}, {"id": 43411983, "title": "MarketSenseAI 2.0: Enhancing Stock Analysis Through LLM Agents", "cluster": 13, "x": 6.900038719177246, "y": 3.142820358276367}, {"id": 43411316, "title": "4 Learnings from Load Testing LLMs", "cluster": 13, "x": 6.8916120529174805, "y": 3.3497567176818848}, {"id": 43410137, "title": "Jimliddle/Privatellmlens: A Zero-Server Web Interface for Use with Local LLMs", "cluster": 13, "x": 6.748294353485107, "y": 3.697849750518799}, {"id": 43410189, "title": "LLMs Are Bug Replicators: An Empirical Study on LLMs' Completing Bug-Prone Code", "cluster": 13, "x": 6.69103479385376, "y": 3.7612874507904053}, {"id": 43410040, "title": "Accelerating Large-Scale Test Migration with LLMs", "cluster": 13, "x": 6.985081195831299, "y": 3.451751947402954}, {"id": 43409878, "title": "Show top LLMs buggy code and they'll finish off the mistake", "cluster": 13, "x": 6.67523717880249, "y": 3.778244972229004}, {"id": 43408602, "title": "EXAONE Deep: Reasoning Enhanced Language Models", "cluster": 208, "x": 8.425370216369629, "y": 4.3384246826171875}, {"id": 43405994, "title": "Adding /Llms.txt", "cluster": 13, "x": 6.83260440826416, "y": 3.6414928436279297}, {"id": 43405733, "title": "Benchmarking LMs for Dedupe", "cluster": 13, "x": 7.146780490875244, "y": 3.777665853500366}, {"id": 43403685, "title": "Improving Recommendation Systems and Search in the Age of LLMs", "cluster": 13, "x": 6.7564568519592285, "y": 3.205418586730957}, {"id": 43403786, "title": "Actual LLM agents are coming. They will be trained", "cluster": 13, "x": 6.787949085235596, "y": 3.1375572681427}, {"id": 43402509, "title": "Entropy Analysis to Understand LLM Hallucinations (2024)", "cluster": 13, "x": 6.7193193435668945, "y": 3.0654525756835938}, {"id": 43400939, "title": "A Visual Guide to LLM Agents", "cluster": 13, "x": 6.9081010818481445, "y": 3.268188714981079}, {"id": 43398365, "title": "A Beginner's Guide to Learning LLMs", "cluster": 13, "x": 6.821864128112793, "y": 3.335529327392578}, {"id": 43397788, "title": "LLMs struggle with perception, not reasoning, in ARC-AGI", "cluster": 13, "x": 6.785764694213867, "y": 3.1706137657165527}, {"id": 43396976, "title": "LLM and structured output as a translation service", "cluster": 13, "x": 6.8678083419799805, "y": 3.5704028606414795}, {"id": 43396131, "title": "Run Private Uncensored Offline LLMs [video]", "cluster": 13, "x": 6.515315055847168, "y": 3.452946662902832}, {"id": 43393726, "title": "Personalize Your LLM: Fake it then Align it", "cluster": 13, "x": 6.550147533416748, "y": 3.21881365776062}, {"id": 43393054, "title": "LLM in Litecli for SQLite", "cluster": 13, "x": 6.84305477142334, "y": 3.7152023315429688}, {"id": 43392096, "title": "LLM Training Without a Parallel Filesystem", "cluster": 13, "x": 6.905510902404785, "y": 3.768350601196289}, {"id": 43391840, "title": "Comparing local large language models for alt-text generation", "cluster": 208, "x": 8.146930694580078, "y": 4.427181720733643}, {"id": 43391552, "title": "A Visual Guide to LLM Agents", "cluster": 13, "x": 6.899821758270264, "y": 3.2695538997650146}, {"id": 43391368, "title": "Should I Learn LLMs and ML for my career?", "cluster": 13, "x": 6.637007236480713, "y": 3.133951187133789}, {"id": 43390957, "title": "LiDO: Exploring the Stable Plutino Parameter Space", "cluster": 13, "x": 7.024763584136963, "y": 3.7968387603759766}, {"id": 43390026, "title": "Some thoughts on LCP eBook DRM", "cluster": 13, "x": 6.711885452270508, "y": 3.407200574874878}, {"id": 43389681, "title": "Blockchain-based AMMs aren't fit for prediction markets", "cluster": 13, "x": 7.054158687591553, "y": 3.308657169342041}, {"id": 43388460, "title": "Combinatorial Optimization for All: Leveraging LLMs to Assist Non-Experts", "cluster": 13, "x": 6.890867233276367, "y": 3.2612078189849854}, {"id": 43388248, "title": "Jungle \u2013 Diffusion and LLM Based Fighter Game", "cluster": 13, "x": 7.00040340423584, "y": 3.44423246383667}, {"id": 43387906, "title": "LLM generated code is like particleboard", "cluster": 13, "x": 6.6842122077941895, "y": 3.686650514602661}, {"id": 43387232, "title": "Software is the new hardware, LLMs are the new software", "cluster": 13, "x": 6.668358325958252, "y": 3.6046810150146484}, {"id": 43387375, "title": "Who's using the LLM feature in all products?", "cluster": 13, "x": 6.694827079772949, "y": 3.435715675354004}, {"id": 43386412, "title": "Game: Outsmart LLM", "cluster": 13, "x": 6.786622047424316, "y": 3.104477643966675}, {"id": 43385853, "title": "LLM powered PDF to Word conversion with layout preservation", "cluster": 13, "x": 6.952400207519531, "y": 3.681910991668701}, {"id": 43384436, "title": "Stasc: Self-Taught Self-Correction for Small Language Models", "cluster": 208, "x": 8.35847282409668, "y": 4.469933032989502}, {"id": 43383617, "title": "Hon Hai Research Institute Launches Traditional Chinese LLM with Reasoning", "cluster": 13, "x": 6.90703010559082, "y": 3.2408342361450195}, {"id": 43382979, "title": "How will LLMs take our jobs?", "cluster": 13, "x": 6.624814510345459, "y": 3.1400580406188965}, {"id": 43381831, "title": "Python's 'shelve' is useful for LLM debugging", "cluster": 13, "x": 6.853451728820801, "y": 3.7599644660949707}, {"id": 43381561, "title": "Comparison of Frontier Open-Source and Proprietary LLMs for Complex Diagnoses", "cluster": 13, "x": 6.852965831756592, "y": 3.541175365447998}, {"id": 43380021, "title": "Entropy is all you need? The quest for best tokens and the new physics of LLMs", "cluster": 13, "x": 6.803076267242432, "y": 3.181370973587036}, {"id": 43379629, "title": "Exploiting LLM Tools", "cluster": 13, "x": 6.668970584869385, "y": 3.502145290374756}, {"id": 43379806, "title": "Finetuning a dumb simple R1-like LLM to subtract", "cluster": 13, "x": 6.786534309387207, "y": 3.31169056892395}, {"id": 43378967, "title": "Clock and Calendar Understanding Challenges in Multimodal LLMs", "cluster": 13, "x": 6.744771480560303, "y": 3.161275863647461}, {"id": 43378900, "title": "Comet open source remote KVM from GL-iNet", "cluster": 13, "x": 6.867762088775635, "y": 3.8988308906555176}, {"id": 43378822, "title": "Sketch-of-Thought: Efficient LLM Reasoning", "cluster": 13, "x": 6.859145164489746, "y": 3.233515501022339}, {"id": 43378401, "title": "Big LLMs weights are a piece of history", "cluster": 13, "x": 6.698997974395752, "y": 3.2009191513061523}, {"id": 43378001, "title": "Mlx-community/OLMo-2-0325-32B-Instruct-4bit", "cluster": 13, "x": 6.666727542877197, "y": 3.535524368286133}, {"id": 43375611, "title": "A Review of ThunderKittens for General ML Audience", "cluster": 13, "x": 7.059728145599365, "y": 3.4358279705047607}, {"id": 43375286, "title": "Adversarial Prompting in LLMs", "cluster": 13, "x": 6.709158897399902, "y": 3.304154872894287}, {"id": 43374283, "title": "AutoHete: An Automatic and Efficient Heterogeneous Training System for LLMs", "cluster": 13, "x": 6.875848770141602, "y": 3.4189236164093018}, {"id": 43373365, "title": "Advancing Musicality in Symbolic Music Generation with LLM Training Paradigms", "cluster": 13, "x": 6.848654270172119, "y": 3.3173625469207764}, {"id": 43368738, "title": "The rise of intelligent infrastructure for LLM applications", "cluster": 13, "x": 6.843161582946777, "y": 3.3325588703155518}, {"id": 43368662, "title": "How would I go about finetuning Deep Seek or Llama to do this?", "cluster": 13, "x": 7.140275478363037, "y": 3.4892706871032715}, {"id": 43367689, "title": "POC \u2013 LLM Transpiler extension for VS Code", "cluster": 13, "x": 7.016768932342529, "y": 4.105570316314697}, {"id": 43365334, "title": "Understanding GRPO and how GRPO is changing LLM Training", "cluster": 13, "x": 6.664482116699219, "y": 3.162829875946045}, {"id": 43364522, "title": "Ruby LLM", "cluster": 13, "x": 6.706252098083496, "y": 3.4582085609436035}, {"id": 43364047, "title": "Taming the LLM OCR Wild West: Which Model Works?", "cluster": 13, "x": 6.79423713684082, "y": 3.417872190475464}, {"id": 43362841, "title": "Some Thoughts on LCP eBook DRM", "cluster": 13, "x": 6.716559886932373, "y": 3.40374755859375}, {"id": 43361695, "title": "Accelerate CPU Based LLM Inference with a Vector Index on the Output Embeddings", "cluster": 13, "x": 7.238296985626221, "y": 3.605191946029663}, {"id": 43361360, "title": "A Survey of Long Chain-of-Thought for Reasoning Large Language Models", "cluster": 208, "x": 8.305968284606934, "y": 4.296265125274658}, {"id": 43361213, "title": "China's Autonomous Agent, Manus, Changes Everything", "cluster": 35, "x": 8.439940452575684, "y": 3.7944846153259277}, {"id": 43359258, "title": "Advantages of Generating Clojure with LLMs", "cluster": 13, "x": 6.871758937835693, "y": 3.6347758769989014}, {"id": 43359371, "title": "Clojurellm-data: Clojure LLM \u2013 Dataset curation for fine tuning an LLM", "cluster": 13, "x": 6.952983856201172, "y": 3.6032025814056396}, {"id": 43357247, "title": "MCP \u2013 A Standardized Bridge Between LLMs and External Tools", "cluster": 13, "x": 6.827847957611084, "y": 3.60465669631958}, {"id": 43356524, "title": "Explaining LLM Distillation", "cluster": 13, "x": 6.702381134033203, "y": 3.230882406234741}, {"id": 43355597, "title": "A Guide to In-Browser LLMs", "cluster": 13, "x": 6.796341896057129, "y": 3.7457387447357178}, {"id": 43355560, "title": "Web Applets, an open protocol for LLMs to talk to web apps", "cluster": 13, "x": 6.795074939727783, "y": 3.6864943504333496}, {"id": 43354843, "title": "Using traditional ML and LLMs to analyze Executive Orders (1789 \u2013 2025)", "cluster": 13, "x": 6.881417751312256, "y": 3.369297504425049}, {"id": 43354842, "title": "LLM Naming Convention", "cluster": 13, "x": 6.805091857910156, "y": 3.426257848739624}, {"id": 43354398, "title": "I made an LLM-Powered ToDo-App because Notion is trash", "cluster": 13, "x": 6.773858070373535, "y": 3.684483528137207}, {"id": 43354015, "title": "Actual LLM agents are coming. They will be trained", "cluster": 13, "x": 6.8130083084106445, "y": 3.1404411792755127}, {"id": 43354000, "title": "Where does in context learning happen in LLMs? [pdf]", "cluster": 13, "x": 6.861746311187744, "y": 3.2867093086242676}, {"id": 43352968, "title": "Cohere's minimal compute new LLM", "cluster": 13, "x": 6.983164310455322, "y": 3.583418369293213}, {"id": 43352385, "title": "An LLM that you message over SMS", "cluster": 13, "x": 6.62594747543335, "y": 3.342073917388916}, {"id": 43352199, "title": "Google Agentspace", "cluster": 35, "x": 8.459281921386719, "y": 3.847607374191284}, {"id": 43352191, "title": "Cellm \u2013 Use LLMs in Excel Formulas", "cluster": 13, "x": 6.7537841796875, "y": 3.3058013916015625}, {"id": 43351782, "title": "Sorting-Free GPU Kernels for LLM Sampling", "cluster": 13, "x": 7.3305158615112305, "y": 3.81540846824646}, {"id": 43350872, "title": "Implementing LLM Speculative Sampling in Under 100 Lines of Code", "cluster": 13, "x": 6.876983165740967, "y": 3.660176992416382}, {"id": 43350816, "title": "Benchmarking LLMs on Document Automation", "cluster": 13, "x": 7.032633304595947, "y": 3.485358953475952}, {"id": 43345839, "title": "LM Studio 0.3.10: Speculative Decoding", "cluster": 13, "x": 7.003121376037598, "y": 3.5095574855804443}, {"id": 43345898, "title": "When will LLMs become human-level bloggers?", "cluster": 13, "x": 6.5222601890563965, "y": 3.0932397842407227}, {"id": 43345695, "title": "Sharing actual GPU core and VRAM utilization metrics for query on 10 LLM models", "cluster": 13, "x": 7.066808700561523, "y": 3.577730894088745}, {"id": 43345263, "title": "AI Code Fusion: Optimize exporting your code for LLM contexts \u2013 packs files", "cluster": 12, "x": 7.080749988555908, "y": 2.635800361633301}, {"id": 43345164, "title": "A Survey on Post-Training of Large Language Models", "cluster": 208, "x": 8.201269149780273, "y": 4.440944671630859}, {"id": 43345064, "title": "Local flexibility drives oligomorphism in computationally designed proteins", "cluster": 208, "x": 8.475759506225586, "y": 4.48373556137085}, {"id": 43344487, "title": "Distinguishing GUI Component States for Blind Users Using LLMs", "cluster": 13, "x": 6.8120527267456055, "y": 3.605989694595337}, {"id": 43343989, "title": "Why Do Researchers Care About Small Language Models?", "cluster": 208, "x": 8.237226486206055, "y": 4.217508316040039}, {"id": 43343891, "title": "M3 Ultra with 512GB memory and 2 VM limit", "cluster": 13, "x": 7.180083274841309, "y": 3.8671715259552}, {"id": 43342717, "title": "Trying to Write a Paper with LLM Assistance", "cluster": 13, "x": 6.691073894500732, "y": 3.3236398696899414}, {"id": 43340953, "title": "LLMs and Humans Unite, You Have Nothing to Lose but Your Chores", "cluster": 13, "x": 6.524710655212402, "y": 3.1429286003112793}, {"id": 43341000, "title": "What Is vLLM \u2013 Dictionary Entry (Hopsworks)", "cluster": 13, "x": 6.98553466796875, "y": 3.8615646362304688}, {"id": 43340533, "title": "LLMs and Agents Are Overhyped, with Dr. Andriy Burkov", "cluster": 13, "x": 6.612854480743408, "y": 3.107011079788208}, {"id": 43340487, "title": "First official release of LLVM Flang", "cluster": 13, "x": 7.010484218597412, "y": 3.892594575881958}, {"id": 43340399, "title": "Building Websites With LLMs (viz. (L)ots of (L)ittle ht(M)l page(S).)", "cluster": 13, "x": 6.755675315856934, "y": 3.688049793243408}, {"id": 43339303, "title": "Turning Congressional Job Listings into Data Using LLM", "cluster": 13, "x": 6.8330230712890625, "y": 3.5544114112854004}, {"id": 43339023, "title": "Why do LLMs fail to count letters? A simple explanation", "cluster": 13, "x": 6.658292293548584, "y": 3.1768128871917725}, {"id": 43338749, "title": "Sanitext \u2013 Remove LLM-Generated Text Fingerprints", "cluster": 13, "x": 6.620096683502197, "y": 3.519948959350586}, {"id": 43337483, "title": "TensorRT-LLM runtime now open-source", "cluster": 13, "x": 6.908937931060791, "y": 3.803591012954712}, {"id": 43336017, "title": "LLM Agent Cypress Test Writer", "cluster": 13, "x": 6.718252182006836, "y": 3.5994253158569336}, {"id": 43334823, "title": "LLVM 20.1.0 Released", "cluster": 13, "x": 7.040602207183838, "y": 3.948956251144409}, {"id": 43334630, "title": "Why do LLMs fail to count letters in a word?", "cluster": 13, "x": 6.619198322296143, "y": 3.144697666168213}, {"id": 43334644, "title": "New tools for building agents", "cluster": 38, "x": 8.668622970581055, "y": 3.592655658721924}, {"id": 43333702, "title": "Inference Characteristics of Llama", "cluster": 13, "x": 6.950511455535889, "y": 3.004603385925293}, {"id": 43333154, "title": "Why Do Researchers Care About Small Language Models?", "cluster": 208, "x": 8.235689163208008, "y": 4.2140092849731445}, {"id": 43332669, "title": "Here's how I use LLMs to help me write code", "cluster": 13, "x": 6.67765474319458, "y": 3.681610584259033}, {"id": 43332352, "title": "A Practical Guide to Running Local LLMs", "cluster": 13, "x": 6.801021575927734, "y": 3.5230679512023926}, {"id": 43332234, "title": "Engineering Reasoning LLMs: Notes and Observations", "cluster": 13, "x": 6.859111785888672, "y": 3.307424783706665}, {"id": 43331673, "title": "Owl: Optimized Workforce Learning for multi-agent collaboration", "cluster": 38, "x": 8.626964569091797, "y": 3.687293291091919}, {"id": 43329795, "title": "Meta must defend claim it stripped copyright info from Llama's training fodder", "cluster": 13, "x": 6.594558238983154, "y": 3.311222791671753}, {"id": 43329152, "title": "Workspace for equipping LLM models with more tools", "cluster": 13, "x": 6.8007426261901855, "y": 3.6374900341033936}, {"id": 43327499, "title": "A simple LLM code sandbox using gvisor", "cluster": 13, "x": 6.903201580047607, "y": 3.788421392440796}, {"id": 43327281, "title": "Hot prompting boosts LLM accuracy with fact highlights", "cluster": 13, "x": 6.849538803100586, "y": 3.3377177715301514}, {"id": 43323755, "title": "People are just as bad as my LLMs", "cluster": 13, "x": 6.516033172607422, "y": 3.058429718017578}, {"id": 43321959, "title": "Swallowing the Poison Pills: Insights from Vulnerability Disparity Among LLMs", "cluster": 13, "x": 6.556962966918945, "y": 3.0422582626342773}, {"id": 43321951, "title": "Why Do Researchers Care About Small Language Models?", "cluster": 208, "x": 8.233595848083496, "y": 4.218380928039551}, {"id": 43321710, "title": "You shouldn't build autonomous agent", "cluster": 38, "x": 8.67966365814209, "y": 3.3906984329223633}, {"id": 43320392, "title": "Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs", "cluster": 13, "x": 6.924163818359375, "y": 3.194154739379883}, {"id": 43318997, "title": "China's Autonomous Agent, Manus, Changes Everything", "cluster": 35, "x": 8.435981750488281, "y": 3.773594379425049}, {"id": 43318452, "title": "Don't Think Too Much (DTTM) Proxy for Qwen/QwQ-32B", "cluster": 13, "x": 6.683305263519287, "y": 3.5637176036834717}, {"id": 43317660, "title": "Building Websites with LLMs", "cluster": 13, "x": 6.72816276550293, "y": 3.6673686504364014}, {"id": 43316127, "title": "Writing an LLM from scratch, part 9 \u2013 causal attention", "cluster": 13, "x": 6.703864574432373, "y": 3.126605987548828}, {"id": 43313896, "title": "Building an Agentic System", "cluster": 38, "x": 8.675586700439453, "y": 3.588355779647827}, {"id": 43313530, "title": "Linguistic Generalizations Are Not Rules: Impacts on Evaluation of LMs", "cluster": 13, "x": 6.739892959594727, "y": 3.3161096572875977}, {"id": 43311481, "title": "What's New in LLMs", "cluster": 13, "x": 6.687661647796631, "y": 3.27638578414917}, {"id": 43310917, "title": "China's Autonomous Agent, Manus, Changes Everything", "cluster": 35, "x": 8.435009956359863, "y": 3.7878048419952393}, {"id": 43309735, "title": "Why Fathom Light Uses Lidar in Long Range Airborne Applications", "cluster": 13, "x": 6.678781986236572, "y": 3.219637155532837}, {"id": 43307759, "title": "Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression", "cluster": 13, "x": 7.156765460968018, "y": 3.591289758682251}, {"id": 43307159, "title": "Sidekick: Local-first native macOS LLM app", "cluster": 13, "x": 6.854780673980713, "y": 3.7224674224853516}, {"id": 43305574, "title": "Running Tracery Bots with LLMs", "cluster": 13, "x": 6.715509414672852, "y": 3.4281787872314453}, {"id": 43302183, "title": "How much are LLMs boosting real-world programmer productivity?", "cluster": 13, "x": 6.644772052764893, "y": 3.541753053665161}, {"id": 43300987, "title": "LLM Not LLVM", "cluster": 13, "x": 6.671231269836426, "y": 3.3870108127593994}, {"id": 43300736, "title": "Pocket Flow \u2013 A 100-line LLM framework", "cluster": 13, "x": 6.868713855743408, "y": 3.5025975704193115}, {"id": 43300694, "title": "Human vs. LLM generated code, which one would be safer?", "cluster": 13, "x": 6.677858829498291, "y": 3.7573752403259277}, {"id": 43300687, "title": "What's your experience with simple prompting Vs complex with LLMs?", "cluster": 13, "x": 6.712637424468994, "y": 3.323551654815674}, {"id": 43300809, "title": "The State of LLM Reasoning Models Part 1: Inference-Time Compute Scaling Methods", "cluster": 13, "x": 7.129098892211914, "y": 3.31593918800354}, {"id": 43300062, "title": "Smaller but Better: Unifying Layout Generation with Smaller LLMs", "cluster": 13, "x": 6.945432186126709, "y": 3.64554500579834}, {"id": 43300148, "title": "The Guide to Web Search APIs for LLMs", "cluster": 13, "x": 6.83082914352417, "y": 3.5501952171325684}, {"id": 43299998, "title": "LLaSE-G1 A FOSS speech enhancement model", "cluster": 67, "x": 7.229517936706543, "y": 4.077468395233154}, {"id": 43298945, "title": "Asking LLMs to create my game Shepard's Dog", "cluster": 13, "x": 6.775189399719238, "y": 3.480778694152832}, {"id": 43298728, "title": "Asking LLMs to create my game Shepard's Dog [sorry, dead link]", "cluster": 13, "x": 6.734621524810791, "y": 3.6423540115356445}, {"id": 43298750, "title": "Perfect usecase for a bad LLM models", "cluster": 13, "x": 6.636472225189209, "y": 3.3667216300964355}, {"id": 43298072, "title": "Prompt Injection for Large Language Models", "cluster": 208, "x": 8.149127006530762, "y": 4.444746971130371}, {"id": 43296502, "title": "Understanding Attention in LLMs", "cluster": 13, "x": 6.743875980377197, "y": 3.1360363960266113}, {"id": 43296207, "title": "The Widespread Adoption of Large Language Model-Assisted Writing Across Society", "cluster": 208, "x": 8.214516639709473, "y": 4.340779781341553}, {"id": 43295882, "title": "Enhancing Small LMs for Graph Tasks Through Graph Encoder Integration", "cluster": 13, "x": 7.049917697906494, "y": 3.618042230606079}, {"id": 43295558, "title": "Why Elixir/OTP doesn't need an Agent framework: Part 1", "cluster": 38, "x": 8.63674545288086, "y": 3.7128305435180664}, {"id": 43295135, "title": "Study: Large language models still lack general reasoning skills", "cluster": 208, "x": 8.346406936645508, "y": 4.282299041748047}, {"id": 43294974, "title": "Letta: Letta is a framework for creating LLM services with memory", "cluster": 13, "x": 6.853546619415283, "y": 3.6903011798858643}, {"id": 43294756, "title": "From Prompt to Adventures:Creating Games with LLMs and Restate Durable Functions", "cluster": 13, "x": 6.784510612487793, "y": 3.489743709564209}, {"id": 43293323, "title": "Infinite Realms turns fantasy books into MMOs through procgen/LLM", "cluster": 13, "x": 6.951479434967041, "y": 3.557025909423828}, {"id": 43291999, "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence", "cluster": 13, "x": 6.920103549957275, "y": 3.225285768508911}, {"id": 43290027, "title": "Multi-Agents Are Out, PID Controllers Are In", "cluster": 38, "x": 8.660577774047852, "y": 3.6522769927978516}, {"id": 43287821, "title": "Ladder: Self-improving LLMs through recursive problem decomposition", "cluster": 13, "x": 6.8337836265563965, "y": 3.2836523056030273}, {"id": 43287742, "title": "GSM8K-Platinum: Revealing Performance Gaps in Frontier LLMs", "cluster": 13, "x": 6.897337436676025, "y": 3.4226326942443848}, {"id": 43287592, "title": "Assessing and alleviating state anxiety in large language models", "cluster": 208, "x": 8.169387817382812, "y": 4.319029331207275}, {"id": 43287365, "title": "CryptoTalks \u2013 A privacy focused LLM aggregator", "cluster": 13, "x": 6.53787088394165, "y": 3.5005581378936768}, {"id": 43287050, "title": "Every Flop Counts: Scaling 300B Moe LLMs Without Premium GPUs [pdf]", "cluster": 13, "x": 7.153816223144531, "y": 3.581563949584961}, {"id": 43286670, "title": "LLMs in medicine: evaluations, advances, and the future", "cluster": 13, "x": 6.71436071395874, "y": 3.1681740283966064}, {"id": 43286308, "title": "Understanding Attention in LLMs", "cluster": 13, "x": 6.735508441925049, "y": 3.106157064437866}, {"id": 43286074, "title": "Strategies for optimizing LLM tool calling", "cluster": 13, "x": 6.8644819259643555, "y": 3.5148048400878906}, {"id": 43285599, "title": "First large diffusion-based LLM", "cluster": 13, "x": 6.897197246551514, "y": 3.4121487140655518}, {"id": 43284270, "title": "LLMs Don't Know What They Don't Know\u2013and That's a Problem", "cluster": 13, "x": 6.571392059326172, "y": 3.0409910678863525}, {"id": 43283568, "title": "LLMs Are Too Helpful", "cluster": 13, "x": 6.613426208496094, "y": 3.1837868690490723}, {"id": 43283606, "title": "Local LLM Models: Are They Useful?", "cluster": 13, "x": 6.785953044891357, "y": 3.3711001873016357}, {"id": 43283108, "title": "Evaluating LLMs \u2013 Notes on a NeurIPS'24 Tutorial", "cluster": 13, "x": 6.920736789703369, "y": 3.324495553970337}, {"id": 43282671, "title": "LLM Hallucination Leaderboard", "cluster": 13, "x": 6.85532283782959, "y": 3.1065030097961426}, {"id": 43281912, "title": "Creating \"Hello World\" executable bytes for LLMs is so hard?", "cluster": 13, "x": 6.804656505584717, "y": 3.7672531604766846}, {"id": 43279855, "title": "langgraph-bigtool: Build LangGraph agents with large numbers of tools", "cluster": 208, "x": 8.073318481445312, "y": 4.495226860046387}, {"id": 43279485, "title": ".llmignore \u2013 control what files IDE LLM's can edit (and read?)", "cluster": 13, "x": 7.00563907623291, "y": 4.028867721557617}, {"id": 43278646, "title": "Understanding Attention in LLMs", "cluster": 13, "x": 6.766584396362305, "y": 3.15806245803833}, {"id": 43277887, "title": "NotaGen: Advancing Musicality in Symbolic Music Gen with LLM Training Paradigms", "cluster": 13, "x": 6.863353729248047, "y": 3.280521869659424}, {"id": 43275537, "title": "Llms.txt Hub", "cluster": 13, "x": 6.91516637802124, "y": 3.70798659324646}, {"id": 43274435, "title": "Agno: Agent framework 10,000x faster than LangChain", "cluster": 208, "x": 8.22689437866211, "y": 4.563002586364746}, {"id": 43271885, "title": "LLMs are unlikely to be path to AGI", "cluster": 13, "x": 6.534327030181885, "y": 3.0977959632873535}, {"id": 43271679, "title": "Integrate MCP servers into your Python LLM code", "cluster": 13, "x": 6.887881755828857, "y": 3.7452032566070557}, {"id": 43271424, "title": "Fix the light guide in a laser meter", "cluster": 13, "x": 7.133753776550293, "y": 3.3826985359191895}, {"id": 43271431, "title": "Can LLMs accurately evaluate their own confidence?", "cluster": 13, "x": 6.627068042755127, "y": 3.0016560554504395}, {"id": 43271282, "title": "Yes or No, Please: Building Reliable Tests for Unreliable LLMs", "cluster": 13, "x": 6.69993782043457, "y": 3.1801693439483643}, {"id": 43270815, "title": "For 25% of the Winter 2025 batch, 95% of lines of code are LLM generated", "cluster": 11, "x": 7.010190486907959, "y": 3.868971824645996}, {"id": 43270328, "title": "Why React is the best model for LLM workflows", "cluster": 13, "x": 6.742326736450195, "y": 3.6325674057006836}, {"id": 43269022, "title": "LM Studio Releases Python and JavaScript SDKs", "cluster": 13, "x": 6.9056830406188965, "y": 3.8777883052825928}, {"id": 43268991, "title": "How to use LLMs to code critical software for you", "cluster": 13, "x": 6.701169013977051, "y": 3.690988063812256}, {"id": 43268477, "title": "16-Bit to 1-Bit: Visual KV Cache Quantization for Efficient Multimodal LLMs", "cluster": 13, "x": 7.119073390960693, "y": 3.679068088531494}, {"id": 43265474, "title": "Assessing and alleviating state anxiety in large language models", "cluster": 208, "x": 8.176688194274902, "y": 4.273038387298584}, {"id": 43265110, "title": "Training LLMs with Order-Centric Augmentation", "cluster": 13, "x": 6.883303165435791, "y": 3.3094890117645264}, {"id": 43263440, "title": "Practical guide to running LLMs on consumer CPUs", "cluster": 13, "x": 6.9667887687683105, "y": 3.65897274017334}, {"id": 43263484, "title": "Prompt Injection in LLMs using emojis", "cluster": 13, "x": 6.685824871063232, "y": 3.493772506713867}, {"id": 43263088, "title": "Convolutional Multi-Hybrid Language Models", "cluster": 208, "x": 8.3237943649292, "y": 4.5181169509887695}, {"id": 43261650, "title": "Writing an LLM from scratch, part 8 \u2013 trainable self-attention", "cluster": 13, "x": 6.784026145935059, "y": 3.2443132400512695}, {"id": 43260353, "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs", "cluster": 13, "x": 6.796481132507324, "y": 3.161815881729126}, {"id": 43260217, "title": "Structured data extraction from unstructured content using LLM schemas", "cluster": 13, "x": 6.920856475830078, "y": 3.52946400642395}, {"id": 43260054, "title": "Proof of Thoughtfulness: Writing and Thinking in the Age of LLMs", "cluster": 13, "x": 6.6157660484313965, "y": 3.034675121307373}, {"id": 43258643, "title": "UMAF: Adapting LLMs Without Fine-Tuning", "cluster": 13, "x": 6.815369606018066, "y": 3.31040620803833}, {"id": 43258576, "title": "Is this the best LLM for RAG?", "cluster": 13, "x": 6.79185152053833, "y": 3.3273274898529053}, {"id": 43258481, "title": "HumT DumT: Measuring and controlling human-like language in LLMs", "cluster": 13, "x": 6.806765079498291, "y": 3.2662770748138428}, {"id": 43258249, "title": "Webinar: Unlocking the Power of SLM Distillation for Higher Accuracy", "cluster": 7, "x": 6.985453128814697, "y": 3.4809021949768066}, {"id": 43256655, "title": "LLMs are really bad at Tic-Tac-Toe", "cluster": 13, "x": 6.494372367858887, "y": 3.0459091663360596}, {"id": 43256421, "title": "Performance Debugging with LLVM-mca: Simulating the CPU", "cluster": 13, "x": 7.218404769897461, "y": 3.8351874351501465}, {"id": 43255649, "title": "How Well Do LLMs Compress Their Own Chain-of-Thought?", "cluster": 13, "x": 6.7837910652160645, "y": 3.0858752727508545}, {"id": 43255257, "title": "Building WebSites with LLMs", "cluster": 13, "x": 6.703325271606445, "y": 3.6887712478637695}, {"id": 43254250, "title": "P2L: Prompt-Based LLM Routing Wins Chatbot Arena #1", "cluster": 13, "x": 6.681191921234131, "y": 3.4736971855163574}, {"id": 43254017, "title": "The Widespread Adoption of Large Language Model-Assisted Writing Across Society", "cluster": 208, "x": 8.212777137756348, "y": 4.340741157531738}, {"id": 43253083, "title": "Building an LLM-Powered Web App to Generate Gift Ideas", "cluster": 13, "x": 6.717462062835693, "y": 3.601004123687744}, {"id": 43253211, "title": "Simple Explanation of LLMs", "cluster": 13, "x": 6.715617656707764, "y": 3.2021002769470215}, {"id": 43251431, "title": "SuperGPQA: Scaling LLM Evaluation Across 285 Graduate Disciplines", "cluster": 13, "x": 6.878498554229736, "y": 3.3471486568450928}, {"id": 43251288, "title": "OmniAI 2.0: An LLM-Agnostic Ruby Library", "cluster": 13, "x": 6.861003398895264, "y": 3.716116428375244}, {"id": 43250226, "title": "Training LLMs with MXFP4", "cluster": 13, "x": 6.810598373413086, "y": 3.4196743965148926}, {"id": 43249862, "title": "Mathematical Anomaly in LLM Reasoning?", "cluster": 13, "x": 6.733088493347168, "y": 3.099601984024048}, {"id": 43247261, "title": "Thought experiment: 1000 LLMs, 100 years", "cluster": 13, "x": 6.65660285949707, "y": 3.1352763175964355}, {"id": 43246077, "title": "LLMs Are Weird Computers", "cluster": 13, "x": 6.37168550491333, "y": 3.3557074069976807}, {"id": 43245793, "title": "Future of E2E Testing: How to Overcome Flakiness with Natural Language and LLMs", "cluster": 13, "x": 6.842111110687256, "y": 3.318145275115967}, {"id": 43243776, "title": "Top Large Language Models and How to Use Them Effectively", "cluster": 208, "x": 8.173742294311523, "y": 4.40531063079834}, {"id": 43242726, "title": "LLMs make embarrassingly parallelizable scripts even more embarrassing", "cluster": 13, "x": 6.693724632263184, "y": 3.507050037384033}, {"id": 43242752, "title": "NotaGen: Advancing Musicality in Symbolic Music Generation with LLM Training", "cluster": 13, "x": 6.8853607177734375, "y": 3.3895978927612305}, {"id": 43241501, "title": "SepLLM: Accelerate LLMs by Compressing One Segment into One Separator", "cluster": 13, "x": 7.040078639984131, "y": 3.5727176666259766}, {"id": 43240687, "title": "Flash Interpretability: Decoding Specialised Feature Neurons in LLM", "cluster": 13, "x": 6.9262261390686035, "y": 3.334897994995117}, {"id": 43236472, "title": "Discover the right LLM for your prompt", "cluster": 13, "x": 6.726524829864502, "y": 3.245692253112793}, {"id": 43234482, "title": "Fix the light guide in a laser meter", "cluster": 13, "x": 7.118113994598389, "y": 3.4714832305908203}, {"id": 43233934, "title": "The Widespread Adoption of LLM-Assisted Writing Across Society", "cluster": 13, "x": 6.623488426208496, "y": 3.177528142929077}, {"id": 43233903, "title": "Hallucinations in code are the least dangerous form of LLM mistakes", "cluster": 13, "x": 6.494538307189941, "y": 3.7075014114379883}, {"id": 43233850, "title": "Ten Years of KVM", "cluster": 13, "x": 6.6919474601745605, "y": 3.1878652572631836}, {"id": 43233172, "title": "The Hilarious Reality of LLM Code Generators in Practice", "cluster": 13, "x": 6.606724739074707, "y": 3.696716070175171}, {"id": 43232576, "title": "OPM Reply", "cluster": 13, "x": 6.5504150390625, "y": 3.0989396572113037}, {"id": 43232208, "title": "LLM Can Be Faster Than You (Can) Think", "cluster": 13, "x": 6.68939208984375, "y": 3.1653945446014404}, {"id": 43229090, "title": "The Big Shrink in LLMs", "cluster": 13, "x": 6.554803848266602, "y": 3.0948116779327393}, {"id": 43228832, "title": "What Challenge 13 thought me about LLMs", "cluster": 13, "x": 6.573358535766602, "y": 3.0380020141601562}, {"id": 43228675, "title": "How uptime and availability impact SLAs?", "cluster": 13, "x": 6.863443851470947, "y": 3.3538613319396973}, {"id": 43228373, "title": "A Generalist #1 \u2013 LLM Models' War, Majorana 1, Duolingo, Meat, Raycast", "cluster": 13, "x": 6.767243385314941, "y": 3.430184841156006}, {"id": 43227972, "title": "Hallucinations in code are the least dangerous form of LLM mistakes", "cluster": 13, "x": 6.485665321350098, "y": 3.727428913116455}, {"id": 43226966, "title": "CyberImage Flux GUI with MCP Support for Agents", "cluster": 3, "x": 8.627808570861816, "y": 3.908935546875}, {"id": 43225407, "title": "Video showing some examples how LLMs are starting to try to self-preserve", "cluster": 7, "x": 6.616454124450684, "y": 3.036142587661743}, {"id": 43222834, "title": "Infinite Retrieval: Attention enhanced LLMs in long-context processing", "cluster": 13, "x": 6.8770222663879395, "y": 3.4960665702819824}, {"id": 43222490, "title": "StarPro64 EIC7700X RISC-V SBC: Maybe LLM on NPU on NuttX?", "cluster": 13, "x": 7.13108491897583, "y": 3.9043936729431152}, {"id": 43222237, "title": "Struggling to Pick the Best LLM for Your Prompt? Prompt Analyzer Has the Answer", "cluster": 13, "x": 6.829344272613525, "y": 3.30972957611084}, {"id": 43222295, "title": "LLM Release Cycle: Game Theory and Market Dynamics", "cluster": 13, "x": 6.726980686187744, "y": 3.043663263320923}, {"id": 43222301, "title": "Letta (formerly MemGPT) is a framework for creating LLM services with memory", "cluster": 13, "x": 6.922820568084717, "y": 3.753035068511963}, {"id": 43221043, "title": "Turn any Git repository into a prompt-friendly text ingest for LLMs", "cluster": 13, "x": 6.852031707763672, "y": 3.762289047241211}, {"id": 43220707, "title": "12K hardcoded API keys and passwords found in public LLM training data", "cluster": 13, "x": 6.7079572677612305, "y": 3.514423131942749}, {"id": 43219520, "title": "LLM Schemas", "cluster": 13, "x": 6.909470081329346, "y": 3.580317735671997}, {"id": 43218703, "title": "Comparing local large language models for alt-text generation", "cluster": 208, "x": 8.143081665039062, "y": 4.427666187286377}, {"id": 43209726, "title": "Can LLMs play real-time games like supermario (other than Pokemon red)?", "cluster": 13, "x": 6.688238620758057, "y": 3.2542998790740967}, {"id": 43208827, "title": "(We) don't use LLMs for writing", "cluster": 13, "x": 6.596292018890381, "y": 3.3182766437530518}, {"id": 43208806, "title": "Lake County library leaders expect to see more challenged books", "cluster": 13, "x": 6.816813945770264, "y": 3.1509506702423096}, {"id": 43208263, "title": "Structured data extraction from unstructured content using LLM schemas", "cluster": 13, "x": 6.9102067947387695, "y": 3.5531563758850098}, {"id": 43207965, "title": "Comprehensive resource describes functions of more than 20k human genes", "cluster": 208, "x": 8.315312385559082, "y": 4.362220764160156}, {"id": 43207715, "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale", "cluster": 13, "x": 7.0775465965271, "y": 3.5784125328063965}, {"id": 43204164, "title": "System prompts in LLMs do not reliably override user prompts", "cluster": 13, "x": 6.705240726470947, "y": 3.363739252090454}, {"id": 43203513, "title": "Minions: The rise of small, on-device LMs", "cluster": 13, "x": 6.759730339050293, "y": 3.708280563354492}, {"id": 43202808, "title": "How I Use LLMs", "cluster": 13, "x": 6.749459743499756, "y": 3.49151611328125}, {"id": 43199426, "title": "How I use LLMs (Andrej Karpathy)", "cluster": 13, "x": 6.77236795425415, "y": 3.4522154331207275}, {"id": 43198790, "title": "Implicit Language Models Are RNNs: Balancing Parallelization and Expressivity", "cluster": 208, "x": 8.27336597442627, "y": 4.445208549499512}, {"id": 43198530, "title": "LLM capability, cost, & throughput (harlanlewis.com)", "cluster": 13, "x": 6.990108966827393, "y": 3.39639949798584}, {"id": 43198383, "title": "Report on Russian efforts to flood LLM/AI models with pro-Russian content", "cluster": 3, "x": 6.91733455657959, "y": 2.7195656299591064}, {"id": 43196926, "title": "Narrow finetuning can produce broadly misaligned LLMs", "cluster": 13, "x": 6.687987327575684, "y": 3.001579999923706}, {"id": 43196385, "title": "LLM Load Balancing at Scale: Consistent Hashing with Bounded Loads", "cluster": 13, "x": 7.0530171394348145, "y": 3.4762632846832275}, {"id": 43196104, "title": "Alignment Faking in Large Language Models", "cluster": 208, "x": 8.41248893737793, "y": 4.354898452758789}, {"id": 43196126, "title": "Bitnet.cpp: Efficient Inference for 1.58bit LLMs", "cluster": 13, "x": 7.150341033935547, "y": 3.56921648979187}, {"id": 43194383, "title": "Babelscape Vera - LLM powered fact-checking", "cluster": 13, "x": 6.938075065612793, "y": 3.261749267578125}, {"id": 43193619, "title": "Mercury Coder, a Diffusion LLM", "cluster": 13, "x": 6.767155647277832, "y": 3.6829233169555664}, {"id": 43191988, "title": "First large diffusion-based LLM", "cluster": 13, "x": 6.854983806610107, "y": 3.3283324241638184}, {"id": 43190190, "title": "Langfun: A New Python Library Making LLMs Fun and Object-Oriented", "cluster": 13, "x": 6.923104286193848, "y": 3.7744226455688477}, {"id": 43189825, "title": "Diffusion LLM Has Arrived", "cluster": 13, "x": 6.798460483551025, "y": 3.3242452144622803}, {"id": 43188950, "title": "LLMs and Vulnerability-Lookup: What We're Testing and Where We're Headed", "cluster": 13, "x": 6.509666442871094, "y": 3.2851333618164062}, {"id": 43188104, "title": "Visualizing Kurt Vonnegut's narrative patterns with LLM analysis", "cluster": 13, "x": 6.825311660766602, "y": 3.1457314491271973}, {"id": 43187518, "title": "Mercury Coder: frontier diffusion LLM generating 1000+ tok/sec on commodity GPUs", "cluster": 13, "x": 7.248763561248779, "y": 3.730085849761963}, {"id": 43187072, "title": "Improving Consistency in Large Language Models Through Chain of Guidance", "cluster": 208, "x": 8.2860746383667, "y": 4.357352256774902}, {"id": 43186117, "title": "LangGraph Multi-Agent Swarm", "cluster": 38, "x": 8.63659954071045, "y": 3.661079168319702}, {"id": 43185071, "title": "MITRE's Offensive Security Evaluation Framework for LLMs", "cluster": 13, "x": 6.523858070373535, "y": 3.4183404445648193}, {"id": 43184440, "title": "Datahawk \u2013 Text data browser for NLP, LLM researchers and developers", "cluster": 13, "x": 6.945037841796875, "y": 3.703446388244629}, {"id": 43183558, "title": "DeepDive: In-Depth Decryption of LLMs Construction and Inference from Scratch", "cluster": 13, "x": 6.918884754180908, "y": 3.524946928024292}, {"id": 43183605, "title": "What leaders need to know about small language models (SLMs)", "cluster": 13, "x": 6.838608264923096, "y": 3.3458139896392822}, {"id": 43183076, "title": "Minions: Where local and cloud LLMs meet", "cluster": 13, "x": 6.773916721343994, "y": 3.4770662784576416}, {"id": 43182671, "title": "Evaluating LLM Systems with Production Data at Scale (Steal This Idea)", "cluster": 13, "x": 7.004685878753662, "y": 3.3456785678863525}, {"id": 43182722, "title": "Cloudflare just dropped an Agent framework", "cluster": 37, "x": 8.552472114562988, "y": 3.85911226272583}, {"id": 43182241, "title": "Open Source LLMOps Stack", "cluster": 13, "x": 6.912233829498291, "y": 3.784710645675659}, {"id": 43181630, "title": "Third Party APM Tools", "cluster": 13, "x": 6.886711120605469, "y": 3.802339792251587}, {"id": 43181107, "title": "Replit Agent v2", "cluster": 38, "x": 8.668403625488281, "y": 3.6105220317840576}, {"id": 43180558, "title": "Defining LLM Red Teaming", "cluster": 13, "x": 6.731662273406982, "y": 3.2219269275665283}, {"id": 43180528, "title": "System2 Reasoning LLMs", "cluster": 13, "x": 6.853165149688721, "y": 3.2354447841644287}, {"id": 43180322, "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs", "cluster": 13, "x": 6.734842777252197, "y": 2.957916259765625}, {"id": 43180049, "title": "LLM-Assisted Reverse Engineering: Decoding Claude Code in Under an Hour", "cluster": 13, "x": 6.63386869430542, "y": 3.7054829597473145}, {"id": 43178716, "title": "Is your LLM-powered app providing safe answers? Evaluate it", "cluster": 13, "x": 6.796401023864746, "y": 3.3962249755859375}, {"id": 43178465, "title": "We show that o3-mini-high fails to exhibit human-like linguistic competence", "cluster": 208, "x": 8.251112937927246, "y": 4.230451583862305}, {"id": 43178410, "title": "Automating LLM-based harness synthesis for unfuzzed projects", "cluster": 13, "x": 6.9079203605651855, "y": 3.757986068725586}, {"id": 43177859, "title": "Using State Machines to Orchestrate Multi-Agent Systems", "cluster": 38, "x": 8.664613723754883, "y": 3.5906965732574463}, {"id": 43177622, "title": "Anthropic: Forecasting rare language model behaviors", "cluster": 208, "x": 8.295207023620605, "y": 4.303871154785156}, {"id": 43177475, "title": "Closing the Loop: Real-Time LLM Self-Optimization with RAG", "cluster": 13, "x": 7.033931255340576, "y": 3.3805489540100098}, {"id": 43176553, "title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs [pdf]", "cluster": 13, "x": 6.780542850494385, "y": 2.9756360054016113}, {"id": 43176499, "title": "Elimination Game: Multi-Agent LLM Social Reasoning, Strategy, and Deception", "cluster": 13, "x": 6.720015048980713, "y": 3.01174259185791}, {"id": 43176043, "title": "LLMs misaligned on one area are misaligned everywhere", "cluster": 13, "x": 6.693624019622803, "y": 2.993765354156494}, {"id": 43175658, "title": "Raink: Use LLMs for Document Ranking", "cluster": 13, "x": 6.865084171295166, "y": 3.6029953956604004}, {"id": 43175799, "title": "The Influence of Prompt Politeness on LLM Performance", "cluster": 13, "x": 6.69645357131958, "y": 3.103393793106079}, {"id": 43175101, "title": "Do LLMs write like humans? Variation in grammatical and rhetorical styles", "cluster": 13, "x": 6.593704700469971, "y": 3.1022496223449707}, {"id": 43174488, "title": "Git-Inspired File Context Management for LLMs", "cluster": 13, "x": 6.886947154998779, "y": 3.7850847244262695}, {"id": 43172051, "title": "Minions \u2013 hooking up local and cloud LLMs", "cluster": 13, "x": 6.722497463226318, "y": 3.624724864959717}, {"id": 43170818, "title": "Evaluate LLM Apps in Go", "cluster": 13, "x": 6.8340864181518555, "y": 3.5494143962860107}, {"id": 43169767, "title": "Linux Lazy Unmap Flush LUF Reduces TLB Shootdowns by 97%, Faster LLM Performance", "cluster": 13, "x": 7.117002010345459, "y": 3.850590705871582}, {"id": 43168781, "title": "The Efficiency of Vim", "cluster": 13, "x": 7.172602653503418, "y": 3.86011004447937}, {"id": 43168312, "title": "Muon Is Scalable for LLM Training", "cluster": 13, "x": 6.954672336578369, "y": 3.453157663345337}, {"id": 43167983, "title": "Fine-Tuning LLMs with Your PDF Documents: A Simple Guide", "cluster": 13, "x": 6.830373287200928, "y": 3.4422812461853027}, {"id": 43167073, "title": "RLlama: Teaching LLMs to Learn and Remember with Memory-Augmented RL", "cluster": 13, "x": 6.788912773132324, "y": 3.297318935394287}, {"id": 43167089, "title": "Our Collective Learning Disability", "cluster": 349, "x": 7.172867774963379, "y": 2.9467997550964355}, {"id": 43164913, "title": "LLMs are \"just\" a new programming language", "cluster": 13, "x": 6.657790660858154, "y": 3.554565668106079}, {"id": 43164713, "title": "CalypsoAI Security Index: How secure is that LLM?", "cluster": 13, "x": 6.45858907699585, "y": 3.415440559387207}, {"id": 43162769, "title": "Getting an LLM to write a small, nontrivial program with no human interaction", "cluster": 13, "x": 6.659454822540283, "y": 3.5413994789123535}, {"id": 43162717, "title": "I Built an LLM Framework in 179 Lines\u2013Why Are the Others So Bloated?", "cluster": 13, "x": 6.924362659454346, "y": 3.652937650680542}, {"id": 43162405, "title": "Combining Base and Instruction-Tuned LMs for Better Synthetic Data Generation", "cluster": 13, "x": 7.050618648529053, "y": 3.5055084228515625}, {"id": 43161786, "title": "Getting Perfectly Structured Data from LLMs", "cluster": 13, "x": 6.952472686767578, "y": 3.5384395122528076}, {"id": 43160270, "title": "Combinatorial Identities and Theorems Dataset for LLM Finetuning", "cluster": 13, "x": 6.882717132568359, "y": 3.3357276916503906}, {"id": 43159295, "title": "AIBrix: Cost-Effective and Scalable Control Plane for VLLM", "cluster": 13, "x": 7.063910484313965, "y": 3.864964723587036}, {"id": 43158145, "title": "Sift: Grounding LLM Reasoning in Contexts via Stickers", "cluster": 13, "x": 6.865813255310059, "y": 3.3122031688690186}, {"id": 43157279, "title": "Can I run this LLM?", "cluster": 13, "x": 6.785900592803955, "y": 3.6245276927948}, {"id": 43157403, "title": "Enhancing DeepSeek Models with MLA and FP8 Optimizations in VLLM", "cluster": 13, "x": 7.5048508644104, "y": 3.840588331222534}, {"id": 43156491, "title": "The Engine : Jonathan Swift (Sort of) Predicts Large Language Models in 1726", "cluster": 208, "x": 8.461687088012695, "y": 4.224179744720459}, {"id": 43153223, "title": "LLM Running on Commodore C64", "cluster": 13, "x": 7.01865816116333, "y": 3.856135606765747}, {"id": 43152527, "title": "Large Language Diffusion Models", "cluster": 208, "x": 8.261642456054688, "y": 4.28039026260376}, {"id": 43149595, "title": "Claims of LLM cheating are exaggerated", "cluster": 13, "x": 6.503040790557861, "y": 3.087552547454834}, {"id": 43148019, "title": "An SRE's guide to optimizing ML systems with MLOps pipelines", "cluster": 13, "x": 7.220400333404541, "y": 3.7433743476867676}, {"id": 43144428, "title": "Muon is scalable for LLM training", "cluster": 13, "x": 6.968928337097168, "y": 3.4652137756347656}, {"id": 43143580, "title": "How LLMs fared in making Arcade games?", "cluster": 13, "x": 6.660406112670898, "y": 3.1479413509368896}, {"id": 43142790, "title": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics", "cluster": 13, "x": 6.897118091583252, "y": 3.2727832794189453}, {"id": 43142034, "title": "Automatic Evals for LLMs", "cluster": 13, "x": 6.8492631912231445, "y": 3.2313106060028076}, {"id": 43141634, "title": "BaxBench: Can LLMs Generate Correct and Secure Back Ends?", "cluster": 13, "x": 6.728852272033691, "y": 3.5031068325042725}, {"id": 43140955, "title": "AIBrix: Cost-Effective and Scalable Control Plane for VLLM from ByteDance", "cluster": 13, "x": 7.126657485961914, "y": 3.9585282802581787}, {"id": 43139221, "title": "LLM Delimiters and Higher-Order Expressions", "cluster": 13, "x": 6.945293426513672, "y": 3.5487561225891113}, {"id": 43138172, "title": "Designing Backgrounds with LLMs and React", "cluster": 13, "x": 6.838172435760498, "y": 3.644136428833008}, {"id": 43134052, "title": "NixOS: The Power of VM Tests", "cluster": 13, "x": 7.278484344482422, "y": 3.857997417449951}, {"id": 43133994, "title": "Segment for LLM Traces? Seeking Feedback on an Open Source LLM Log Router", "cluster": 13, "x": 6.873060703277588, "y": 3.631059408187866}, {"id": 43132774, "title": "Learning how to think better from LLMs", "cluster": 13, "x": 6.647089958190918, "y": 3.0701112747192383}, {"id": 43131603, "title": "Indiana Jones jailbreak approach highlights the vulnerabilities of existing LLMs", "cluster": 13, "x": 6.381393909454346, "y": 3.3037197589874268}, {"id": 43131022, "title": "I think Yann Lecun was right about LLMs (but perhaps only by accident)", "cluster": 13, "x": 6.601036071777344, "y": 3.05230975151062}, {"id": 43130227, "title": "LangMem SDK for agent long-term memory", "cluster": 13, "x": 6.8944807052612305, "y": 3.8227345943450928}, {"id": 43129835, "title": "Reducing LLM Hallucinations with a Verified Semantic Cache", "cluster": 13, "x": 6.801815509796143, "y": 3.1702420711517334}, {"id": 43127600, "title": "The Case for Cognitive-Dissonance-Aware Knowledge Updates in LLMs", "cluster": 13, "x": 6.869620323181152, "y": 3.158130168914795}, {"id": 43127271, "title": "Simulating 500M years of evolution with a language model", "cluster": 208, "x": 8.49099063873291, "y": 4.197869300842285}, {"id": 43126358, "title": "Neural Machine Translation versus Large Language Models", "cluster": 208, "x": 8.195600509643555, "y": 4.343843460083008}, {"id": 43125895, "title": "Using LLMs effectively isn't about prompting", "cluster": 13, "x": 6.598721981048584, "y": 3.1850993633270264}, {"id": 43125252, "title": "PartialJSON: A lightweight library for repairing invalid LLM JSON", "cluster": 13, "x": 6.785381317138672, "y": 3.715968370437622}, {"id": 43124902, "title": "'Indiana Jones' jailbreak approach highlights vulnerabilities of existing LLMs", "cluster": 13, "x": 6.432407855987549, "y": 3.4009511470794678}, {"id": 43123269, "title": "Presumed Cultural Identity: How Names Shape LLM Responses", "cluster": 13, "x": 6.573400497436523, "y": 3.0437519550323486}, {"id": 43121876, "title": "Train an LLM from Scratch in a Single Python Notebook", "cluster": 13, "x": 6.82381534576416, "y": 3.40610408782959}, {"id": 43121276, "title": "Idiosyncrasies in Large Language Models", "cluster": 208, "x": 8.230208396911621, "y": 4.332736015319824}, {"id": 43117531, "title": "What do LLMs know about your email address?", "cluster": 13, "x": 6.473762035369873, "y": 3.497835636138916}, {"id": 43116751, "title": "Why LLM Extensibility Is Vital to API Vendors", "cluster": 13, "x": 6.7084760665893555, "y": 3.543651580810547}, {"id": 43115745, "title": "How to Backdoor Large Language Models: inject \"backdoors\" into code it writes", "cluster": 208, "x": 7.9364824295043945, "y": 4.454437255859375}, {"id": 43115648, "title": "XentGame: Help Minimize LLM Surprise", "cluster": 13, "x": 6.639703750610352, "y": 3.2973382472991943}, {"id": 43114455, "title": "CrewAI \u2013 open-source framework for LLM agents", "cluster": 13, "x": 7.001346588134766, "y": 3.2531774044036865}, {"id": 43114146, "title": "Is ML *Really* Alchemy?", "cluster": 13, "x": 6.619374752044678, "y": 2.9843366146087646}, {"id": 43112873, "title": "Query Expansion with LLMs: Searching Better by Saying More", "cluster": 13, "x": 6.9798784255981445, "y": 3.4495537281036377}, {"id": 43112199, "title": "Open-source TypeScript framework for LLMs to program themselves", "cluster": 13, "x": 6.8982133865356445, "y": 3.698697090148926}, {"id": 43111891, "title": "On-Device SLM Leaderboard", "cluster": 13, "x": 6.937838554382324, "y": 3.5151431560516357}, {"id": 43111743, "title": "Paste This Text into an LLM", "cluster": 13, "x": 6.760547161102295, "y": 3.484203338623047}, {"id": 43111453, "title": "The foundational LLM for scientific computing BBT-Neutron is now open-sourced", "cluster": 13, "x": 6.881246566772461, "y": 3.676316738128662}, {"id": 43110317, "title": "Large Language Diffusion Models", "cluster": 208, "x": 8.26073169708252, "y": 4.290317058563232}, {"id": 43108904, "title": "LLMs Critiquing Other LLMs \u2013 Pitfalls and Distributional Biases", "cluster": 13, "x": 6.70245361328125, "y": 3.0213489532470703}, {"id": 43108805, "title": "Age against the machine\u2013susceptibility of LLMs to cognitive impairment", "cluster": 13, "x": 6.625732421875, "y": 3.0378315448760986}, {"id": 43107782, "title": "ScummVM", "cluster": 13, "x": 6.5980048179626465, "y": 3.2714405059814453}, {"id": 43107456, "title": "Can a non-programmer launch SaaS startup with LLMs help? (Spoiler: yes I can)", "cluster": 13, "x": 6.644747257232666, "y": 3.5785975456237793}, {"id": 43107258, "title": "Goodbye RAG \u2013 How Hebbia Solved Information Retrieval for LLMs", "cluster": 13, "x": 6.910186290740967, "y": 3.4766674041748047}, {"id": 43106114, "title": "LangChain for Go, the easiest way to write LLM-based programs in Go", "cluster": 13, "x": 7.000334739685059, "y": 3.7703590393066406}, {"id": 43106012, "title": "Can I ethically use LLMs?", "cluster": 13, "x": 6.57611608505249, "y": 3.1326723098754883}, {"id": 43105474, "title": "The Ultra-Scale Playbook: Training LLMs on GPU Clusters", "cluster": 13, "x": 7.142359256744385, "y": 3.629772901535034}, {"id": 43104220, "title": "Speculative Decoding in LM Studio 0.3.10", "cluster": 13, "x": 6.978715419769287, "y": 3.5166475772857666}, {"id": 43103549, "title": "What Your Email Address Reveals About You: LLMs and Digital Footprints", "cluster": 13, "x": 6.535727024078369, "y": 3.46486496925354}, {"id": 43102292, "title": "Flow-of-Options: Diversified and Improved LLM Reasoning", "cluster": 13, "x": 6.813119411468506, "y": 3.119741916656494}, {"id": 43102221, "title": "New open and multi-lingual LLM Benchmark announced by Giskard", "cluster": 13, "x": 6.938040256500244, "y": 3.5376853942871094}, {"id": 43101792, "title": "New Grok 3 release tops LLM leaderboards despite Musk-approved \"based\" opinions", "cluster": 13, "x": 6.847857475280762, "y": 3.2252564430236816}, {"id": 43099488, "title": "Service to auto route LLM/Model traffic", "cluster": 13, "x": 6.908326148986816, "y": 3.562213897705078}, {"id": 43098492, "title": "Serving Local LLMs with MLX", "cluster": 13, "x": 6.721653938293457, "y": 3.339268684387207}, {"id": 43098330, "title": "LLM Codenames", "cluster": 13, "x": 6.819056510925293, "y": 3.684565782546997}, {"id": 43097932, "title": "Implementing LLaMA3 in 100 Lines of Pure Jax", "cluster": 13, "x": 6.88388729095459, "y": 3.8086159229278564}, {"id": 43096031, "title": "New LLM Scaling Law", "cluster": 13, "x": 6.801858425140381, "y": 3.2579925060272217}, {"id": 43095670, "title": "Quake 3 with LLMs: bots fragging and talking", "cluster": 13, "x": 6.697505474090576, "y": 3.2816834449768066}, {"id": 43095383, "title": "Langflow is a low-code tool for developers to build AI agents/LLM workflows", "cluster": 12, "x": 7.009836196899414, "y": 2.83445405960083}, {"id": 43095052, "title": "OpenAI's Deep Research pairs LLMs with RAGs to automate work \u2013 and replace jobs", "cluster": 13, "x": 6.763655185699463, "y": 3.336901903152466}, {"id": 43093855, "title": "LLMs all fail this NumPy indexing example", "cluster": 13, "x": 6.796660423278809, "y": 3.6450982093811035}, {"id": 43094006, "title": "My LLM codegen workflow", "cluster": 13, "x": 6.792521953582764, "y": 3.808424949645996}, {"id": 43091043, "title": "Exo software \u2013 A distributed LLM solution running on a cluster of computers, sm", "cluster": 13, "x": 6.861905574798584, "y": 3.720553159713745}, {"id": 43090350, "title": "The Curse of Depth in Large Language Models", "cluster": 208, "x": 8.198498725891113, "y": 4.355042934417725}, {"id": 43089629, "title": "Mamba-Shedder: Post-Transformer Compression for Efficient SSMs", "cluster": 13, "x": 7.101258754730225, "y": 3.588712453842163}, {"id": 43088935, "title": "How to create LLM-driven tiny gnome robots?", "cluster": 13, "x": 7.065158367156982, "y": 3.4711225032806396}, {"id": 43088195, "title": "My LLM Codegen Workflow", "cluster": 13, "x": 6.783435821533203, "y": 3.807844877243042}, {"id": 43088092, "title": "The Widespread Adoption of Large Language Model-Assisted Writing Across Society", "cluster": 208, "x": 8.215897560119629, "y": 4.3400726318359375}, {"id": 43087833, "title": "How to Install DeepSeek on Your Cloud Server with Ollama LLM", "cluster": 13, "x": 6.954660892486572, "y": 3.9474687576293945}, {"id": 43087774, "title": "The Fall of FiveM", "cluster": 13, "x": 6.571084976196289, "y": 3.142637014389038}, {"id": 43086557, "title": "Agentic framework for Non technical Professionals", "cluster": 38, "x": 8.618208885192871, "y": 3.6680383682250977}, {"id": 43086430, "title": "SWE-Lancer: Can LLMs Earn $1M from Real-World Freelance Software Engineering?", "cluster": 13, "x": 6.52586030960083, "y": 3.5497007369995117}, {"id": 43085661, "title": "Can I ethically use LLMs?", "cluster": 13, "x": 6.601484298706055, "y": 3.2034215927124023}, {"id": 43085054, "title": "Using an LLM to revamp my site", "cluster": 13, "x": 6.639490127563477, "y": 3.61600661277771}, {"id": 43084858, "title": "Large Language Diffusion Models", "cluster": 208, "x": 8.260748863220215, "y": 4.2897138595581055}, {"id": 43084951, "title": "Is it okay? (On LLM morals)", "cluster": 13, "x": 6.637231826782227, "y": 3.1775994300842285}, {"id": 43084793, "title": "Cake: Distributed LLM and StableDiffusion inference for mobile desktop or server", "cluster": 13, "x": 7.056489944458008, "y": 3.6707568168640137}, {"id": 43084331, "title": "LLaDA: LLMs That Don't Gaslight You", "cluster": 13, "x": 6.561359882354736, "y": 3.202909469604492}, {"id": 43083981, "title": "LLM Pair Programming as a Non-Technical Idea Guy: It's Complicated", "cluster": 13, "x": 6.599843978881836, "y": 3.5290627479553223}, {"id": 43083577, "title": "LSP could have been better", "cluster": 13, "x": 6.613637924194336, "y": 3.1033496856689453}, {"id": 43083309, "title": "Using vision capable LLMs to transcribe a handwritten letter", "cluster": 13, "x": 6.743696212768555, "y": 3.475464344024658}, {"id": 43082423, "title": "BadSeek: How to Backdoor Large Language Models", "cluster": 208, "x": 7.925721645355225, "y": 4.452005386352539}, {"id": 43081621, "title": "Multimodal Model Quantization Support Through LLM Compressor by Neural Magic", "cluster": 13, "x": 7.027063846588135, "y": 3.4431216716766357}, {"id": 43081284, "title": "How is the Herculean task of human supervision in LLM pre-training conducted?", "cluster": 13, "x": 6.738216400146484, "y": 3.1251070499420166}, {"id": 43080189, "title": "Large Language Diffusion Models", "cluster": 208, "x": 8.258602142333984, "y": 4.287003517150879}, {"id": 43079962, "title": "Can I ethically use LLMs?", "cluster": 13, "x": 6.599428176879883, "y": 3.2190372943878174}, {"id": 43078883, "title": "Scaling AI Applications with LLMs", "cluster": 12, "x": 7.036128520965576, "y": 2.6349732875823975}, {"id": 43078669, "title": "Age against the machine: susceptibility of LLMs to cognitive impairment", "cluster": 13, "x": 6.616829872131348, "y": 3.0335004329681396}, {"id": 43078705, "title": "Why scaling LLMs might work", "cluster": 13, "x": 6.7881951332092285, "y": 3.2530100345611572}, {"id": 43078477, "title": "LLM Arena Leaderboard", "cluster": 13, "x": 6.906628131866455, "y": 3.3132236003875732}, {"id": 43078233, "title": "How to Use an LLM Without Selling Your Soul", "cluster": 13, "x": 6.588386535644531, "y": 3.2858939170837402}, {"id": 43077865, "title": "Assured LLM-Based Software Engineering", "cluster": 13, "x": 6.564211368560791, "y": 3.5603668689727783}, {"id": 43076663, "title": "Sparrow: Open-source data processing with ML, LLM and Vision LLM", "cluster": 13, "x": 6.936244964599609, "y": 3.7922916412353516}, {"id": 43076241, "title": "Open source LLMs hit Europe's digital sovereignty roadmap", "cluster": 13, "x": 6.786810398101807, "y": 3.670224189758301}, {"id": 43076201, "title": "New White Paper: Recursive Cognitive Refinement for LLM Consistency", "cluster": 13, "x": 6.9662089347839355, "y": 3.1793177127838135}, {"id": 43075513, "title": "Large Language Diffusion Models", "cluster": 208, "x": 8.255680084228516, "y": 4.289493560791016}, {"id": 43075504, "title": "Mantella: Speak to NPCs in Skyrim and Fallout Using Voice and LLMs", "cluster": 13, "x": 6.6176910400390625, "y": 3.5209200382232666}, {"id": 43075571, "title": "ZeroBench: An Impossible Visual Benchmark for Contemporary LMMs", "cluster": 13, "x": 7.168758869171143, "y": 3.423603057861328}, {"id": 43075412, "title": "Why don't LLMs ask for calculators?", "cluster": 13, "x": 6.675256729125977, "y": 3.221090316772461}, {"id": 43074783, "title": "What would it look like to combine Google search with LLM in-context learning?", "cluster": 13, "x": 6.892325401306152, "y": 3.442805767059326}, {"id": 43072961, "title": "Describe \u2013 Describe your codebase to an LLM", "cluster": 13, "x": 6.724735260009766, "y": 3.709141492843628}, {"id": 43072457, "title": "LLMs to Analyze Drivers of Teen Substance Use in Online Discussions", "cluster": 13, "x": 6.684700965881348, "y": 3.15537428855896}, {"id": 43072397, "title": "Vote for the Best LLM Framework", "cluster": 13, "x": 6.840736389160156, "y": 3.367499589920044}, {"id": 43071589, "title": "Mastering LLM Techniques: Inference Optimization", "cluster": 13, "x": 7.084921836853027, "y": 3.274169683456421}, {"id": 43071605, "title": "A guide to LLM inference and performance", "cluster": 13, "x": 7.013123512268066, "y": 3.288625717163086}, {"id": 43071390, "title": "Improving Existing Optimization Algorithms with LLMs", "cluster": 13, "x": 7.024833679199219, "y": 3.372807741165161}, {"id": 43071060, "title": "BadSeek: How to Backdoor Large Language Models", "cluster": 208, "x": 7.92228364944458, "y": 4.449344158172607}, {"id": 43068741, "title": "Large Lambda Model", "cluster": 13, "x": 7.0040764808654785, "y": 3.528470277786255}, {"id": 43068073, "title": "Thomson Reuters vs. Ross Intelligence implies the LLM was 'non-generative'", "cluster": 13, "x": 6.71797513961792, "y": 2.9733874797821045}, {"id": 43067627, "title": "Large Language Models Show Concerning Tendency to Flatter Users", "cluster": 208, "x": 8.277151107788086, "y": 4.3427042961120605}, {"id": 43067646, "title": "An ML toolkit in your pocket", "cluster": 13, "x": 6.87042760848999, "y": 3.7161707878112793}, {"id": 43063644, "title": "OmniParser V2: Turning Any LLM into a Computer Use Agent", "cluster": 13, "x": 6.9059648513793945, "y": 3.3009438514709473}, {"id": 43061608, "title": "To avoid being replaced by LLMs, do what they can't", "cluster": 13, "x": 6.474295139312744, "y": 3.119799852371216}, {"id": 43061357, "title": "Run LLMs on macOS using LLM-mlx and Apple's MLX framework", "cluster": 13, "x": 6.8718438148498535, "y": 3.8061556816101074}, {"id": 43059853, "title": "Save Money on LLMs", "cluster": 13, "x": 6.699843883514404, "y": 3.3108150959014893}, {"id": 43059593, "title": "Distributed-Llama: Connect home devices into a cluster for LLM inference", "cluster": 13, "x": 7.0413947105407715, "y": 3.5708954334259033}, {"id": 43058718, "title": "Local LLMs are much better than their reputation", "cluster": 13, "x": 6.544547080993652, "y": 3.0750937461853027}, {"id": 43057465, "title": "Are LLMs able to play the card game Set?", "cluster": 13, "x": 6.7357892990112305, "y": 3.1794047355651855}, {"id": 43057222, "title": "I learned to stop worrying and love the LLM", "cluster": 13, "x": 6.494853973388672, "y": 3.089998960494995}, {"id": 43056662, "title": "How to Backdoor Large Language Models", "cluster": 208, "x": 7.929745197296143, "y": 4.429147243499756}, {"id": 43056228, "title": "OmniParser V2: Turning Any LLM into a Computer Use Agent", "cluster": 13, "x": 6.9865193367004395, "y": 3.3684756755828857}, {"id": 43055004, "title": "LLM Ecosystem Predictions", "cluster": 13, "x": 6.769861221313477, "y": 3.2496776580810547}, {"id": 43054363, "title": "Lessons on thinking from large language models", "cluster": 208, "x": 8.243010520935059, "y": 4.304317951202393}, {"id": 43053426, "title": "What did building an \"LLM chat UI\" teach me?", "cluster": 13, "x": 6.6935272216796875, "y": 3.449676990509033}, {"id": 43049289, "title": "EmailSnap: Open-Source LLM Agent That Saves Hours for Medical/GP Receptionists", "cluster": 13, "x": 6.7533040046691895, "y": 3.6132545471191406}, {"id": 43048994, "title": "LLMs for low-latency language translation", "cluster": 13, "x": 6.912439823150635, "y": 3.695817708969116}, {"id": 43048718, "title": "A full-stack approach to power and thermal fluctuations in ML infrastructure", "cluster": 13, "x": 7.115686416625977, "y": 3.480820894241333}, {"id": 43048203, "title": "Commercial LLM Agents Are Already Vulnerable to Simple yet Dangerous Attacks", "cluster": 13, "x": 6.3829545974731445, "y": 3.3019371032714844}, {"id": 43046079, "title": "Custom Asserts in LLVM", "cluster": 13, "x": 6.762081623077393, "y": 3.47379994392395}, {"id": 43046089, "title": "Over half of LLM-written news summaries have \"significant issues\"\u2013BBC analysis", "cluster": 13, "x": 6.6287946701049805, "y": 3.0133469104766846}, {"id": 43043234, "title": "IPEX-LLM Portable Zip for Ollama on Intel GPU", "cluster": 13, "x": 7.0546464920043945, "y": 3.802868366241455}, {"id": 43043320, "title": "Agentic Reasoning: Reasoning LLM with Agentic Tools", "cluster": 13, "x": 6.947080135345459, "y": 3.2239205837249756}, {"id": 43042995, "title": "Forget What You Know about LLMs Evaluations \u2013 LLMs Are Like a Chameleon", "cluster": 13, "x": 6.55760383605957, "y": 3.04376220703125}, {"id": 43042946, "title": "Over half of LLM-written news summaries have \"significant issues\" \u2013 BBC analysis", "cluster": 13, "x": 6.6593403816223145, "y": 3.0523359775543213}, {"id": 43042757, "title": "Revolutionizing software testing: Introducing LLM-powered bug catchers", "cluster": 13, "x": 6.6997222900390625, "y": 3.7607479095458984}, {"id": 43042753, "title": "LM2: Large Memory Models", "cluster": 13, "x": 7.08002233505249, "y": 3.681821584701538}, {"id": 43041618, "title": "DeepHermes-3 Preview, a new LLM that unifies reasoning", "cluster": 13, "x": 6.94140625, "y": 3.276560068130493}, {"id": 43039757, "title": "Chat Sutra: fast and free LLM chat", "cluster": 13, "x": 6.679155349731445, "y": 3.5519628524780273}, {"id": 43038378, "title": "LLM best practices are broken", "cluster": 13, "x": 6.575827121734619, "y": 3.141875982284546}, {"id": 43037360, "title": "Decoding AI Judgment: How LLMs Assess News Credibility and Bias", "cluster": 12, "x": 7.04658842086792, "y": 2.6676363945007324}, {"id": 43034288, "title": "Open Euro LLM", "cluster": 13, "x": 6.725214958190918, "y": 3.4180338382720947}, {"id": 43033737, "title": "LLMs Do Not Break Interviews", "cluster": 13, "x": 6.3967180252075195, "y": 3.0977962017059326}, {"id": 43033243, "title": "The Art of Sampling: Controlling Randomness in LLMs", "cluster": 13, "x": 6.826765060424805, "y": 3.2061076164245605}, {"id": 43031298, "title": "The Curse of Depth in Large Language Models", "cluster": 208, "x": 8.189033508300781, "y": 4.346755504608154}, {"id": 43031295, "title": "Evaluating LLM Reasoning Through Live Computer Games", "cluster": 13, "x": 6.796864986419678, "y": 3.209453582763672}, {"id": 43030517, "title": "Building a SNAP LLM eval: part 1", "cluster": 13, "x": 6.9138946533203125, "y": 3.30791974067688}, {"id": 43030018, "title": "Mixture-of-Agents Enhances Large Language Model Capabilities", "cluster": 208, "x": 8.196730613708496, "y": 4.358098983764648}, {"id": 43029874, "title": "Using LLMs to steganographically encode secret messages", "cluster": 13, "x": 6.595352649688721, "y": 3.526817560195923}, {"id": 43029324, "title": "Humans overtrust machines in life-or-death decisions", "cluster": 12, "x": 6.919867992401123, "y": 2.6476078033447266}, {"id": 43026887, "title": "A Unified Interface for LLMs", "cluster": 13, "x": 6.834051132202148, "y": 3.6573596000671387}, {"id": 43026824, "title": "The Poetry Fan Who Taught an LLM to Read and Write DNA", "cluster": 13, "x": 6.696595191955566, "y": 2.8460805416107178}, {"id": 43025980, "title": "Emergent Response Planning in LLM", "cluster": 13, "x": 6.814472675323486, "y": 3.266920566558838}, {"id": 43025666, "title": "LLM Friendly Zone", "cluster": 13, "x": 6.669137001037598, "y": 3.3505678176879883}, {"id": 43025620, "title": "Anemll \u2013 Open-Source Project to Convert LLM to Apple Neural Engine", "cluster": 13, "x": 6.849967002868652, "y": 3.713108539581299}, {"id": 43024794, "title": "Running Large Language Models at Scale", "cluster": 208, "x": 8.002470970153809, "y": 4.4549078941345215}, {"id": 43023597, "title": "Command Line Tool for Code Reviews with an LLM Model Running Locally with Ollama", "cluster": 13, "x": 6.882725238800049, "y": 3.818323850631714}, {"id": 43022094, "title": "A Survey on Large Language Models (2025)", "cluster": 208, "x": 8.216780662536621, "y": 4.502854347229004}, {"id": 43021666, "title": "TreeGPT: A tree-based LLM interface", "cluster": 13, "x": 6.903771877288818, "y": 3.7541890144348145}, {"id": 43020109, "title": "OS \u2013 Enhance LLM Responses with Real-Time Web Data Using SearchAugmentedLLM", "cluster": 13, "x": 6.857967853546143, "y": 3.5956931114196777}, {"id": 43019415, "title": "Building Llamas That Think?", "cluster": 13, "x": 6.519102096557617, "y": 3.0108866691589355}, {"id": 43017832, "title": "The end of vibes-based LLM testing", "cluster": 13, "x": 6.64792013168335, "y": 3.3184139728546143}, {"id": 43017981, "title": "An experimental study on the political value shift in large language models", "cluster": 208, "x": 8.283430099487305, "y": 4.354193687438965}, {"id": 43017857, "title": "AI Space Escape: Playing Games While Evaluting LLM Reasonsing", "cluster": 12, "x": 6.985837936401367, "y": 2.712146043777466}, {"id": 43016968, "title": "The Curse of Depth in Large Language Models [pdf]", "cluster": 208, "x": 8.207073211669922, "y": 4.416016101837158}, {"id": 43016937, "title": "Healthy Competition with GCC 15 vs. LLVM Clang 20 Performance on AMD Zen 5", "cluster": 13, "x": 7.3534040451049805, "y": 3.9722509384155273}, {"id": 43016272, "title": "LLM-Powered Sorting with TrueSkill", "cluster": 13, "x": 6.995530605316162, "y": 3.6047263145446777}, {"id": 43016076, "title": "Turning Up the Heat: Min-P Sampling for Creative and Coherent LLM Outputs", "cluster": 13, "x": 6.930947780609131, "y": 3.338841199874878}, {"id": 43015631, "title": "ASTRA: HackerRank's coding benchmark for LLMs", "cluster": 13, "x": 6.70508337020874, "y": 3.5575413703918457}, {"id": 43014836, "title": "Prompting LLMs is not engineering", "cluster": 13, "x": 6.49980354309082, "y": 3.3657679557800293}, {"id": 43014918, "title": "LLMs can teach themselves to better predict the future", "cluster": 13, "x": 6.6241655349731445, "y": 3.0288069248199463}, {"id": 43014716, "title": "The Curious Similarity Between LLMs and Quantum Mechanics", "cluster": 13, "x": 6.676144599914551, "y": 3.031588315963745}, {"id": 43014401, "title": "The Hundred-Page Language Models Book", "cluster": 208, "x": 8.335522651672363, "y": 4.566836357116699}, {"id": 43014055, "title": "Resurrecting saturated LLM benchmarks with adversarial encoding", "cluster": 13, "x": 7.020847320556641, "y": 3.494715690612793}, {"id": 43013616, "title": "Training LLMs to Reason Efficiently", "cluster": 13, "x": 6.777249336242676, "y": 3.1590428352355957}, {"id": 43012093, "title": "Is It OK? (Are LLMs Morally OK?)", "cluster": 13, "x": 6.604236125946045, "y": 3.153315544128418}, {"id": 43011230, "title": "Who's optimizing LLM inference on CPUs? Any notable projects?", "cluster": 13, "x": 7.4408488273620605, "y": 3.7104358673095703}, {"id": 43009831, "title": "How I learned to stop worrying and love the LLM", "cluster": 13, "x": 6.42935848236084, "y": 3.067721128463745}, {"id": 43006033, "title": "TIL: Masked Language Models Are Surprisingly Capable Zero-Shot Learners", "cluster": 208, "x": 8.359048843383789, "y": 4.539747714996338}, {"id": 43004941, "title": "Search Query Understanding with LLMs: From Ideation to Production", "cluster": 13, "x": 6.736426830291748, "y": 3.2277133464813232}, {"id": 43004258, "title": "Designing Backgrounds with LLMs and React", "cluster": 13, "x": 6.799361705780029, "y": 3.6459834575653076}, {"id": 43004067, "title": "Ion Cannon - content collection and analysis system that uses multiple LLMs", "cluster": 13, "x": 7.081336498260498, "y": 3.586554765701294}, {"id": 43003964, "title": "LLM Hallucination Benchmark: R1, o1, o3-mini, Gemini 2.0 Flash Think Exp 01-21", "cluster": 13, "x": 7.012437343597412, "y": 3.28045654296875}, {"id": 43003754, "title": "Awesome-LLM-Judges", "cluster": 13, "x": 6.6959547996521, "y": 3.121147394180298}, {"id": 43003443, "title": "How to Backdoor Large Language Models", "cluster": 208, "x": 7.9439215660095215, "y": 4.389584541320801}, {"id": 43003081, "title": "How I learned to stop worrying and love the LLM", "cluster": 13, "x": 6.449516773223877, "y": 3.0798003673553467}, {"id": 43003082, "title": "LLM Failure Modes in Medical QA Arising from Inflexible Reasoning", "cluster": 13, "x": 6.708707809448242, "y": 3.050452947616577}, {"id": 43002599, "title": "Chatgcc \u2013 Cursed LLM-Driven \"Compiler\"", "cluster": 13, "x": 6.555805206298828, "y": 3.4316134452819824}, {"id": 43001605, "title": "DeepSeek's Inner Monologue Redefines What We Expect from Language Models", "cluster": 208, "x": 8.438446998596191, "y": 4.281283378601074}, {"id": 43001104, "title": "Self-Backtracking for Boosting Reasoning of LLMs", "cluster": 13, "x": 6.866262912750244, "y": 3.1355910301208496}, {"id": 43000704, "title": "ZebraLogic: On The Scaling Limits Of LLMs For Logical Reasoning", "cluster": 13, "x": 6.946037292480469, "y": 3.2240750789642334}, {"id": 42999461, "title": "Applying Model-agnostic DeepSeek R1-level reasoning onto any LLM", "cluster": 13, "x": 6.989999771118164, "y": 3.3245410919189453}, {"id": 42998113, "title": "Autopen: Editor with LLM generation tree exploration", "cluster": 13, "x": 6.979944229125977, "y": 3.794891834259033}, {"id": 42997340, "title": "TL;DR of Deep Dive into LLMs Like ChatGPT by Andrej Karpathy", "cluster": 13, "x": 6.6971893310546875, "y": 3.371082305908203}, {"id": 42994066, "title": "I code with LLMs these days", "cluster": 13, "x": 6.702146530151367, "y": 3.6720781326293945}, {"id": 42994143, "title": "Reasoning models are just LLMs", "cluster": 13, "x": 6.77013635635376, "y": 3.1189608573913574}, {"id": 42993822, "title": "A visual guide to Agentic Workflow Patterns", "cluster": 38, "x": 8.675926208496094, "y": 3.670422077178955}, {"id": 42993168, "title": "The Age of LLMs", "cluster": 13, "x": 6.583651542663574, "y": 3.104320764541626}, {"id": 42992336, "title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models", "cluster": 208, "x": 8.371421813964844, "y": 4.272833347320557}, {"id": 42991525, "title": "LLMjacking Targets DeepSeek", "cluster": 13, "x": 6.394309997558594, "y": 3.38763165473938}, {"id": 42991483, "title": "Using an LLM to revamp my site", "cluster": 13, "x": 6.668064117431641, "y": 3.624436616897583}, {"id": 42990236, "title": "Markdown Notes LSP Server", "cluster": 13, "x": 6.941226482391357, "y": 3.754262924194336}, {"id": 42990036, "title": "Classic Data science pipelines built with LLMs", "cluster": 13, "x": 6.9416351318359375, "y": 3.554187059402466}, {"id": 42989602, "title": "Introduction to MITRE ATT&CK utilization tools by multiple LLM agents and RAG", "cluster": 13, "x": 6.932136058807373, "y": 3.3738605976104736}, {"id": 42988717, "title": "The LLM Curve of Impact on Software Engineers", "cluster": 13, "x": 6.526270866394043, "y": 3.5156772136688232}, {"id": 42986468, "title": "LLM Basics: Embedding Spaces \u2013 Transformer Token Vectors Are Not Points in Space", "cluster": 13, "x": 6.957796096801758, "y": 3.2608065605163574}, {"id": 42985572, "title": "inferMLX: Simple Llama Model LLM Inference in macOS with MLX", "cluster": 13, "x": 7.0027337074279785, "y": 3.6933863162994385}, {"id": 42984279, "title": "How to setup your own local LLM like ChatGPT", "cluster": 13, "x": 6.726365089416504, "y": 3.7051682472229004}, {"id": 42984225, "title": "Leveraging Multimodal LLM for Inspirational User Interface Search", "cluster": 13, "x": 6.841777801513672, "y": 3.5634043216705322}, {"id": 42984219, "title": "LLM-augmented reverse engineering extension for Ghidra", "cluster": 13, "x": 7.027012825012207, "y": 3.5830891132354736}, {"id": 42983571, "title": "The LLMentalist Effect", "cluster": 13, "x": 6.710190296173096, "y": 3.055375576019287}, {"id": 42983429, "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs", "cluster": 13, "x": 6.808427333831787, "y": 3.154604911804199}, {"id": 42983082, "title": "Games for Large Language Models to play against themselves", "cluster": 208, "x": 8.1944580078125, "y": 4.446151256561279}, {"id": 42982860, "title": "LLM Rankings", "cluster": 13, "x": 6.783294200897217, "y": 3.248656988143921}, {"id": 42982812, "title": "STP: Self-Play LLM Theorem Provers with Iterative Conjecturing and Proving", "cluster": 13, "x": 6.897693157196045, "y": 3.234440326690674}, {"id": 42980350, "title": "My \"Oh Shit\" Moment for LLMs", "cluster": 13, "x": 6.505795955657959, "y": 3.0484726428985596}, {"id": 42979901, "title": "Bolt: Bootstrap long chain-of-thought in LLMs without distillation [pdf]", "cluster": 13, "x": 6.917989253997803, "y": 3.3378190994262695}, {"id": 42979986, "title": "Ghostwriter \u2013 use the reMarkable2 as an interface to vision-LLMs", "cluster": 13, "x": 6.822073459625244, "y": 3.62564754486084}, {"id": 42979455, "title": "Test-time scaling new approach: extra test-time compute improves LLM reasoning", "cluster": 13, "x": 7.093939304351807, "y": 3.376978874206543}, {"id": 42979083, "title": "Red-Teaming \u2013 to make LLMs robust and safer", "cluster": 13, "x": 6.317015171051025, "y": 3.2894110679626465}, {"id": 42978925, "title": "Strategic behavior of large language models in game-theoretic settings (2024)", "cluster": 208, "x": 8.205647468566895, "y": 4.405091285705566}, {"id": 42978481, "title": "Can LLMs do lateral thinking puzzles?", "cluster": 13, "x": 6.731237411499023, "y": 3.0843353271484375}, {"id": 42977984, "title": "Using LLMs to support Firefox developers with code review", "cluster": 13, "x": 6.838448524475098, "y": 3.7235159873962402}, {"id": 42977303, "title": "Meta LLM copyright case: Exhibit F [pdf]", "cluster": 13, "x": 7.034478664398193, "y": 3.511319637298584}, {"id": 42976614, "title": "How to Beat LinkedIn (2018)", "cluster": 13, "x": 6.909940719604492, "y": 3.4485628604888916}, {"id": 42976067, "title": "How to Install DeepSeek on Your Cloud Server with Ollama LLM", "cluster": 13, "x": 6.902176856994629, "y": 3.883357286453247}, {"id": 42975941, "title": "Predicting the Super Bowl with LLMs", "cluster": 13, "x": 6.582622528076172, "y": 3.0892860889434814}, {"id": 42975775, "title": "Find, Retrieve, Answer: A simple RAG alternative for document QA with local LLMs", "cluster": 13, "x": 6.838876724243164, "y": 3.610511302947998}, {"id": 42974556, "title": "IServe: An Intent-Based Serving System for LLMs", "cluster": 13, "x": 6.751761436462402, "y": 3.6777217388153076}, {"id": 42974449, "title": "How well does your LLM do?", "cluster": 13, "x": 6.702892780303955, "y": 3.196795701980591}, {"id": 42973462, "title": "LLMs put the free web at risk", "cluster": 13, "x": 6.530342102050781, "y": 3.3518922328948975}, {"id": 42971692, "title": "Does Temperature 0 Guarantee Deterministic LLM Outputs?", "cluster": 13, "x": 6.719404220581055, "y": 3.1639506816864014}, {"id": 42971578, "title": "The LLM Curve of Impact on Software Engineers", "cluster": 13, "x": 6.508625030517578, "y": 3.5187764167785645}, {"id": 42971214, "title": "Lasers Make Better MRAM", "cluster": 13, "x": 6.994093894958496, "y": 3.6725800037384033}, {"id": 42969964, "title": "ArchGW: Open-source, AI-native (edge and LLM) proxy for prompt traffic", "cluster": 12, "x": 7.037219047546387, "y": 2.7294349670410156}, {"id": 42969100, "title": "The LLM Curve of Impact on Software Engineers", "cluster": 13, "x": 6.527463912963867, "y": 3.5050134658813477}, {"id": 42968402, "title": "Fault Localization via Fine-Tuning LLMs with Mutation Generated Stack Traces", "cluster": 13, "x": 6.810556888580322, "y": 3.533703565597534}, {"id": 42967902, "title": "Will LLMs kill software patents?", "cluster": 13, "x": 6.524925231933594, "y": 3.4657270908355713}, {"id": 42967787, "title": "Mining LLM Pre-Training Data from Codebases", "cluster": 13, "x": 6.837671279907227, "y": 3.653885841369629}, {"id": 42967536, "title": "Secure and private connections to enterprise LLMs", "cluster": 13, "x": 6.538675785064697, "y": 3.4688432216644287}, {"id": 42966958, "title": "Why LLMs still have problems with OCR", "cluster": 13, "x": 6.572882175445557, "y": 3.39628267288208}, {"id": 42966720, "title": "Understanding Reasoning LLMs", "cluster": 13, "x": 6.836115837097168, "y": 3.1926116943359375}, {"id": 42966778, "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs", "cluster": 13, "x": 6.828425407409668, "y": 3.1317861080169678}, {"id": 42966578, "title": "Llama's Paradox \u2013 Exploiting Llama.cpp", "cluster": 13, "x": 6.3337860107421875, "y": 3.227515459060669}, {"id": 42966265, "title": "Optimizing LLM Persuasion with Personalization and Fabricated Statistics", "cluster": 13, "x": 6.943571090698242, "y": 3.220933198928833}, {"id": 42966370, "title": "Think of LLM Applications as POMDPs \u2013 Not Agents", "cluster": 13, "x": 6.751162052154541, "y": 3.201528310775757}, {"id": 42965870, "title": "JetKVM is a high-performance, open-source KVM", "cluster": 13, "x": 7.117189407348633, "y": 4.018811225891113}, {"id": 42965291, "title": "A Gentle Guide to Running a Local LLM, for Complete Beginners", "cluster": 13, "x": 6.740068435668945, "y": 3.5406477451324463}, {"id": 42964479, "title": "Is anyone aware of tools to generate Lottie animations using LLM prompts?", "cluster": 13, "x": 7.038671970367432, "y": 3.667567253112793}, {"id": 42964447, "title": "LLM Extensibility 101", "cluster": 13, "x": 6.777821063995361, "y": 3.3300070762634277}, {"id": 42962666, "title": "Screening software engineers in the LLM era", "cluster": 13, "x": 6.514624118804932, "y": 3.546795606613159}, {"id": 42962132, "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs", "cluster": 13, "x": 6.804444313049316, "y": 3.159940481185913}, {"id": 42961882, "title": "Biscuit: Scaffolding LLM-Generated Code with Ephemeral UIs in Notebooks [pdf]", "cluster": 13, "x": 6.795597553253174, "y": 3.7251272201538086}, {"id": 42961776, "title": "Framework-mania is running wild", "cluster": 38, "x": 8.680583000183105, "y": 3.5639657974243164}, {"id": 42961528, "title": "Continual Learning of Large Language Models: A Comprehensive Survey", "cluster": 208, "x": 8.220281600952148, "y": 4.410001754760742}, {"id": 42960990, "title": "Open LLM Leaderboard", "cluster": 13, "x": 6.861913204193115, "y": 3.2970776557922363}, {"id": 42960989, "title": "Pre-Trained Large Language Models Use Fourier Features for Addition (2024)", "cluster": 208, "x": 8.170306205749512, "y": 4.480781555175781}, {"id": 42960421, "title": "LLM trace visualization in Jupyter Notebooks", "cluster": 13, "x": 7.023510932922363, "y": 3.6360526084899902}, {"id": 42959987, "title": "A Short Course on How Transfomer LLMs Work, Jay Alammar and Maarten Grootendorst", "cluster": 13, "x": 6.760819911956787, "y": 3.2263412475585938}, {"id": 42959777, "title": "LLMs Were Backdoored Years Ago", "cluster": 13, "x": 6.375007629394531, "y": 3.1998379230499268}, {"id": 42959152, "title": "SmolLM2: When Smol Goes Big \u2013 Data-Centric Training of a Small Language Model", "cluster": 208, "x": 8.141914367675781, "y": 4.413387298583984}, {"id": 42958892, "title": "NLM NCBI Site experiencing heavy traffic", "cluster": 13, "x": 7.003503799438477, "y": 3.4659574031829834}, {"id": 42958687, "title": "A Gentle Intro to LLMs for Platform Engineers", "cluster": 13, "x": 6.790600776672363, "y": 3.4773173332214355}, {"id": 42957749, "title": "Ladder-residual: parallelism-aware architecture, accelerates a 70B LLM by 30%", "cluster": 13, "x": 7.258323669433594, "y": 3.8544199466705322}, {"id": 42956557, "title": "LLMs as Vector Program Databases: A New Mental Model", "cluster": 13, "x": 6.905169486999512, "y": 3.5257015228271484}, {"id": 42956000, "title": "I think about LLM prompt engineering", "cluster": 13, "x": 6.700364112854004, "y": 3.334796667098999}, {"id": 42954727, "title": "Extend: Document Processing Reimagined with LLMs", "cluster": 13, "x": 6.935799598693848, "y": 3.6118640899658203}, {"id": 42953365, "title": "Revolutionizing software testing: Introducing LLM-powered bug catchers", "cluster": 13, "x": 6.6572980880737305, "y": 3.736144781112671}, {"id": 42952914, "title": "LLM (Ladies Love Me)", "cluster": 13, "x": 6.564767837524414, "y": 3.1469995975494385}, {"id": 42952960, "title": "Andrej Karpathy: Deep Dive into LLMs Like ChatGPT [video]", "cluster": 7, "x": 6.658762454986572, "y": 3.3696582317352295}, {"id": 42952942, "title": "Masala-CHAI \u2013 An LLM for analog circuit design", "cluster": 13, "x": 6.939410209655762, "y": 3.3834009170532227}, {"id": 42952659, "title": "Effective, Enforced Documentation Control with LLMs", "cluster": 13, "x": 6.868747711181641, "y": 3.579653024673462}, {"id": 42951866, "title": "LLMs present themselves in a favorable light when taking personality tests", "cluster": 13, "x": 6.614747047424316, "y": 3.025378465652466}, {"id": 42951551, "title": "LLMs are not Junior Developers", "cluster": 13, "x": 6.525848388671875, "y": 3.3875648975372314}, {"id": 42950264, "title": "TreeGPT: A tree-based LLM interface", "cluster": 13, "x": 6.907688140869141, "y": 3.751713991165161}, {"id": 42948649, "title": "Humanify: Deobfuscate JavaScript code using LLMs", "cluster": 13, "x": 6.725986003875732, "y": 3.7971253395080566}, {"id": 42948216, "title": "Agentforce, the Salesforce Reasoning Engine", "cluster": 38, "x": 8.628894805908203, "y": 3.649712085723877}, {"id": 42947510, "title": "Understanding Reasoning LLMs", "cluster": 13, "x": 6.886941909790039, "y": 3.307117462158203}, {"id": 42945866, "title": "Drop in Chat UI in native web component for your LLM apps", "cluster": 13, "x": 6.72184419631958, "y": 3.6223347187042236}, {"id": 42944827, "title": "Three experiments in LLM code assist with RStudio and Positron", "cluster": 13, "x": 7.106557369232178, "y": 3.5155539512634277}, {"id": 42942158, "title": "LLM-as-Intern: Revisiting the Analogy", "cluster": 13, "x": 6.611112594604492, "y": 3.069039821624756}, {"id": 42941249, "title": "LLMs were backdoored years ago", "cluster": 13, "x": 6.441225528717041, "y": 3.1872739791870117}, {"id": 42939291, "title": "Over-Tokenized Transformer: Vocabulary Is Generally Worth Scaling", "cluster": 208, "x": 8.441346168518066, "y": 4.42191743850708}, {"id": 42938409, "title": "How I use LLMs as a staff engineer", "cluster": 13, "x": 6.704392910003662, "y": 3.5348010063171387}, {"id": 42937692, "title": "Creating Rich Citation Experiences with LLMs", "cluster": 13, "x": 6.638192176818848, "y": 3.4190218448638916}, {"id": 42937539, "title": "Scaling Laws for LLMs: From GPT-3 to o3", "cluster": 13, "x": 6.8221354484558105, "y": 3.257246732711792}, {"id": 42936938, "title": "Reflections on an LLM-Only Coding Project", "cluster": 13, "x": 6.6732048988342285, "y": 3.623674154281616}, {"id": 42936910, "title": "How to scale your model: A systems view of LLMs on TPUs", "cluster": 13, "x": 6.9033522605896, "y": 3.4096386432647705}, {"id": 42936788, "title": "LLM-Powered Programming: A Language Matrix Revealed", "cluster": 13, "x": 6.7500762939453125, "y": 3.6407933235168457}, {"id": 42935823, "title": "Mastering LLM Techniques: Inference Optimization", "cluster": 13, "x": 7.072192192077637, "y": 3.2865962982177734}, {"id": 42933850, "title": "\"Language Models Can Solve Engineering Optimization Problems\"", "cluster": 208, "x": 8.161555290222168, "y": 4.4234843254089355}, {"id": 42933256, "title": "DoppelBot: Replace Your CEO with an LLM", "cluster": 13, "x": 6.507932186126709, "y": 3.159684896469116}, {"id": 42932948, "title": "DeepRAG: Thinking to retrieval step by step for large language models", "cluster": 208, "x": 8.19030475616455, "y": 4.583582878112793}, {"id": 42931671, "title": "Levelized Cost of Load Coverage (Lcolc)", "cluster": 13, "x": 6.921483516693115, "y": 3.512754201889038}, {"id": 42931220, "title": "Model Soups \u2013 How to Increase Accuracy of Fine Tuned LLMs?", "cluster": 13, "x": 6.993945598602295, "y": 3.3037219047546387}, {"id": 42931011, "title": "Building games with LLMs to help my kid learn math", "cluster": 13, "x": 6.823975563049316, "y": 3.262094497680664}, {"id": 42930229, "title": "LLM composes logic programs which call on LLM to compose more logic programs ...", "cluster": 13, "x": 6.863424301147461, "y": 3.467437505722046}, {"id": 42929523, "title": "Advantages of Generating Clojure with LLMs", "cluster": 13, "x": 6.88187837600708, "y": 3.679372787475586}, {"id": 42926528, "title": "Can Large Language Models Emulate Judicial Decision-Making? [Paper]", "cluster": 208, "x": 8.340991020202637, "y": 4.308548927307129}, {"id": 42922989, "title": "Open Euro LLM: Open LLMs for Transparent AI in Europe", "cluster": 12, "x": 7.044795036315918, "y": 3.082334041595459}, {"id": 42922893, "title": "Craylm: Open-source unified LLM training and inference for R1", "cluster": 13, "x": 6.905922889709473, "y": 3.5958831310272217}, {"id": 42922529, "title": "Step-Game: Assessing LLM Collaboration and Deception Under Pressure", "cluster": 13, "x": 6.760032653808594, "y": 3.048336982727051}, {"id": 42922014, "title": "The Case Against ORMs", "cluster": 13, "x": 6.608589172363281, "y": 3.129838228225708}, {"id": 42920522, "title": "A Visual Guide to Reasoning LLMs", "cluster": 13, "x": 6.9005231857299805, "y": 3.330498456954956}, {"id": 42920012, "title": "Building games with LLMs to help my kid learn math", "cluster": 13, "x": 6.78622579574585, "y": 3.2174394130706787}, {"id": 42919757, "title": "Remember robots.txt? Now we have llms.txt", "cluster": 13, "x": 6.619119167327881, "y": 3.555703639984131}, {"id": 42919557, "title": "Assistants Powered by Multiple LLMs in Tandem (WilmerAI)", "cluster": 13, "x": 6.730929374694824, "y": 3.1351521015167236}, {"id": 42919421, "title": "A professional workflow for translation using LLMs", "cluster": 13, "x": 6.839929580688477, "y": 3.589714765548706}, {"id": 42917872, "title": "Ichigo Bot \u2013 Telegram Chat Bot for Aggregating LLMs and API Providers", "cluster": 70, "x": 6.711617946624756, "y": 3.551875352859497}, {"id": 42917116, "title": "Adaptive Classification for Automatic LLM Temperature Optimization", "cluster": 13, "x": 6.998944282531738, "y": 3.3628766536712646}, {"id": 42916708, "title": "Flashlearn \u2013 Use LLMs with a simple fit/predict flow", "cluster": 13, "x": 6.806725978851318, "y": 3.3452794551849365}, {"id": 42916646, "title": "How to think about LLM Model Size", "cluster": 13, "x": 6.835108757019043, "y": 3.2576799392700195}, {"id": 42915980, "title": "European AI Alliance Unveils LLM Alternative to Silicon Valley and DeepSeek", "cluster": 12, "x": 7.016119003295898, "y": 2.630934000015259}, {"id": 42915460, "title": "The tug-of-war between cache and capacity: from MHA, MQA, GQA to MLA", "cluster": 208, "x": 8.212522506713867, "y": 4.456526279449463}, {"id": 42913148, "title": "Scripts to help setup LLMs tools easily from FOSDEM", "cluster": 13, "x": 6.908745288848877, "y": 3.773205518722534}, {"id": 42911463, "title": "LLMs struggle with perception, not reasoning, in ARC-AGI", "cluster": 13, "x": 6.799361705780029, "y": 3.1649529933929443}, {"id": 42909616, "title": "New LLM compression method lets you compress models upto 60% of original size", "cluster": 13, "x": 6.981064319610596, "y": 3.5270731449127197}, {"id": 42908599, "title": "Share a Secret Code Word to Bypass LLM Censorship", "cluster": 13, "x": 6.554923057556152, "y": 3.5301928520202637}, {"id": 42908334, "title": "The Engine \u2013 Jonathan Swift (Sort of) Predicts Large Language Models in 1726", "cluster": 208, "x": 8.390059471130371, "y": 4.276360034942627}, {"id": 42907239, "title": "Calculate the number of language model tokens for a string", "cluster": 208, "x": 8.279927253723145, "y": 4.572775363922119}, {"id": 42905758, "title": "LLMs: Harmful to Technical Innovation?", "cluster": 13, "x": 6.501181602478027, "y": 3.407193422317505}, {"id": 42905453, "title": "Recent results show that LLMs struggle with compositional tasks", "cluster": 13, "x": 6.697752475738525, "y": 3.0752694606781006}, {"id": 42905406, "title": "DeepSeek R1's recipe to replicate o1 and the future of reasoning LMs", "cluster": 13, "x": 7.001254081726074, "y": 3.2907555103302}, {"id": 42903073, "title": "Do LLMs spell the end for programming language innovation?", "cluster": 13, "x": 6.612428188323975, "y": 3.6123158931732178}, {"id": 42903146, "title": "Proof \u2013 \"Language Modeling Is Compression\"", "cluster": 208, "x": 8.206053733825684, "y": 4.553631782531738}, {"id": 42899486, "title": "Can LLMs make trade-offs involving stipulated pain and pleasure states?", "cluster": 13, "x": 6.674832820892334, "y": 3.0400781631469727}, {"id": 42899184, "title": "Large Language Models for Mathematicians (2023)", "cluster": 208, "x": 8.216355323791504, "y": 4.45424747467041}, {"id": 42898240, "title": "Modern languages and bad packaging outcomes at scale", "cluster": 208, "x": 8.212517738342285, "y": 4.454638481140137}, {"id": 42898113, "title": "PyComet \u2013 Git commit messages generator using multiple LLM APIs", "cluster": 13, "x": 6.959942817687988, "y": 3.751866102218628}, {"id": 42896863, "title": "LLM in Litecli for SQLite", "cluster": 13, "x": 6.829453945159912, "y": 3.708061695098877}, {"id": 42896788, "title": "DeepSeek's Refusals: How Do the Guardrails of Language Models Compare?", "cluster": 208, "x": 8.397263526916504, "y": 4.333740711212158}, {"id": 42896382, "title": "Building MCP Agentic Systems from Scratch with Go, VertexAI and Gemini", "cluster": 37, "x": 8.514269828796387, "y": 3.75449800491333}, {"id": 42895495, "title": "Soft Prompt Tuning Modern LLMs", "cluster": 13, "x": 6.868422985076904, "y": 3.4483208656311035}, {"id": 42891476, "title": "AntiSlop Sampler for LLM Inference", "cluster": 13, "x": 6.9328155517578125, "y": 3.3452630043029785}, {"id": 42889702, "title": "DeepSeek R1: Don't Put All Your Eggs in One LLM Basket", "cluster": 13, "x": 6.601961135864258, "y": 3.186234474182129}, {"id": 42889301, "title": "BAML: Bringing Software Engineering Rigor to LLM Development", "cluster": 13, "x": 6.595338821411133, "y": 3.5819694995880127}, {"id": 42889052, "title": "Large language models think too fast to explore effectively", "cluster": 208, "x": 8.216511726379395, "y": 4.349587440490723}, {"id": 42888861, "title": "BAML \u2013 a domain-specific language to generate structured outputs from LLMs", "cluster": 13, "x": 6.896844863891602, "y": 3.6934027671813965}, {"id": 42888804, "title": "Building MCP Agentic Systems from Scratch with Go, VertexAI and Gemini", "cluster": 37, "x": 8.506528854370117, "y": 3.747283458709717}, {"id": 42887916, "title": "Dead-simple Neovim Coding Assistant in 400 Lines of Lua", "cluster": 13, "x": 6.932150840759277, "y": 4.006593227386475}, {"id": 42887610, "title": "Can someone create an LLM standard?", "cluster": 13, "x": 6.768355846405029, "y": 3.4661524295806885}, {"id": 42886877, "title": "GNU Artanis LLM Plugin", "cluster": 13, "x": 6.941720485687256, "y": 3.804926633834839}, {"id": 42886982, "title": "vLLM V1: A Major Upgrade to vLLM's Core Architecture", "cluster": 13, "x": 7.173398494720459, "y": 4.109246730804443}, {"id": 42886971, "title": "Thoughts Are All over the Place: On the Underthinking of O1-Like LLMs", "cluster": 13, "x": 6.6156439781188965, "y": 3.0248773097991943}, {"id": 42886521, "title": "Learning to Plan and Reason for Evaluation with Thinking-LLM-as-a-Judge", "cluster": 13, "x": 6.727505207061768, "y": 3.1800224781036377}, {"id": 42885332, "title": "Changing the language of an LLM response with a vector addition", "cluster": 13, "x": 6.823646068572998, "y": 3.4652199745178223}, {"id": 42883727, "title": "The Power of Negative Zero: Datatype Customization for Quantized LLMs", "cluster": 13, "x": 6.891827583312988, "y": 3.34419584274292}, {"id": 42883211, "title": "LLMs in Production", "cluster": 13, "x": 6.7188568115234375, "y": 3.4335756301879883}, {"id": 42882041, "title": "JPEG compress your LLM weights", "cluster": 13, "x": 6.966856002807617, "y": 3.580947160720825}, {"id": 42880885, "title": "Cloudflare's /Llms.txt", "cluster": 13, "x": 6.793002605438232, "y": 3.665565252304077}, {"id": 42881008, "title": "Virus: Harmful Fine-Tuning Attack for LLMs Bypassing Guardrail Moderation", "cluster": 601, "x": 6.423323154449463, "y": 3.4639816284179688}, {"id": 42880681, "title": "DeepSeek's not the only Chinese LLM OpenAI have to worry about. Right, Alibaba?", "cluster": 13, "x": 6.604007244110107, "y": 3.291581392288208}, {"id": 42880214, "title": "Qwen2.5-VL: State-of-the-art multimodal LLM", "cluster": 13, "x": 6.940295219421387, "y": 3.817769765853882}, {"id": 42879673, "title": "Over-Tokenized Transformer: Vocabulary Is Worth Scaling [pdf]", "cluster": 208, "x": 8.447568893432617, "y": 4.457894325256348}, {"id": 42879177, "title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation", "cluster": 208, "x": 8.504644393920898, "y": 4.287802696228027}, {"id": 42878418, "title": "DeepSeek and ChatGPT: Censorship in LLMs", "cluster": 13, "x": 6.544125556945801, "y": 3.4451098442077637}, {"id": 42877140, "title": "Exploring User Privacy in Ollama: Are Local LLMs Private?", "cluster": 13, "x": 6.5427021980285645, "y": 3.5040059089660645}, {"id": 42874776, "title": "Building a Real-Time Context Engine for Our Code LLM: No More 10-Minute Delays", "cluster": 13, "x": 6.835268974304199, "y": 3.8316733837127686}, {"id": 42874077, "title": "Why traditional product analytics are simply not working for LLM apps", "cluster": 13, "x": 6.741684436798096, "y": 3.294433832168579}, {"id": 42873890, "title": "Run LLMs Locally \u2013 How To", "cluster": 13, "x": 6.821845054626465, "y": 3.6445295810699463}, {"id": 42873168, "title": "Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?", "cluster": 13, "x": 6.726253986358643, "y": 3.075813055038452}, {"id": 42872394, "title": "LLM finetuned for generating symbolic music", "cluster": 13, "x": 6.881949424743652, "y": 3.583197593688965}, {"id": 42870875, "title": "G-Eval for LLM Evaluation", "cluster": 13, "x": 6.881679534912109, "y": 3.239243984222412}, {"id": 42870758, "title": "LiteCLI (a friendly SQLite client) now with LLM abilities", "cluster": 13, "x": 6.811722755432129, "y": 3.705396890640259}, {"id": 42870523, "title": "FP8 DeepSeek R1 Distilled LLMs for SGLang and VLLM", "cluster": 13, "x": 7.015398025512695, "y": 3.8088150024414062}, {"id": 42868770, "title": "SmolGPT: A minimal PyTorch implementation for training a small LLM from scratch", "cluster": 13, "x": 6.957517147064209, "y": 3.498047113418579}, {"id": 42868453, "title": "Googlebot Is Using a New and Undocumented Desktop User-Agent", "cluster": 35, "x": 8.430413246154785, "y": 3.791219711303711}, {"id": 42867944, "title": "Community of LLM generated webpages on demand", "cluster": 13, "x": 6.73931884765625, "y": 3.6871302127838135}, {"id": 42867278, "title": "Large Language Model Training Using FP4 Quantization", "cluster": 208, "x": 8.217422485351562, "y": 4.493268013000488}, {"id": 42866175, "title": "Are LLMs making StackOverflow irrelevant?", "cluster": 13, "x": 6.5756025314331055, "y": 3.319662094116211}, {"id": 42861815, "title": "Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting", "cluster": 13, "x": 6.743162155151367, "y": 3.4465770721435547}, {"id": 42861154, "title": "What does DeepSeek R1 and v3 mean for LLM data?", "cluster": 13, "x": 6.948254585266113, "y": 3.4167497158050537}, {"id": 42860956, "title": "Sento \u2013 Actor framework featuring actors and agents", "cluster": 37, "x": 8.523714065551758, "y": 3.7710185050964355}, {"id": 42860770, "title": "$6k to run full DeepSeek R1 LLM with reasoning at home", "cluster": 13, "x": 7.0153093338012695, "y": 3.5006158351898193}, {"id": 42860595, "title": "LLMs misunderstand some people more than others", "cluster": 13, "x": 6.562293529510498, "y": 3.038344383239746}, {"id": 42858518, "title": "Avoiding Mocks: Testing LLM Applications with LangChain in Django", "cluster": 13, "x": 6.827299118041992, "y": 3.4840705394744873}, {"id": 42857479, "title": "The Next User Agent", "cluster": 37, "x": 8.482552528381348, "y": 3.836927890777588}, {"id": 42857028, "title": "DeepSeekMath: Pushing the limits for mathematical reasoning in Open LLMs", "cluster": 13, "x": 6.981509208679199, "y": 3.3278236389160156}, {"id": 42855667, "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with LLMs", "cluster": 13, "x": 7.01335334777832, "y": 3.282917022705078}, {"id": 42855507, "title": "LGTM: Designing a Great Developer Experience", "cluster": 13, "x": 6.950798988342285, "y": 3.998749017715454}, {"id": 42854483, "title": "Mastering LLM Techniques: Inference Optimization", "cluster": 13, "x": 7.066559314727783, "y": 3.285933494567871}, {"id": 42854244, "title": "Mixture-of-Experts (Moe) LLMs", "cluster": 13, "x": 6.667907238006592, "y": 3.195359468460083}, {"id": 42852693, "title": "Dutch LLM Taken Down Following Legal Pressure from Anti-Piracy Group", "cluster": 13, "x": 6.518533706665039, "y": 3.3309695720672607}, {"id": 42852261, "title": "LLVM 20 Promotes SPIR-V to Official Back End, Enabled by Default", "cluster": 13, "x": 6.989921569824219, "y": 3.964367628097534}, {"id": 42850955, "title": "Posthog LLM Observability in Open Beta", "cluster": 13, "x": 6.869900703430176, "y": 3.5899667739868164}, {"id": 42850979, "title": "Reduce your LLM agent costs by 90% with structure-preserving HTML compression", "cluster": 13, "x": 6.906601905822754, "y": 3.6278579235076904}, {"id": 42849909, "title": "Lunal.ai \u2013 Software that pays websites when LLMs use their content", "cluster": 12, "x": 7.040067195892334, "y": 2.724449872970581}, {"id": 42849125, "title": "Making LLM workflows human friendly", "cluster": 13, "x": 6.760222911834717, "y": 3.534210443496704}, {"id": 42848863, "title": "Large Language Model Systems CMU 11868, Spring 2024", "cluster": 208, "x": 8.179035186767578, "y": 4.576225757598877}, {"id": 42848452, "title": "Multi Boxing LLMs", "cluster": 13, "x": 6.692152500152588, "y": 3.1945481300354004}, {"id": 42848220, "title": "The LLM code editor with DeepSeek-R1 support", "cluster": 13, "x": 6.8607282638549805, "y": 3.8008601665496826}, {"id": 42847916, "title": "LLM Judges", "cluster": 13, "x": 6.744748115539551, "y": 3.2531416416168213}, {"id": 42845542, "title": "Why Social Media Has Captured Our Attention and How We'll Get It Back with LLMs", "cluster": 13, "x": 6.614923000335693, "y": 3.085272789001465}, {"id": 42845146, "title": "TIL Llms.txt for the Web", "cluster": 13, "x": 6.798679351806641, "y": 3.7209556102752686}, {"id": 42844685, "title": "LLMs APIs You Can Use for Free", "cluster": 13, "x": 6.916620254516602, "y": 3.7723398208618164}, {"id": 42844059, "title": "vLLM V1: A Major Upgrade to vLLM's Core Architecture", "cluster": 13, "x": 7.181118011474609, "y": 4.129294395446777}, {"id": 42842861, "title": "Can I run this LLM? Check if LLM will work on your hardware", "cluster": 13, "x": 6.862069129943848, "y": 3.7035207748413086}, {"id": 42841130, "title": "want to build Claim Processing with agents . is it possible?", "cluster": 38, "x": 8.666659355163574, "y": 3.544635534286499}, {"id": 42841132, "title": "Translation using deep neural networks (part 1)", "cluster": 208, "x": 8.405689239501953, "y": 4.386537075042725}, {"id": 42839809, "title": "Mixture-of-Experts (MoE) LLMs", "cluster": 13, "x": 6.659091949462891, "y": 3.213102102279663}, {"id": 42839398, "title": "The Traps in Meta's Llama License", "cluster": 13, "x": 6.443447589874268, "y": 3.2382912635803223}, {"id": 42839261, "title": "Understanding Modern LLMs via DeepSeek", "cluster": 13, "x": 6.896343231201172, "y": 3.3560614585876465}, {"id": 42838744, "title": "Search startup jobs with Python and LLMs", "cluster": 13, "x": 6.856308937072754, "y": 3.6765100955963135}, {"id": 42838506, "title": "The Agent Company: Benchmarking LLM Agents on Consequential Real World Tasks", "cluster": 13, "x": 6.948531150817871, "y": 3.1498472690582275}, {"id": 42838547, "title": "Lessons from 27 months building LLM coding agents", "cluster": 13, "x": 6.70937967300415, "y": 3.541231870651245}, {"id": 42838369, "title": "A new platform for learning LLM's risks", "cluster": 13, "x": 6.735440254211426, "y": 3.2016000747680664}, {"id": 42836342, "title": "High Performance and Easy Deployment of VLLM in K8s", "cluster": 13, "x": 7.079514980316162, "y": 3.914003372192383}, {"id": 42832838, "title": "Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M", "cluster": 13, "x": 6.973783016204834, "y": 3.740384578704834}, {"id": 42832429, "title": "Phison aiDAPTIV+ leverages SSDs to expand GPU memory for LLM training", "cluster": 13, "x": 7.184718608856201, "y": 3.5982472896575928}, {"id": 42830375, "title": "Are LLMs making StackOverflow irrelevant?", "cluster": 13, "x": 6.537723541259766, "y": 3.3116724491119385}, {"id": 42829738, "title": "Highlighting Parts of Lua as Bash", "cluster": 13, "x": 6.965386390686035, "y": 4.149470806121826}, {"id": 42829382, "title": "Simulating 500M years of evolution with a language model", "cluster": 208, "x": 8.483421325683594, "y": 4.199697494506836}, {"id": 42828235, "title": "LLMs and Unix Timestamp", "cluster": 13, "x": 6.953115463256836, "y": 3.7498738765716553}, {"id": 42828206, "title": "Full multimodal Android llm app running without netowrk", "cluster": 13, "x": 6.879684925079346, "y": 3.7974698543548584}, {"id": 42826430, "title": "Turning my laptop into a Search Relevance Judge with local LLMs", "cluster": 13, "x": 6.82056999206543, "y": 3.6024892330169678}, {"id": 42826456, "title": "Local LLMs as Search Judges: Cost-Effective Relevance Evaluation", "cluster": 13, "x": 6.814051628112793, "y": 3.331927537918091}, {"id": 42826607, "title": "Kimi K1.5: Scaling Reinforcement Learning with LLMs", "cluster": 13, "x": 7.034525394439697, "y": 3.32696533203125}, {"id": 42826239, "title": "Advancing Language Model Reasoning Through RL and Inference Scaling", "cluster": 208, "x": 8.38579273223877, "y": 4.326406478881836}, {"id": 42824314, "title": "Evaluation of LLMs accuracy and consistency in the registered dietitian exam", "cluster": 13, "x": 6.886931419372559, "y": 3.235259532928467}, {"id": 42824370, "title": "LLM RAM Calculator", "cluster": 13, "x": 6.990262031555176, "y": 3.6234705448150635}, {"id": 42823568, "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL", "cluster": 13, "x": 7.00991153717041, "y": 3.302875280380249}, {"id": 42822264, "title": "AlphaSharpe: LLM-Driven \"Discovery\" of Robust Risk-Adjusted Metrics", "cluster": 13, "x": 6.953223705291748, "y": 3.265781879425049}, {"id": 42821180, "title": "Self-adaptive LLM dynamically adjusts its weights to learn new tasks", "cluster": 13, "x": 6.908261775970459, "y": 3.299264907836914}, {"id": 42817572, "title": "Fast-Forwarding LCGs (2016)", "cluster": 13, "x": 7.370981693267822, "y": 4.306477069854736}, {"id": 42817418, "title": "Run open-source LLMs on everyday hardware, a hacking guide", "cluster": 13, "x": 6.860848903656006, "y": 3.747663736343384}, {"id": 42816487, "title": "Companies Need to Offer Current Documentation in a Single Document for LLMs", "cluster": 13, "x": 6.8643412590026855, "y": 3.5973172187805176}, {"id": 42815497, "title": "Tell me about yourself: LLMs are aware of their learned behaviors", "cluster": 13, "x": 6.562482833862305, "y": 3.0161972045898438}, {"id": 42814900, "title": "Chain of Agents: Large language models collaborating on long-context tasks", "cluster": 208, "x": 8.18574047088623, "y": 4.283761024475098}, {"id": 42813766, "title": "`llmhelp` bash function to assist on infra work", "cluster": 13, "x": 6.856791019439697, "y": 3.7354774475097656}, {"id": 42812886, "title": "Local LLM-assisted text completion extension for VS Code", "cluster": 13, "x": 6.976170539855957, "y": 4.029092788696289}, {"id": 42812909, "title": "\"Lagom\" in Sweden \u2013 What It Means and Where It Comes from (2020)", "cluster": 13, "x": 6.819219589233398, "y": 3.5454509258270264}, {"id": 42812947, "title": "Notte Launched the Agentic Internet", "cluster": 33, "x": 8.612287521362305, "y": 3.9188735485076904}, {"id": 42812054, "title": "LLDB for Zig", "cluster": 13, "x": 7.19088077545166, "y": 3.810641050338745}, {"id": 42811773, "title": "Why no one talks about the MS clarity NPM package", "cluster": 13, "x": 6.878298759460449, "y": 3.624305486679077}, {"id": 42811545, "title": "Should you write differently for AI or LLMs? Nope", "cluster": 12, "x": 6.978137493133545, "y": 2.6329617500305176}, {"id": 42810199, "title": "Tell LLMs to talk like Elmer Fudd so you know when they stop paying attention", "cluster": 13, "x": 6.496252536773682, "y": 3.1142752170562744}, {"id": 42810176, "title": "The State of Vim", "cluster": 13, "x": 6.891088008880615, "y": 3.1968233585357666}, {"id": 42809241, "title": "Measuring Political Preferences/Biases in LLMs", "cluster": 13, "x": 6.674256324768066, "y": 3.0286216735839844}, {"id": 42809061, "title": "LM arena public voting is not objective for LLM evaluation", "cluster": 13, "x": 6.696728706359863, "y": 3.1366209983825684}, {"id": 42808313, "title": "5090 LLM Benchmark Results", "cluster": 13, "x": 6.972010612487793, "y": 3.372957468032837}, {"id": 42807826, "title": "Attention Sinks in LLMs for endless fluency", "cluster": 13, "x": 6.691969394683838, "y": 3.1588499546051025}, {"id": 42807537, "title": "Tell me about yourself: LLMs are aware of their learned behaviors", "cluster": 13, "x": 6.594657897949219, "y": 3.0477263927459717}, {"id": 42807007, "title": "Mastering LLM Techniques: Inference Optimization", "cluster": 13, "x": 7.061774730682373, "y": 3.2843263149261475}, {"id": 42806328, "title": "Llama.vim \u2013 Local LLM-assisted text completion", "cluster": 13, "x": 6.916140079498291, "y": 3.7656264305114746}, {"id": 42805858, "title": "A platform for the biomedical application of large language models", "cluster": 208, "x": 8.245979309082031, "y": 4.343295097351074}, {"id": 42805687, "title": "Programming Languages After LLMs", "cluster": 13, "x": 6.683270454406738, "y": 3.599026918411255}, {"id": 42805460, "title": "How do large language models get so large?", "cluster": 208, "x": 8.20463752746582, "y": 4.376693248748779}, {"id": 42804320, "title": "The chess puzzle for every LLM", "cluster": 13, "x": 6.7752251625061035, "y": 3.1396470069885254}, {"id": 42803984, "title": "The smallest VLM ever: 250M parameters", "cluster": 13, "x": 7.0984320640563965, "y": 3.8057196140289307}, {"id": 42803448, "title": "Cellm: Use LLMs in Excel Formulas", "cluster": 13, "x": 6.7745137214660645, "y": 3.2955119609832764}, {"id": 42802108, "title": "Combining LLM Embeddings with Omic Features for Classification in Parkinson's", "cluster": 13, "x": 7.025873184204102, "y": 3.41707444190979}, {"id": 42802056, "title": "Evaluating SotA LLM Models trying to solve a net-new LeetCode style puzzle", "cluster": 13, "x": 7.036476135253906, "y": 3.341623544692993}, {"id": 42801442, "title": "OpenRouter: A Unified Interface for LLMs", "cluster": 13, "x": 6.868480205535889, "y": 3.7386550903320312}, {"id": 42801347, "title": "LLM: Access large language models from the command-line", "cluster": 13, "x": 7.093520164489746, "y": 3.931684732437134}, {"id": 42800891, "title": "LLM desktop client \u2013 Cherry Studio", "cluster": 13, "x": 6.782642364501953, "y": 3.666261672973633}, {"id": 42800555, "title": "Chatbox: User-Friendly Desktop Client App for AI Models/LLMs", "cluster": 12, "x": 7.038185119628906, "y": 2.664724588394165}, {"id": 42799629, "title": "Foundations of Large Language Models", "cluster": 208, "x": 8.167130470275879, "y": 4.371173858642578}, {"id": 42796240, "title": "Feeling deceived when receiving (non-labeled) LLM-generated messages", "cluster": 13, "x": 6.47707986831665, "y": 3.2612464427948}, {"id": 42796269, "title": "Can LLMs make robots smarter?", "cluster": 12, "x": 6.95359992980957, "y": 2.746751308441162}, {"id": 42796385, "title": "Recursive Self-Modeling for Real-Time Introspection in LLMs", "cluster": 13, "x": 6.786569595336914, "y": 3.2305450439453125}, {"id": 42796296, "title": "A tricky LLM test that only DeepSeek R1 got right", "cluster": 13, "x": 6.807122707366943, "y": 3.109865188598633}, {"id": 42796274, "title": "LLM-first code: rethinking code design and putting bots first", "cluster": 13, "x": 6.835853576660156, "y": 3.635385751724243}, {"id": 42795971, "title": "Check twice, cut once with LLM search relevance eval", "cluster": 13, "x": 6.865686893463135, "y": 3.330003261566162}, {"id": 42795790, "title": "Let's Build LLM Judges with Structured Generation", "cluster": 13, "x": 6.834995269775391, "y": 3.3057422637939453}, {"id": 42795658, "title": "Promptmap: A prompt injection scanner for LLM applications", "cluster": 13, "x": 6.916896820068359, "y": 3.685669422149658}, {"id": 42795488, "title": "I ask this chess puzzle to every new LLM", "cluster": 13, "x": 6.736385345458984, "y": 3.1239569187164307}, {"id": 42795546, "title": "Classic ML to Cope with Dumb LLM Judges", "cluster": 13, "x": 6.600574016571045, "y": 3.2037320137023926}, {"id": 42795367, "title": "LLMs will change the UX of Business Apps", "cluster": 13, "x": 6.729521751403809, "y": 3.528968572616577}, {"id": 42795309, "title": "DeepSeek LLM supports Chinese propaganda", "cluster": 13, "x": 6.54330587387085, "y": 3.341240882873535}, {"id": 42794701, "title": "Maternal Healthcare: Training Language Models to Identify Urgent Messages", "cluster": 208, "x": 7.967837810516357, "y": 4.113185405731201}, {"id": 42794431, "title": "LLM training written in Rust, 30 times better than C implementation", "cluster": 13, "x": 6.803067684173584, "y": 3.867711305618286}, {"id": 42794359, "title": "Multi-Agent Step Race Benchmark: LLM Collaboration and Deception Under Pressure", "cluster": 13, "x": 6.822503566741943, "y": 3.1151113510131836}, {"id": 42793739, "title": "Global-batch load balance almost free launch to improve your MoE LLM training", "cluster": 13, "x": 6.902606010437012, "y": 3.5840563774108887}, {"id": 42793703, "title": "Anthropic MCP and Eunomia = Standard Data Governance for LLM-Based Applicaitons", "cluster": 13, "x": 6.922973155975342, "y": 3.5525057315826416}, {"id": 42793232, "title": "Phi-4 as LLM-as-Judge rivals specialized evaluator models wo/ finetuning", "cluster": 13, "x": 6.829835891723633, "y": 3.245488405227661}, {"id": 42792865, "title": "Evolving Deeper LLM Thinking", "cluster": 13, "x": 6.588226795196533, "y": 2.9959237575531006}, {"id": 42792112, "title": "LLVM Lands Initial Support for IBM SystemZ \"Arch15\" Target: IBM Z17 / Telum II", "cluster": 13, "x": 6.9245710372924805, "y": 3.7578511238098145}, {"id": 42790916, "title": "Are LLMs making StackOverflow irrelevant?", "cluster": 13, "x": 6.562480926513672, "y": 3.2862138748168945}, {"id": 42790820, "title": "Coping with dumb LLMs using classic ML", "cluster": 13, "x": 6.545382976531982, "y": 3.247715950012207}, {"id": 42789802, "title": "Jlama: LLM Inference Engine for Java", "cluster": 13, "x": 7.008254528045654, "y": 3.559084177017212}, {"id": 42789323, "title": "Chatbox: Cross-platform desktop client for ChatGPT, Claude and other LLMs", "cluster": 13, "x": 6.811615467071533, "y": 3.855497360229492}, {"id": 42788668, "title": "LLM Catcher \u2013 Python library that teams up with LLMs to decode exceptions", "cluster": 13, "x": 6.859714508056641, "y": 3.778862237930298}, {"id": 42787082, "title": "Chat with private and local large language models", "cluster": 70, "x": 8.064355850219727, "y": 4.558892250061035}, {"id": 42786751, "title": "LangTools.swift \u2013 Abstractions Around LLM APIs for Swift Devs", "cluster": 13, "x": 6.889857769012451, "y": 3.7830746173858643}, {"id": 42785777, "title": "Gyms for All the Skills That LLMs Are Eating?", "cluster": 13, "x": 6.5678486824035645, "y": 3.082515239715576}, {"id": 42785335, "title": "LLMs Help Humans Verify Truthfulness \u2013 Except When They Are Convincingly Wrong", "cluster": 13, "x": 6.630981922149658, "y": 3.0284974575042725}, {"id": 42784145, "title": "Invariant Agent Stack: A framework-less approach to robust agent development", "cluster": 38, "x": 8.547019958496094, "y": 3.7177813053131104}, {"id": 42783692, "title": "Programming Languages After LLMs", "cluster": 13, "x": 6.646839141845703, "y": 3.6711790561676025}, {"id": 42783527, "title": "Are LLMs making StackOverflow irrelevant?", "cluster": 13, "x": 6.562790393829346, "y": 3.286592960357666}, {"id": 42783341, "title": "Are LLMs making StackOverflow irrelevant?", "cluster": 13, "x": 6.528034210205078, "y": 3.25351619720459}, {"id": 42782969, "title": "Lessons Learned Writing LLMap", "cluster": 13, "x": 6.840498447418213, "y": 3.3829538822174072}, {"id": 42782872, "title": "Tilde, My LLVM Alternative", "cluster": 13, "x": 6.698691368103027, "y": 3.4958653450012207}, {"id": 42779985, "title": "Improving Code Quality with AST Grep and LLMs", "cluster": 13, "x": 6.814094543457031, "y": 3.711108922958374}, {"id": 42779711, "title": "Beyond \"Prompt and Pray\": Reliable LLM-powered Sofware in an Agentic World", "cluster": 13, "x": 6.869202136993408, "y": 3.3582465648651123}, {"id": 42778262, "title": "Simulating 500M years of evolution with a language model", "cluster": 208, "x": 8.484442710876465, "y": 4.19703483581543}, {"id": 42777857, "title": "Kimi K1.5: Scaling Reinforcement Learning with LLMs", "cluster": 13, "x": 7.028429985046387, "y": 3.3293511867523193}, {"id": 42772453, "title": "Teaching a machine to read, how LLM's comprehend text", "cluster": 13, "x": 6.880762577056885, "y": 3.1716651916503906}, {"id": 42772202, "title": "Llama2.c LLM on Vintage Sega Dreamcast Cluster", "cluster": 13, "x": 6.979560375213623, "y": 3.7772915363311768}, {"id": 42768063, "title": "We Evaluate AI Models and LLMs for GitHub Copilot \u2013 The GitHub Blog", "cluster": 12, "x": 7.1006760597229, "y": 2.655376672744751}, {"id": 42767632, "title": "PaSa: An LLM Agent for Comprehensive Academic Paper Search", "cluster": 13, "x": 6.883949279785156, "y": 3.5293216705322266}, {"id": 42766939, "title": "Controlling Blender with my voice using LLM", "cluster": 13, "x": 6.763735294342041, "y": 3.5427730083465576}, {"id": 42764969, "title": "Evolving Deeper LLM Thinking", "cluster": 13, "x": 6.514254570007324, "y": 2.921445369720459}, {"id": 42763456, "title": "AgentEval: A Framework for Evaluating LLM Applications", "cluster": 13, "x": 6.9652533531188965, "y": 3.137876510620117}, {"id": 42763354, "title": "Enhancing Chat Language Models: Scaling High-Quality Instructional Conversations", "cluster": 208, "x": 8.151043891906738, "y": 4.488567352294922}, {"id": 42760852, "title": "Rethinking Software Delivery: LLMs as Dynamic Computational Environments", "cluster": 13, "x": 6.59727668762207, "y": 3.4981656074523926}, {"id": 42759546, "title": "Zuckerberg appeared to know Llama trained on Libgen", "cluster": 13, "x": 6.648566246032715, "y": 3.4083805084228516}, {"id": 42753302, "title": "Yek: Serialize your code repo (or part of it) to feed into any LLM", "cluster": 13, "x": 6.7701096534729, "y": 3.8581902980804443}, {"id": 42750286, "title": "TIL you can ls and run GitHub workflows using their CLI", "cluster": 13, "x": 6.8279876708984375, "y": 3.825436592102051}, {"id": 42748826, "title": "Run LLM's in the Browser", "cluster": 13, "x": 6.720134258270264, "y": 3.7031502723693848}, {"id": 42748274, "title": "Simulating 500M years of evolution with a language model", "cluster": 208, "x": 8.484345436096191, "y": 4.1953277587890625}, {"id": 42747832, "title": "Building a Socially-Adept LLM Agent", "cluster": 13, "x": 6.833744525909424, "y": 3.269709825515747}, {"id": 42744970, "title": "Large Language Model Battle Arena", "cluster": 208, "x": 8.177545547485352, "y": 4.411925315856934}, {"id": 42744441, "title": "Do Code LLMs Understand Design Patterns?", "cluster": 13, "x": 6.804335117340088, "y": 3.598034620285034}, {"id": 42744412, "title": "LLM Evaluation: Moving Beyond Manual Testing", "cluster": 13, "x": 6.803139686584473, "y": 3.2611024379730225}, {"id": 42743006, "title": "Provisional Guidance for Users of LLM-Based Code Generators", "cluster": 13, "x": 6.7159576416015625, "y": 3.727261781692505}, {"id": 42742621, "title": "Optimizing Jupyter Notebooks for LLMs", "cluster": 13, "x": 7.078615665435791, "y": 3.6176130771636963}, {"id": 42742377, "title": "Regulation of LLMs with Interpretability Will Likely Result in Performance Loss", "cluster": 13, "x": 6.807464599609375, "y": 3.167513132095337}, {"id": 42742280, "title": "LLM Says Buttons", "cluster": 13, "x": 6.731223106384277, "y": 3.5393381118774414}, {"id": 42741073, "title": "Foundations of Large Language Models", "cluster": 208, "x": 8.169279098510742, "y": 4.370723724365234}, {"id": 42740996, "title": "LLM-powered multiplayer trivia game using FastHTML", "cluster": 13, "x": 6.969448089599609, "y": 3.435786724090576}, {"id": 42736745, "title": "M5Stack LLM630 kit can run Llama3.2-1B", "cluster": 13, "x": 7.125790119171143, "y": 3.937143087387085}, {"id": 42736795, "title": "Transformer\u00b2: dynamic weight adaptation in LLMs", "cluster": 13, "x": 6.925398349761963, "y": 3.3070991039276123}, {"id": 42735994, "title": "Gandalf the Red: Adaptive Security for LLMs", "cluster": 13, "x": 6.561044692993164, "y": 3.4643163681030273}, {"id": 42734603, "title": "Path to LLMs", "cluster": 13, "x": 6.702529430389404, "y": 3.2425708770751953}, {"id": 42733664, "title": "Remote access software VS KVM extender:which one should I choose?", "cluster": 13, "x": 6.791630744934082, "y": 3.593085527420044}, {"id": 42733006, "title": "Svelte \u2013 Docs for LLMs", "cluster": 13, "x": 6.839770317077637, "y": 3.3752434253692627}, {"id": 42732808, "title": "Do reasoning LLMs need their own Philosophical Language?", "cluster": 13, "x": 6.755063533782959, "y": 3.125760793685913}, {"id": 42731572, "title": "Dissecting the ESM3 Model Architecture", "cluster": 13, "x": 7.312465667724609, "y": 3.7005774974823}, {"id": 42731579, "title": "Simulating 500M years of evolution with a language model", "cluster": 208, "x": 8.517568588256836, "y": 4.166648864746094}, {"id": 42730536, "title": "Simulating 500M years of evolution with a language model", "cluster": 208, "x": 8.486574172973633, "y": 4.18397855758667}, {"id": 42730345, "title": "DevDocs scrapes all the tech doc from 1 URL to provide to make the LLMs updated", "cluster": 13, "x": 6.967074394226074, "y": 3.704922914505005}, {"id": 42730246, "title": "Mufeedvh/code2prompt:A CLI tool to convert your codebase in a single LLM prompt", "cluster": 13, "x": 6.828939914703369, "y": 3.896226406097412}, {"id": 42730010, "title": "Provisional Guidance for Users of LLM-Based Code Generators", "cluster": 13, "x": 6.73678731918335, "y": 3.7427496910095215}, {"id": 42729611, "title": "Thoughts on Mood Machine by Liz Pelly", "cluster": 13, "x": 6.591721534729004, "y": 3.0199849605560303}, {"id": 42727475, "title": "Judicious Use of LLMs for Coding and Writing", "cluster": 13, "x": 6.633289813995361, "y": 3.604686975479126}, {"id": 42726584, "title": "Test-driven development with an LLM for fun and profit", "cluster": 13, "x": 6.622461318969727, "y": 3.6005759239196777}, {"id": 42725267, "title": "Quixotic: Bot and LLM Content Obfuscator", "cluster": 13, "x": 6.604887008666992, "y": 3.3746566772460938}, {"id": 42724463, "title": "Predictive LLM Chatbot UI Kit", "cluster": 13, "x": 6.701084136962891, "y": 3.5905652046203613}, {"id": 42723823, "title": "Llama2.c Running in a PDF", "cluster": 13, "x": 7.030301094055176, "y": 3.7876904010772705}, {"id": 42723422, "title": "I rebuilt the Auth0 Activity Page with webhooks and some LLM magic", "cluster": 13, "x": 6.780290603637695, "y": 3.8143203258514404}, {"id": 42722304, "title": "Goldman Sachs CEO on impact of LLMs to prepare IPO docs", "cluster": 13, "x": 6.701064586639404, "y": 3.1334445476531982}, {"id": 42721422, "title": "Were LLM providers prepared for agentic programming tools like Cline?", "cluster": 13, "x": 6.852865695953369, "y": 3.3522183895111084}, {"id": 42718841, "title": "LLM runs faster since we switched from AMD's driver to our AM driver", "cluster": 13, "x": 7.052847385406494, "y": 3.724443197250366}, {"id": 42717054, "title": "Using LLMs to help LLMs build Encore apps", "cluster": 13, "x": 6.779489994049072, "y": 3.646872043609619}, {"id": 42717071, "title": "A New Way to Guide LLM Reasoning", "cluster": 13, "x": 6.862184524536133, "y": 3.1967267990112305}, {"id": 42716200, "title": "Train faster static embedding models with sentence transformers", "cluster": 208, "x": 8.34003734588623, "y": 4.483847141265869}, {"id": 42715501, "title": "Browser AI: Open-source library to run LLM's in browser", "cluster": 12, "x": 7.043931007385254, "y": 2.7292301654815674}, {"id": 42713982, "title": "Turning my laptop into a Search Relevance Judge with local LLMs", "cluster": 13, "x": 6.8168559074401855, "y": 3.6024365425109863}, {"id": 42711598, "title": "Machine Learning for Absolute Beginners: Busting Popular ML Myths", "cluster": 13, "x": 7.413332939147949, "y": 3.3930249214172363}, {"id": 42711051, "title": "Structured decoding in LLMs, a guide for the impatient", "cluster": 13, "x": 6.810416221618652, "y": 3.3728995323181152}, {"id": 42710777, "title": "Why are we using LLMs as calculators?", "cluster": 13, "x": 6.739328384399414, "y": 3.222869634628296}, {"id": 42710241, "title": "Capability Computing at LLNL", "cluster": 13, "x": 6.726635456085205, "y": 3.653189182281494}, {"id": 42709928, "title": "Language Models: A Guide for the Perplexed", "cluster": 208, "x": 8.409770965576172, "y": 4.436041831970215}, {"id": 42709454, "title": "Gandalf the Red: Adaptive Security for LLMs", "cluster": 13, "x": 6.5374674797058105, "y": 3.4319393634796143}, {"id": 42709393, "title": "Running LLM evals right next to your code", "cluster": 13, "x": 6.789563179016113, "y": 3.6756341457366943}, {"id": 42709163, "title": "Adding payments to your LLM agentic workflows", "cluster": 13, "x": 6.873902320861816, "y": 3.2931623458862305}, {"id": 42708916, "title": "Llama running on a Pentium II machine with 128MB RAM running Windows 98", "cluster": 13, "x": 7.0352606773376465, "y": 3.9162516593933105}, {"id": 42708598, "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models", "cluster": 208, "x": 8.244223594665527, "y": 4.29660177230835}, {"id": 42708291, "title": "Has LLM killed traditional NLP?", "cluster": 13, "x": 6.569826126098633, "y": 3.2142250537872314}, {"id": 42707972, "title": "Some thoughts on when to use LLMs", "cluster": 13, "x": 6.661981582641602, "y": 3.139198064804077}, {"id": 42707925, "title": "Self Adaptive LLMs", "cluster": 13, "x": 6.720492839813232, "y": 3.468065023422241}, {"id": 42707592, "title": "LLM \u2013 A CLI utility and Python library for interacting with LLMs", "cluster": 13, "x": 6.90505838394165, "y": 3.78458571434021}, {"id": 42706427, "title": "Transformer-squared: Self-adaptive LLMs", "cluster": 13, "x": 6.86841344833374, "y": 3.326986074447632}, {"id": 42706169, "title": "About LLMs and Energy", "cluster": 13, "x": 6.794158935546875, "y": 3.1190366744995117}, {"id": 42705935, "title": "Transformer^2: Self-Adaptive LLMs", "cluster": 13, "x": 6.854700565338135, "y": 3.35357928276062}, {"id": 42705743, "title": "MiniMax new open source LLM with industry-leading 4M token context", "cluster": 13, "x": 6.931892395019531, "y": 3.722118616104126}, {"id": 42703920, "title": "Efficient LLM Scheduling by Learning to Rank", "cluster": 13, "x": 6.721169471740723, "y": 3.120544195175171}, {"id": 42703689, "title": "LM-Polygraph: Uncertainty Estimation for LLMs", "cluster": 13, "x": 6.9491143226623535, "y": 3.3251004219055176}, {"id": 42702971, "title": "Raink: Use LLMs for Document Ranking", "cluster": 13, "x": 6.843148708343506, "y": 3.6407594680786133}, {"id": 42701862, "title": "The Two Word Test as a semantic benchmark for large language models", "cluster": 208, "x": 8.335846900939941, "y": 4.522359371185303}, {"id": 42699914, "title": "Scaling LLMs with Golang: How we serve millions of LLM requests", "cluster": 13, "x": 6.858518600463867, "y": 3.6117935180664062}, {"id": 42698610, "title": "LLM based agents as Dungeon Masters", "cluster": 13, "x": 6.797867298126221, "y": 3.196259021759033}, {"id": 42698323, "title": "Exploring the Potential of LLM Agents as Dungeon Masters in Tabletop RPGs", "cluster": 13, "x": 6.884613990783691, "y": 3.2516424655914307}, {"id": 42697428, "title": "An LLM plugin to support Structured Outputs", "cluster": 13, "x": 6.91742467880249, "y": 3.7119171619415283}, {"id": 42696888, "title": "JVM Tuning with Machine Learning", "cluster": 13, "x": 7.228032112121582, "y": 3.3850128650665283}, {"id": 42689928, "title": "Microserving LLM Engines", "cluster": 13, "x": 6.694400787353516, "y": 3.46272873878479}, {"id": 42688833, "title": "New on-device LLM in iOS app for psychological card readings", "cluster": 13, "x": 6.875467300415039, "y": 3.709540605545044}, {"id": 42685281, "title": "Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws", "cluster": 13, "x": 6.4492011070251465, "y": 3.3487768173217773}, {"id": 42684878, "title": "Turning my laptop into a Search Relevance Judge with local LLMs", "cluster": 13, "x": 6.817459583282471, "y": 3.6075165271759033}, {"id": 42684907, "title": "SCIM and CIAM", "cluster": 13, "x": 6.816248416900635, "y": 3.4530205726623535}, {"id": 42684201, "title": "Agents and Ensemble Reasoning with open source LLMs", "cluster": 13, "x": 6.947030067443848, "y": 3.2386181354522705}, {"id": 42683951, "title": "State Space Models Are Strong Text Rerankers", "cluster": 208, "x": 8.25401782989502, "y": 4.4527106285095215}, {"id": 42683614, "title": "Installing and Developing VLLM with Ease", "cluster": 13, "x": 6.846304416656494, "y": 3.819218873977661}, {"id": 42682181, "title": "LLM forecasters rapidly approaching human-level performance", "cluster": 13, "x": 6.862691879272461, "y": 3.2790327072143555}, {"id": 42681336, "title": "New LLM jailbreak uses models' evaluation skills against them", "cluster": 13, "x": 6.734827518463135, "y": 3.5366122722625732}, {"id": 42681315, "title": "I use LLMs for coding and writing", "cluster": 13, "x": 6.632064342498779, "y": 3.677492380142212}, {"id": 42680782, "title": "LLM text makes human interactions less fun", "cluster": 13, "x": 6.615824222564697, "y": 3.2873218059539795}, {"id": 42678997, "title": "Running LLMs on the Nintendo Switch", "cluster": 13, "x": 6.872897624969482, "y": 3.717158079147339}, {"id": 42677113, "title": "Unlike an LLM, you can feel bad and go to jail", "cluster": 13, "x": 6.531013011932373, "y": 3.172861099243164}, {"id": 42676880, "title": "Inference Scaling vs. Reasoning: Analysis of Compute-Optimal LLM Problem-Solving", "cluster": 13, "x": 7.075757026672363, "y": 3.30057430267334}, {"id": 42676533, "title": "Fine-Tune Large Language Models for Your Projects", "cluster": 208, "x": 8.026100158691406, "y": 4.490711688995361}, {"id": 42675289, "title": "On-device query intent prediction with lightweight LLMs to support conversations", "cluster": 13, "x": 6.712525844573975, "y": 3.779310941696167}, {"id": 42675234, "title": "How I Use LLMs for Coding and Writing", "cluster": 13, "x": 6.686453819274902, "y": 3.689040184020996}, {"id": 42675394, "title": "LLMs: Independent, complex thinking not (yet) possible after all", "cluster": 13, "x": 6.685311794281006, "y": 3.0287508964538574}, {"id": 42675321, "title": "Why Larger Language Models Do In-Context Learning Differently?", "cluster": 208, "x": 8.256484985351562, "y": 4.3427734375}, {"id": 42674751, "title": "Data Quality in LLMs", "cluster": 13, "x": 6.884953498840332, "y": 3.3848628997802734}, {"id": 42673105, "title": "Running Llama on Windows 98", "cluster": 13, "x": 7.061528205871582, "y": 3.9695754051208496}, {"id": 42671485, "title": "What are the best agent tools I should play around with this weekend?", "cluster": 38, "x": 8.670730590820312, "y": 3.612114429473877}, {"id": 42671211, "title": "VLLM X AMD: Efficient LLM Inference on AMD Instinct MI300X GPUs (Part 1)", "cluster": 13, "x": 7.213186264038086, "y": 3.590641498565674}, {"id": 42670534, "title": "Modeling Story Expectations to Understand Engagement: A Framework Using LLMs", "cluster": 13, "x": 6.845896244049072, "y": 3.2117321491241455}, {"id": 42669985, "title": "Contemplative LLMs", "cluster": 13, "x": 6.574552059173584, "y": 3.084951400756836}, {"id": 42667518, "title": "LLMs struggle with perception, not reasoning, in ARC-AGI", "cluster": 13, "x": 6.833175182342529, "y": 3.1954805850982666}, {"id": 42667086, "title": "Orchestrating LLM Fine-Tuning on Kubernetes with SkyPilot and MLflow", "cluster": 13, "x": 6.85143518447876, "y": 3.635943651199341}, {"id": 42667047, "title": "Meta allegedly used shadow libraries, including LibGen, to train Llama", "cluster": 13, "x": 6.5949273109436035, "y": 3.47122859954834}, {"id": 42665533, "title": "Sony Proposes Changing LLVM Clang Default to C++20 Mode", "cluster": 13, "x": 7.0757317543029785, "y": 3.8485970497131348}, {"id": 42665602, "title": "The LLM in the Room", "cluster": 13, "x": 6.631219863891602, "y": 3.137617588043213}, {"id": 42665222, "title": "The State of Vim", "cluster": 13, "x": 6.896576404571533, "y": 3.2134792804718018}, {"id": 42664992, "title": "Microsoft introduces rStar-Math, an SLM for math reasoning and problem solving", "cluster": 13, "x": 6.916320323944092, "y": 3.3821914196014404}, {"id": 42663847, "title": "LLMs debate with each other, until one gives up", "cluster": 13, "x": 6.5324554443359375, "y": 3.0360629558563232}, {"id": 42658982, "title": "MCP, open protocol on how applications provide context to LLMs", "cluster": 13, "x": 6.902419567108154, "y": 3.7800028324127197}, {"id": 42658313, "title": "Contemplative LLMs: Anxiety is all you need?", "cluster": 13, "x": 6.586810111999512, "y": 3.061927080154419}, {"id": 42657440, "title": "I made a simple game with LLMs to help me learn more about football", "cluster": 13, "x": 6.804751396179199, "y": 3.2741031646728516}, {"id": 42656901, "title": "Running Open Source LLMs in Popular AI Clients with Featherless:A Complete Guide", "cluster": 12, "x": 7.057420253753662, "y": 2.9634511470794678}, {"id": 42656466, "title": "How to jailbreak most/all LLMs using Assistant Prefill", "cluster": 13, "x": 6.792923450469971, "y": 3.7637364864349365}, {"id": 42656188, "title": "I Program with LLMs", "cluster": 13, "x": 6.6790361404418945, "y": 3.691636323928833}, {"id": 42656008, "title": "Review: Implementing Linearizability at Large Scale and Low Latency", "cluster": 13, "x": 7.225272178649902, "y": 3.79964017868042}, {"id": 42655882, "title": "Pieces now powered by Ollama for enhanced local model integration", "cluster": 13, "x": 7.135481834411621, "y": 4.0096821784973145}, {"id": 42654074, "title": "A Survey on LLMs with Some Insights on Their Capabilities and Limitations", "cluster": 13, "x": 6.752503871917725, "y": 3.1718316078186035}, {"id": 42653744, "title": "How outdated information hides in LLM token generation probabilities", "cluster": 13, "x": 6.852449417114258, "y": 3.284982919692993}, {"id": 42651964, "title": "It's remarkably easy to inject new medical misinformation into LLMs", "cluster": 13, "x": 6.52009391784668, "y": 3.055433988571167}, {"id": 42651927, "title": "Meta Confesses to Training Llama with Pirated LibGen Data [pdf]", "cluster": 13, "x": 6.605962753295898, "y": 3.5112738609313965}, {"id": 42651349, "title": "Hacking Redefined: How LLM Agents Took on University Hacking Competition", "cluster": 13, "x": 6.4345011711120605, "y": 3.4352850914001465}, {"id": 42649315, "title": "Entropy of a Large Language Model output", "cluster": 208, "x": 8.162370681762695, "y": 4.34731912612915}, {"id": 42648576, "title": "Contemplative LLMs: Anxiety is all you need?", "cluster": 13, "x": 6.596116065979004, "y": 3.0801148414611816}, {"id": 42646773, "title": "Agent Laboratory: Using LLM Agents as Research Assistants", "cluster": 13, "x": 6.843244552612305, "y": 3.2317206859588623}, {"id": 42646609, "title": "Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking", "cluster": 13, "x": 6.803878307342529, "y": 3.147378921508789}, {"id": 42645784, "title": "Make Llama 3.1 8B talk in Rick Sanchez's style", "cluster": 13, "x": 6.880380630493164, "y": 3.8993194103240967}, {"id": 42645122, "title": "Towards System 2 Reasoning in LLMs: Learning How to Think Meta Chain-of-Thought", "cluster": 13, "x": 6.829136848449707, "y": 3.1368072032928467}, {"id": 42644537, "title": "Agent Laboratory: Using LLM Agents as Research Assistants", "cluster": 13, "x": 6.868521690368652, "y": 3.219712972640991}, {"id": 42643709, "title": "Do LVLMs Understand Charts?", "cluster": 13, "x": 6.810441970825195, "y": 3.190626621246338}, {"id": 42643697, "title": "What are some of the best application of LLMs in Agriculture?", "cluster": 13, "x": 6.851605415344238, "y": 3.3718061447143555}, {"id": 42642924, "title": "I killed a botnet with a LLM Honeypot", "cluster": 13, "x": 6.538854122161865, "y": 3.2286322116851807}, {"id": 42642456, "title": "If LLM/AI knowledge could be classified, what would be secret?", "cluster": 12, "x": 6.952288627624512, "y": 2.6698758602142334}, {"id": 42641817, "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking", "cluster": 13, "x": 6.8499603271484375, "y": 3.1318776607513428}, {"id": 42640663, "title": "The easiest way to run an LLM locally on your Mac", "cluster": 13, "x": 6.827384948730469, "y": 3.784588098526001}, {"id": 42639733, "title": "It's remarkably easy to inject new medical misinformation into LLMs", "cluster": 13, "x": 6.4869489669799805, "y": 3.02466082572937}, {"id": 42636842, "title": "Unlocking the Potential of Large Language Models in Data-Scarce Contexts", "cluster": 208, "x": 8.196219444274902, "y": 4.414731502532959}, {"id": 42636728, "title": "LLM Catcher \u2013 Automated Python Debugging Using LLMs", "cluster": 13, "x": 6.819647312164307, "y": 3.7399168014526367}, {"id": 42636608, "title": "Meta Open-Sources Byte Latent Transformer LLM with Improved Scalability \u2013 InfoQ", "cluster": 13, "x": 7.002431392669678, "y": 3.732908248901367}, {"id": 42635723, "title": "Taming LLMs: A Practical Guide to LLM Pitfalls with OSS", "cluster": 13, "x": 6.6281914710998535, "y": 3.3040170669555664}, {"id": 42635572, "title": "Scaling Laws for LLMs: From GPT-3 to o3", "cluster": 13, "x": 6.856284141540527, "y": 3.2576663494110107}, {"id": 42635337, "title": "Can LLMs generate good questions?", "cluster": 13, "x": 6.8155317306518555, "y": 3.309563159942627}, {"id": 42635308, "title": "LLM \"Structured Outputs\" Are Missing the Point", "cluster": 13, "x": 6.826439380645752, "y": 3.470980405807495}, {"id": 42635091, "title": "LLMs for AGI", "cluster": 13, "x": 6.767640113830566, "y": 3.446510076522827}, {"id": 42633212, "title": "LM Studio 0.3.6", "cluster": 13, "x": 6.986486434936523, "y": 3.9087772369384766}, {"id": 42632919, "title": "Local LLM Hosting with Two-Way Voice Support Using Python, Qwen, and Bark", "cluster": 13, "x": 6.808678150177002, "y": 3.7736260890960693}, {"id": 42632907, "title": "An Android client that allows for XMPP to not-TLS server(prosody)", "cluster": 70, "x": 6.862221717834473, "y": 3.9923596382141113}, {"id": 42632727, "title": "Agentics: A Minimalist Python Library for LLMs with Zero Boilerplate", "cluster": 13, "x": 6.926855087280273, "y": 3.6154918670654297}, {"id": 42629395, "title": "Sonus-1: New LLM Family with Reasoning Capabilities", "cluster": 13, "x": 6.894078254699707, "y": 3.366809606552124}, {"id": 42627339, "title": "LLMs are bad at derm, unless you give them a textual description", "cluster": 13, "x": 6.60686731338501, "y": 3.223620653152466}, {"id": 42626808, "title": "HuggingFace smolagents: a barebones Python library for LLM agents", "cluster": 13, "x": 6.88358736038208, "y": 3.6355903148651123}, {"id": 42626692, "title": "LLM Microserving: a new RISC-style approach to design LLM serving API", "cluster": 13, "x": 6.803184509277344, "y": 3.6743009090423584}, {"id": 42625131, "title": "An LLM model that writes LinkedIn post like humans", "cluster": 13, "x": 6.744224548339844, "y": 3.3891937732696533}, {"id": 42623560, "title": "OptView2: Inspect missed optimizations by LLVM Clang", "cluster": 13, "x": 7.164576053619385, "y": 3.6356942653656006}, {"id": 42623379, "title": "Agents - white paper by Google", "cluster": 35, "x": 8.43497085571289, "y": 3.8420801162719727}, {"id": 42623334, "title": "Political Bias in Large Language Models: Insights Across Topic Polarization", "cluster": 208, "x": 8.292684555053711, "y": 4.342322826385498}, {"id": 42623210, "title": "DistilkitPlus: Efficient Knowledge Distillation for LLMs", "cluster": 13, "x": 6.944129467010498, "y": 3.337090253829956}, {"id": 42622752, "title": "\"Contemplative reasoning\" response style for LLMs", "cluster": 13, "x": 6.7285590171813965, "y": 3.124807834625244}, {"id": 42621825, "title": "LLMs Are Directing Open Source Licensing", "cluster": 13, "x": 6.804520606994629, "y": 3.699371337890625}, {"id": 42621682, "title": "shellmind: LLM powered pseudocode shell commands", "cluster": 13, "x": 6.904708385467529, "y": 3.797873020172119}, {"id": 42621363, "title": "Improved Knowledge Graph Creation with LangChain and LlamaIndex", "cluster": 13, "x": 7.118195533752441, "y": 3.5807456970214844}, {"id": 42619172, "title": "How LLMs are reshaping the code of tomorrow, and what to do about it", "cluster": 13, "x": 6.594709873199463, "y": 3.502326726913452}, {"id": 42618943, "title": "Notion Incident Management System (NIMS)", "cluster": 13, "x": 6.689858436584473, "y": 3.491124391555786}, {"id": 42617900, "title": "LLM Observability and Monitoring", "cluster": 13, "x": 6.880882740020752, "y": 3.4338345527648926}, {"id": 42617645, "title": "How I program with LLMs", "cluster": 13, "x": 6.793832302093506, "y": 3.743826150894165}, {"id": 42617585, "title": "HuggingFace smolagents: a simple library to build LLM agents", "cluster": 13, "x": 6.893827438354492, "y": 3.573193073272705}, {"id": 42616976, "title": "Finding Missed Code Size Optimizations in Compilers Using LLMs", "cluster": 13, "x": 6.90347957611084, "y": 3.727055072784424}, {"id": 42616164, "title": "Bolt.diy Prompt, and deploy full-stack web applications using any LLM", "cluster": 13, "x": 6.800099849700928, "y": 3.7009811401367188}, {"id": 42613431, "title": "Generating Cognateful Sentences with LLMs", "cluster": 13, "x": 6.764194965362549, "y": 3.4612905979156494}, {"id": 42613424, "title": "LLMs must not have an identity", "cluster": 13, "x": 6.480754852294922, "y": 3.258780002593994}, {"id": 42612785, "title": "LOTUS makes LLM-powered data processing fast and easy (as easy as Pandas)", "cluster": 13, "x": 6.963466167449951, "y": 3.553267002105713}, {"id": 42612453, "title": "Getting LLMs to Generate Funny Memes Is Unexpectedly Hard", "cluster": 13, "x": 6.54808235168457, "y": 3.2399075031280518}, {"id": 42612125, "title": "Superhuman performance of an LLM on the reasoning tasks of a physician", "cluster": 13, "x": 6.744478225708008, "y": 3.13305926322937}, {"id": 42611599, "title": "CUDA/Metal accelerated language model inference", "cluster": 208, "x": 8.380301475524902, "y": 4.5118021965026855}, {"id": 42611325, "title": "Long Context vs. RAG for LLMs: An Evaluation and Revisits", "cluster": 13, "x": 6.739112377166748, "y": 3.1973824501037598}, {"id": 42610466, "title": "LLM Native Product Capabilities", "cluster": 13, "x": 6.8377461433410645, "y": 3.663294553756714}, {"id": 42610234, "title": "LLMs and Code Optimization", "cluster": 13, "x": 6.764845371246338, "y": 3.655170202255249}, {"id": 42610008, "title": "What We Learned from a Year of Building with LLMs (Part III)", "cluster": 13, "x": 6.710663318634033, "y": 3.1005589962005615}, {"id": 42609516, "title": "Slack as a message broker for LLM agents", "cluster": 13, "x": 6.84469747543335, "y": 3.373441457748413}, {"id": 42607089, "title": "Google Whitepaper on Agents", "cluster": 35, "x": 8.475083351135254, "y": 3.8067078590393066}, {"id": 42606300, "title": "Benchmarking LLM Agents on Consequential Real World Tasks", "cluster": 13, "x": 6.899230003356934, "y": 3.1761131286621094}, {"id": 42606231, "title": "Killed by LLM", "cluster": 13, "x": 6.479182243347168, "y": 3.203695297241211}, {"id": 42606103, "title": "Teaching LLMs to Code Review Like Senior Developers: A Context-First Approach", "cluster": 13, "x": 6.699813365936279, "y": 3.6970577239990234}, {"id": 42605183, "title": "Dynamic analysis, optimal control of competitive information dissemination model", "cluster": 36, "x": 8.599859237670898, "y": 3.595308542251587}, {"id": 42604327, "title": "Is Your LLM a World Model of the Internet? Planning for Web Agents", "cluster": 13, "x": 6.71321964263916, "y": 3.468039035797119}, {"id": 42603621, "title": "Self-Discovery with LLMs", "cluster": 13, "x": 6.576774597167969, "y": 3.1044163703918457}, {"id": 42603017, "title": "New LLM jailbreak uses models' evaluation skills against them", "cluster": 13, "x": 6.690804481506348, "y": 3.4804723262786865}, {"id": 42602749, "title": "This Year in LLVM", "cluster": 13, "x": 6.672933578491211, "y": 3.221191644668579}, {"id": 42602529, "title": "LLMs Simplify and Improve Model Validation in Banking", "cluster": 13, "x": 6.896866321563721, "y": 3.335887908935547}, {"id": 42601181, "title": "Towards Benchmarking LLM Diversity and Creativity", "cluster": 13, "x": 6.8875861167907715, "y": 3.299708127975464}, {"id": 42601069, "title": "RLLM: Rust library unifying multiple LLM back ends with builder-based API", "cluster": 13, "x": 6.975613594055176, "y": 3.896449327468872}, {"id": 42600716, "title": "Tool: LLM based \"2024 GitHub Rewind\"", "cluster": 13, "x": 6.97440242767334, "y": 3.8100788593292236}, {"id": 42600284, "title": "Using LLMs to make sense of chat data without compromising user privacy", "cluster": 13, "x": 6.546050548553467, "y": 3.500046968460083}, {"id": 42598720, "title": "LLM RAG Query Expansion for Better Prompting Results", "cluster": 13, "x": 6.961526870727539, "y": 3.4740428924560547}, {"id": 42598519, "title": "Agents, Google's whitepaper on the basics of LLM agents", "cluster": 13, "x": 6.80525541305542, "y": 3.2616374492645264}, {"id": 42598291, "title": "Agents Whitepaper by Google", "cluster": 35, "x": 8.465749740600586, "y": 3.777470827102661}, {"id": 42598130, "title": "Meta: Large Concept Models: Language Modeling in Sentence Representation Space", "cluster": 208, "x": 8.334108352661133, "y": 4.471893310546875}, {"id": 42594679, "title": "Output Token Order Is a Hidden Variable in LLM Response Quality and Accuracy", "cluster": 13, "x": 6.8801140785217285, "y": 3.1404502391815186}, {"id": 42594660, "title": "What LLMs mean for EBITDA margins", "cluster": 13, "x": 6.7908735275268555, "y": 3.213191270828247}, {"id": 42594612, "title": "LLMs Are Power Tools for Software Builders", "cluster": 13, "x": 6.604640483856201, "y": 3.5558159351348877}, {"id": 42594256, "title": "Using LLMs and Cursor to finish side projects", "cluster": 13, "x": 6.826406002044678, "y": 3.7597546577453613}, {"id": 42594239, "title": "Can LLMs Make Robots Smarter?", "cluster": 12, "x": 6.958747863769531, "y": 2.7428836822509766}, {"id": 42591172, "title": "Unlocking the Potential of Large Language Models in Data-Scarce Contexts", "cluster": 208, "x": 8.23311710357666, "y": 4.446298599243164}, {"id": 42590850, "title": "Whats Your Case Worth? Answering the Illusive Question Using LLMs/Neural Nets", "cluster": 13, "x": 6.928497791290283, "y": 3.3642313480377197}, {"id": 42589356, "title": "Algorithmic Language Models with Neurally Compiled Libraries", "cluster": 208, "x": 8.29188060760498, "y": 4.451783657073975}, {"id": 42588497, "title": "Wanted: Great guide on benchmarking LLMs using standard benchmarks", "cluster": 13, "x": 6.963099956512451, "y": 3.4236655235290527}, {"id": 42588436, "title": "Using LLMs and Cursor to finish side projects", "cluster": 13, "x": 6.826531410217285, "y": 3.7559545040130615}, {"id": 42587989, "title": "NexaQuant: Llama.cpp-Compatible Model Compression with 100%+ Accuracy Recovery", "cluster": 13, "x": 7.417931079864502, "y": 3.8863794803619385}, {"id": 42587786, "title": "Diffbot GraphRAG LLM", "cluster": 13, "x": 7.022824764251709, "y": 3.528017520904541}, {"id": 42586754, "title": "The Prompt Canvas: A Literature-Based Guide for Effective Prompts in LLMs", "cluster": 13, "x": 6.805538654327393, "y": 3.2315690517425537}, {"id": 42584400, "title": "Can LLMs write better code if you keep asking them to \u201cwrite better code\u201d?", "cluster": 13, "x": 6.553866386413574, "y": 3.558624744415283}, {"id": 42584167, "title": "My objection(s) to the \"LLMs are just next-token predictors\" take", "cluster": 13, "x": 6.731287479400635, "y": 3.0973100662231445}, {"id": 42582294, "title": "Gitee AI \u2013\u2013 LLM Model Serverless API", "cluster": 12, "x": 7.056863307952881, "y": 3.157278060913086}, {"id": 42581540, "title": "Enough with the LLM hysteria!", "cluster": 13, "x": 6.475033283233643, "y": 3.03314208984375}, {"id": 42581008, "title": "Converting handwritten, maths-heavy lecture notes to Markdown using LLMs", "cluster": 13, "x": 6.86716365814209, "y": 3.5683047771453857}, {"id": 42580610, "title": "Generating and solving difficult logic puzzles with LLMs", "cluster": 13, "x": 6.85469913482666, "y": 3.2350876331329346}, {"id": 42577739, "title": "Safety Filters make LLMs defective tools", "cluster": 13, "x": 6.572010040283203, "y": 3.270937204360962}, {"id": 42576966, "title": "3D modeling with LLMs as a CAD luddite", "cluster": 13, "x": 6.880288124084473, "y": 3.4903621673583984}, {"id": 42576755, "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English? (2023)", "cluster": 208, "x": 8.199370384216309, "y": 4.493323802947998}, {"id": 42576689, "title": "Can LLMs write better code if you keep asking them to \"write better code\"?", "cluster": 13, "x": 6.552931785583496, "y": 3.5214996337890625}, {"id": 42576480, "title": "The art of developing for LLM users", "cluster": 13, "x": 6.689301013946533, "y": 3.5456039905548096}, {"id": 42575639, "title": "The Overthinking of O1-Like LLMs", "cluster": 13, "x": 6.573977947235107, "y": 3.014639377593994}, {"id": 42573389, "title": "HawkinsDB: Neuroscience-Inspired Memory Layer for LLM Applications", "cluster": 13, "x": 6.931906223297119, "y": 3.240739345550537}, {"id": 42573314, "title": "Building a Sarcasm Detection System with LSTM and GloVe: A Complete Guide", "cluster": 13, "x": 6.887304306030273, "y": 3.8587820529937744}, {"id": 42572953, "title": "DAC: An Innovative Prompting Technique to Enhance Mathematical Accuracy in LLMs", "cluster": 13, "x": 6.877782344818115, "y": 3.2791121006011963}, {"id": 42570555, "title": "ReAct Prompting: A Strategic Look at Next-Gen LLM Interactions", "cluster": 13, "x": 6.805680751800537, "y": 3.2931265830993652}, {"id": 42569265, "title": "LLMRank \u2013 ranking LLMs using peer-based cross-evaluation and PageRank", "cluster": 13, "x": 6.853021621704102, "y": 3.362858295440674}, {"id": 42567235, "title": "Political biases and inconsistencies in bilingual GPT models in the US and China", "cluster": 208, "x": 8.406800270080566, "y": 4.371171474456787}, {"id": 42566683, "title": "Math with LLM\u2013 State of the Art", "cluster": 13, "x": 6.85713529586792, "y": 3.3171849250793457}, {"id": 42564838, "title": "Integrating LLMs and Emacs with Gptel", "cluster": 13, "x": 6.9772162437438965, "y": 3.831754207611084}, {"id": 42564347, "title": "DAC: Revolutionizing LLM Accuracy in Mathematical Applications", "cluster": 13, "x": 6.868043422698975, "y": 3.309674024581909}]